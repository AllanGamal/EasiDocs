[
    {
        "page_content": "1\nBIG IDEAS2024Annual ResearchReportFOR INFORMATIONAL PURPOSES ONLYBIG IDEAS 2024JANUARY 31, 2024\nARK Investment Management LLC. This is not a recommendation in relation to any named particular securities/cryptocurrencies and no warranty or guarantee is provided. Any references to particular securities/cryptocurrencies are for illustrative purposes only. There is no assurance that the Adviser will make any investments with the same or similar characteristics as any investment presented. The reader should not assume that an investment identified was or will be profitable. PAST PERFORMANCE IS NOT INDICATIVE OF FUTURE PERFORMANCE, FUTURE RETURNS ARE NOT GUARANTEED.",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 0
        }
    },
    {
        "page_content": "2BIG IDEAS 2024: DISCLOSURERisks Of Investing In InnovationPlease note: Companies that ARK believes are capitalizing on disruptive innovation and developing technologies to displace older technologies or create new markets may not in fact do so. ARK aims to educate investors and seeks to size the potential investment opportunity, noting that risks and uncertainties may impact our projections and research models. Investors should use the content presented for informational purposes only, and be aware of market risk, disruptive innovation risk, regulatory risk, and risks related to certain innovation areas. Please read risk disclosure carefully.DISRUPTIVE INNOVATIONRAPID PACE OF CHANGE\nUNCERTAINTY AND UNKNOWNS EXPOSURE ACROSS SECTORS AND MARKET CAPRISK OF INVESTING IN INNOVATIONREGULATORY HURDLES\nCOMPETITIVE LANDSCAPEPOLITICAL OR LEGAL PRESSURE\nSources: ARK Investment Management LLC, 2023.\u00e0   Aim to understand the regulatory, market, sector,        and company risks. (See Disclosure Page)\u00e0   Aim for a cross-sector understanding of technology   and combine top-down and bottom-up research.",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 1
        }
    },
    {
        "page_content": "3BIG IDEAS 2024: INTRODUCTIONBig Ideas 2024ARK Invest proudly presents \"Big Ideas 2024: Disrupting the Norm, Defining the Future.\" A tradition since 2017, Big Ideas offers a comprehensive analysis of technological convergence and its potential to revolutionize industries and economies.ARK seeks to deliver long-term capital appreciation by investing in the leaders, enablers, and beneficiaries of disruptive innovation. With a belief that innovation is key not only to growth but also to resilience, ARK emphasizes the necessity of a strategic allocation to innovation in every investor's portfolio. This approach aims to tap into the exponential growth opportunities often overlooked in broad-based indices, while simultaneously providing a hedge against the risks posed by incumbents facing disruption.We hope you enjoy Big Ideas 2024.Disrupting The Norm, Defining The Future",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 2
        }
    },
    {
        "page_content": "4\nReusable Rockets143Technological Convergence5Artificial Intelligence19Bitcoin Allocation34Bitcoin In 202343Smart Contracts 53Digital Consumers64Digital Wallets75Precision Therapies87Multiomic Tools & Technology96Electric Vehicles104Robotics113Robotaxis122Autonomous Logistics1333D Printing153TABLE OF CONTENTS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 3
        }
    },
    {
        "page_content": "5\n5Technological Convergence\nSources: ARK Investment Management LLC, 2024. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Brett WintonChief FuturistARK Venture Investment Committee MemberResearch By:BIG IDEAS 2024",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 4
        }
    },
    {
        "page_content": "6According to ARK\u2019s research, convergence among disruptive technologies will define this decade. Five major technology platforms\u2014Artificial Intelligence, Public Blockchains, MultiomicSequencing, Energy Storage, and Robotics\u2014are coalescing and should transform global economic activity.Technological convergence could create tectonic macroeconomic shifts more impactful than the first and second industrial revolutions. Globally, real economic growth could accelerate from 3% on average during the past 125 years to more than 7% during the next 7 years as robots reinvigorate manufacturing, robotaxistransform transportation, and artificial intelligence amplifies knowledge worker productivity. Catalyzed by breakthroughs in artificial intelligence, the global equity market value associated with disruptive innovation could increase from 16% of the total* to more than 60% by 2030. As a result, the annualized equity return associated with disruptive innovation could exceed 40% during the next seven years, increasing its market capitalization from ~$19 trillion today to roughly $220 trillion by 2030.*Throughout this section, we include public blockchain value as part of all calculations and forecasts of \u201cequity market value.\u201d Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.CONVERGENCE",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 5
        }
    },
    {
        "page_content": "7\nFive Innovation Platforms Are Converging And Defining This Technological Era\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Public BlockchainsUpon large-scale adoption, all money and contracts likely will migrate onto Public Blockchains that enableand verifydigital scarcityand proof of ownership. The financial ecosystem is likely to reconfigure to accommodate the rise of Cryptocurrencies and Smart Contracts. These technologies increase transparency, reduce the influence of capital and regulatory controls, and collapse contract execution costs. In such a world, Digital Wallets would become increasingly necessary as more assets become money-like, and corporations and consumers adapt to the new financial infrastructure. Corporate structures themselves may be called into question. Multiomic SequencingThe cost to gather, sequence, and understand digital biological data is falling precipitously. Multiomic Technologies provide research scientists, therapeutic organizations and health platforms with unprecedented access to DNA, RNA, protein, and digital health data. Cancer care should transform with pan-cancer blood tests. Multiomic data should feed into novel Precision Therapies using emerging gene editing techniques that target and cure rare diseases and chronic conditions. Multiomics shouldunlock entirely new Programmable Biology capabilities, including the design and synthesis of novel biological constructs with applications across industries, particularly agriculture and food production.  Artificial IntelligenceComputational systems and software that evolve with data can solve intractable problems, automate knowledge work, and accelerate technology\u2019s integration into every economic sector. The adoption of Neural Networks should prove more momentous than the introduction of the internet and potentially create 10s of trillion dollars of value. At scale these systems will require unprecedented computational resources, and AI-specific compute hardware should dominate the Next Gen Cloud datacenters that train and operate AI models. The potential for end-users is clear: a constellation of AI-driven Intelligent Devices that pervade people's lives, changing the way that they spend, work, and play. The adoption of artificial intelligence should transform every sector, impact every business, and catalyze every innovation platform. Energy Storage Declining costs of Advanced Battery Technology should cause an explosion in form factors, enabling Autonomous Mobility systems that collapse the cost of getting people and things from place to place. Electric drivetrain cost declines should unlock micro-mobility and aerial systems, including flying taxis, enabling business models that transform the landscape of cities. Autonomy should reduce the cost of taxi, delivery, and surveillance by an order of magnitude, enabling frictionless transport that could increase the velocity of e-commerce and make individual car ownership the exception rather than the rule. These innovations combined with large-scale stationary batteries should cause a transformation in energy, substituting electricity for liquid fuel and pushing generation infrastructure towards the edge of the network. RoboticsCatalyzed by artificial intelligence, Adaptive Robots can operate alongside humans and navigate legacy infrastructure, changing the way products are made and sold. 3D Printing should contribute to the digitization of manufacturing, increasing not only the performance and precision of end-use parts but also the resilience of supply chains. Meanwhile, the world\u2019s fastest robots, Reusable Rockets, should continue to reduce the cost of launching satellite constellations and enable uninterruptible connectivity. A nascent innovation platform, robotics could collapse the cost of distance with hypersonic travel, the cost of manufacturing complexity with 3D printers, and the cost of production with AI-guided robots. CONVERGENCE",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 6
        }
    },
    {
        "page_content": "8Converging Technologies Are Generating A Historic Technological Wave\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying sources, including Bekar et al. 2017, which may be provided upon request. The chart uses GPT 4 prompting to survey a comprehensive list of general purpose technologies using the identification framework detailed therein. Where available, academic literature is also used to assess attributable economic impact. A GPT-4 scoring rubric assesses technology-by-technology impacts. The impact measured directly is matched against the scoring to tune all scores to produce technology-by-technology estimates of economic impact (even when direct measures of economic impact are unattainable). Consistent with General Purpose Technology theory, these technologies are assumed to go through a period of investment in which economic impact is negative before productivity advances begin to realize into economic data. All technologies are assumed to have the same diffusion and realization cycle. If recent technologies are assumed to diffuse more quickly, the current wave would appear steeper. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.-2381318178017851790179518001805181018151820182518301835184018451850185518601865187018751880188518901895190019051910191519201925193019351940194519501955196019651970197519801985199019952000200520102015202020252030Estimated Economic Impact of General Purpose Technologies(Annual Percentage Point Additions to Real GDP Growth And Consumer Surplus)\nIntegrated CircuitNuclear PowerContainerizationInternetCellphonesGPSThe WebPCsBiotechFiber opticsE-CommerceRenewables3D PrintingReusable RocketsAdaptiverobotsAdvanced BatteriesAutonomous MobilityCloud ComputingAIIntelligent DevicesMultiomicTechnologyPrecision TherapiesProgrammable BiologyDigital WalletsSmart ContractsCryptocurrenciesInternal Combustion EngineElectricityTelephoneRadioRefrigerationAir ConditioningChemicals & Synthetic MaterialsAutomobileAssembly LineTelevisionJet EngineRailroadsTelegraphPhotographyBicycleSteam Engine\n2025 F2030 FCONVERGENCE",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 7
        }
    },
    {
        "page_content": "9AI Serves As The Central Technology Catalyst\nHighestHighMidLowLowestThe Technology Convergence matrix illustrates the relationships between and among technologies.\nMore detailed version of this graphic, including detailed scoring information and justification available here. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.CryptocurrenciesSmartContractsDigitalWalletsPrecisionTherapiesMultiomicTechnologyProgrammableBiologyNeuralNetworksNext GenCloudIntelligentDevicesAutonomousMobilityAdvanced BatteryTechnologyRenewableRocketsAdaptiveRobotics3DPrintingCrypto-currenciesSmart ContractsDigitalWalletsPrecision TherapiesMultiomicTechnologyNeuralNetworksNext GenCloudIntelligentDevicesAutonomousMobilityAdvancedBatteryTechnologyRenewableRocketsAdaptiveRobotics3DPrintingConvergence Score\nTechnology\nCatalyzing TechnologyCONVERGENCE\nProgrammableBiology",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 8
        }
    },
    {
        "page_content": "10AI Is Accelerating Faster Than Forecasters Anticipated\nSources: ARK Investment Management LLC, 2024, based on data from Metaculus, including benchmark details, as of January 3, 2024. Benchmark broadly requires the successful passage of an adversarial two-hour Tuning test, broad success on a Q&A knowledge and logic benchmark, and the successful interpretation of and execution complex model car assembly instruction, all within a single system. Green lines are derived estimates for time to general purpose AI (strongly formulated) based upon forecasts for a weaker benchmark. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Google demonstrates advanced conversational agent, LLaMda2ChatGPTlaunches to the publicGPT-4 launchesOpenAIannounces GPT-3\n110100\n201920202021202220232024202520262027202820292030Number of YearsExpected Years Until Launch Of A General Artificial Intelligence System(Log Scale) Pre GPT-3 average80 years\nIf forecast error continuesIf forecast is well-tuned34 years18 years8 years50 yearsCONVERGENCE",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 9
        }
    },
    {
        "page_content": "11Individual Technology Advances Can Coalesce And Cascade Into Massive New Market OpportunitiesAdvanced AI enables robotaxis to rely on fewer, less expensive sensors.Battery electric drivetrains reduce robotaxi operating costs by 60%.\n$0.31 $0.12 Internal combustionElectricRobotaxiOperating Cost Per MileBy Drivetrain TypeNeural NetworksAdvanced Battery Technology+=The combination of AI and battery electric drivetrains enables robotaxi systems to scale.Autonomous Mobility\nIn addition to better batteries and AI, general purpose robots will require better:\u2022Electric motors\u2022Power electronics\u2022Sensors\u2022Power-efficient computeAs robotaxis scale, the cost of each technology should decline according to its learning curve.Adaptive Robotics\n*Waymo manufacturing costs are estimated based upon public statements. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results. 5010015020 0\nWaymoTeslaThousands, $Robotaxi Manufacturing Costs(Per Vehicle, 2024)*\n5 LIDARs, 29 cameras, 6 radars, 8 ultrasonic sensors9 CamerasCONVERGENCE",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 10
        }
    },
    {
        "page_content": "12The Impact Of These Technologies On The Economy Should Prove Dramatic\n*Adaptive Robotics, Autonomous Mobility, and AI Impact are ARK Invest estimates. AI estimate includes consumer surpluses that may not be captured in traditional economic statistics. IT productivity impact likely also undercounts consumer surplus. Industrial Robot and IT impact measures impact on US, Europe, and Japanese economies. Steam Engine impact is measured against the UK economy. Sources: ARK Investment Management LLC, 2024, based on data from Crafts 2004, O\u2019Mahony et al. 2009, and McKinsey Global Institute  2017. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.***0%20%40%60%80%100%120%140%\nIndustrial robots(1997 to 2007)Information Technology(1995 to 2005)Adaptive robotics(2023 to 2030)Autonomous Mobility(2023 to 2030)Steam engine(1830 to 1910)AI(2023 to 2030)Economic Impact of Select Major Technologies(Cumulative Increase In Real GDP Attributable to Technology After Introduction)\nIndustrial Robots(1997 to 2007)Adaptive Robotics *(2023 to 2030)Steam Engine(1830 to 1910)CONVERGENCE",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 11
        }
    },
    {
        "page_content": "13Technological Innovation Could Be Disruptive Enough To Dominate Global Equity Market Capitalizations2023Equity Market Cap EstimateNon-innovation  $98 trillionDisruptive Innovation  $19 trillionTotal   $117 trillion2030Equity Market Cap ForecastNon-innovation  $140 trillionDisruptive Innovation $220 trillionTotal   $360 trillionAnnual GrowthForecast3%42%17%ArtificiaI Intelligence37%Energy Storage50%Public Blockchains48%Robotics78%Multiomic Sequencing39%\nNote: Forecasted numbers are rounded. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of external sources, including the World Federation of Exchanges and the MSCI ACWI IMI Innovation Index which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results. CONVERGENCEPublic BlockchainsMultiomic Sequencing\nEnergy StorageAIRobotics",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 12
        }
    },
    {
        "page_content": "14Expectations For Public BlockchainsTechnology2040 PossibilitiesARK\u2019s 2030 Expectation of ProgressCryptocurrenciesCryptocurrencies have displaced most permission-based, centrally controlled monetary systems, enabling financial ecosystems to reformulate around a digital asset that can eliminate counterparty risk while continuing to facilitate transaction flows. The reformulation began at the edges of the traditional financial system in geographies with broken money systems and in markets otherwise mis-served by traditional financial intermediaries. In developed markets, cryptocurrencies initially served as a store of value, providing little direct utility. Over time, the efficiencies of a truly neutral digital currency, primarily bitcoin, have prevailed over other financial architectures.Global money supply has grown in tandem with GDP, and cryptocurrencies now account for ~10% of the total. Little of that value accrual is attributable to the direct displacement of money though there are instances in emerging markets. Much of the appreciation is a function of low single-digit percent allocations by institutional and high net worth individuals as well as corporate and nation-state treasuries. Cryptocurrencies continue to displace gold as a flight-to-safety asset, taking 40% share of the market. Utility use cases such as remittances and global settlements account for ~10% and~ 5% of volumes, respectivelySmart ContractsMost contracts have migrated to open-source protocols that enableand verifydigital scarcityand proof of ownership. Risk-sharing arrangements are more transparent, assets of all sorts are securitized, bought, and sold more easily, and counterparty risks have diminished substantially. The importance of traditional financial intermediaries has dwindled, even as more human activity becomes commercialized. Decentralized protocols, enabled by balance-sheet-light digital wallet platforms, facilitate most traditional financial functions. Consumer internet services rely on business models enabled by digital asset ownership. Every corporate entity and every consumer has adapted as centralized corporate structures themselves are called into question.Global financial assets as percent of GDP have continued to increase, with less than 5% secured by smart contracting platforms\u2014a dynamic consistent with the adoption curve of dialup internet. At 1%, the gross take from tokenized assets on decentralized protocols is less than a third of the fees that traditional financial institutions extract. Application protocols, which pay a larger share of fees to incentivize network participants, account for 75% of gross decentralized protocol revenues. The blended net take rate between application layer protocols and Level 1 protocols is roughly 60bps. \nDigital WalletsDigital wallets enable nearly every person with a connected device to transmit and receive money instantly, fundamentally transforming the through-flow of commercial and financial experiences. Digital wallets that facilitate wholesale pricing of financial services for individual users have disrupted retail banking relationships, fundamentally transforming consumer relationships with financial service providers. In addition to their financial functions, digital wallets are distribution platforms for a variety of digital services\u2014from ride-hailing to e-commerce\u2014and are secure repositories for digital health and other sensitive data. Traditional financial service institutions and their associated payment processing value chains have given way largely to internet-enabled digital wallets for most economic activity.Roughly 90% of smartphone users rely on digital wallets to some degree. The majority uses digital wallets as the front-end for more than half of meaningful financial functions. Digital wallet platform providers continue to rely on traditional ecosystems to facilitate financial activities like lending but can extract lead generation fees of 5-20% for delivering customers to those institutions. They also can capture 3-10% commerce facilitation fees for e-commerce activity directed through their platforms. Sources: ARK Investment Management LLC, 2024. In the above table, we characterize the convergent technological capabilities that we believe may manifest by 2030 and 2050. We stress that these scenarios, written in the present tense, are possible outcomes\u2014not assured outcomes\u2014and that the future may play out differently. This ARK analysis is based on a range of external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security.Although the scenarios described in the table below are written in present tense, they are forecasted, possible outcomes based on ARK's views. These possible outcomes may not be realized in the future due to a number of uncertainties. The information provided should not be considered investment advice and should not form the basis of any investment decision.CONVERGENCE",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 13
        }
    },
    {
        "page_content": "15Expectations For MultiomicSequencingTechnology2040 PossibilitiesARK\u2019s 2030 Expectation of ProgressPrecision TherapiesTechnology enables the manipulation of molecular biological systems, catalyzing a new generation of more efficacious and durable precision therapies. CRISPR-based gene-editing enables the manipulation of DNA directly with increasing specificity. RNA-acting therapeutic techniques restrict the area of DNA that can be transcribed into proteins. AI-advances enable the targeting of specific proteins that cause underlying disorders. These breakthroughs have shortened development timelines for and increased the efficacy of curative therapies that command higher prices than traditional therapies. Researchers are aiming to cure most rare diseases. Traditional health service spending declines, ceding economic terrain to molecular cures.Precision therapies make up 25% of newly released drugs. By improving the quality of life, lowering ancillary medical costs, and often effectively curing diseases, they command average price premiums of 7x relative to traditional drugs. Combined with expected improvements in R&D efficiencies, these drugs add 15% or ~$300 billion to drug revenues in 2030.\nMultiomic TechnologiesCatalyzed by the precipitous fall in sequencing costs, researchers and clinicians routinely collect patients\u2019 epigenomic, transcriptomic, and proteomic data. With increasingly comprehensive digital health readouts from intelligent devices and emerging AI tools, they align this panoply of multiomic data to understand, predict, and treat disease. As a result, cancer care has transformed completely: multiomic technologies detect cancer at early stages, target treatment more precisely, and provide recurrence monitoring. Regular blood-based pan-cancer tests are a standard of care for patients in middle age. Multiomic technology has increased biotech R&D efficiency, as clinical trials target patient populations and measure outcomes more precisely and easily. Combined with AI, multiomic technology has transformed the relationship between patients and health systems. Digital health providers, diagnostic tool companies, and molecular testing companies are leading the charge. Legacy drug franchises and health service systems have lost their prominence. Wasteful healthcare spending declines as healthy lives extend.At full penetration, R&D efficiency associated with drug development could double, thanks to AI-enhanced multiomic technology. By 2030, nearly all new drug development programs incorporate multiomics into preclinical R&D, and ~50% incorporate AI into clinical programs. Realized returns on R&D have improved by 10% with line-of-sight to a near doubling of R&D returns by 2035. Early detection multi-cancer blood tests have become standard of care as they have cut cancer mortality by 25% for some age cohorts. In developed markets, 30% of patients benefit from the new diagnostics regime. \nProgrammable BiologyAI tools, improved genomic synthesis techniques, and scalable biological manufacturing techniques enable novel, lower cost biological constructs with predictable performance, powering a renaissance in agriculture and materials science. Programmable biology enables breakthroughs in materials science and bio-based fuels that increase food production and reduce environmental externalities. Molecular biological primitives offer a substrate for new robust computation architectures.Still restricted to early stage and development projects, gene synthesis generates $10 billion in annual revenue. Programmable biology platforms capture 10% of precision therapy revenue. Those platforms generate another $30 billion in revenue with gross margins at ~70%, EBITDA margins in the 35% range, and free cash flow margins at ~20%. Sources: ARK Investment Management LLC, 2024. In the above table, we characterize the convergent technological capabilities that we believe may manifest by 2030 and 2050. We stress that these scenarios, written in the present tense, are possible outcomes\u2014not assured outcomes\u2014and that the future may play out differently. This ARK analysis is based on a range of external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security.Although the scenarios described in the table below are written in present tense, they are forecasted, possible outcomes based on ARK's views. These possible outcomes may not be realized in the future due to a number of uncertainties. The information provided should not be considered investment advice and should not form the basis of any investment decision.CONVERGENCE",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 14
        }
    },
    {
        "page_content": "16Expectations For Energy StorageTechnology2040 PossibilitiesARK\u2019s 2030 Expectation of ProgressAutonomous MobilityRobots move people and parcels from place to place and have changed the economics of physical movement entirely. The cost of taxi, delivery, and observation have fallen by an order of magnitude. Traveling by robotaxi is the norm and owning a personal vehicle the exception. Frictionless drone and robot delivery has catalyzed the velocity of ecommerce. The data generated by autonomous mobility systems provide pervasive, real-time insights into the state of the world. Consumers and businesses that harness autonomous mobility platforms are benefitting, while prior incumbents in the automotive, logistics, retail, and insurance sectors have been upended.Autonomous robotaxis have transformed global transport, as point-to-point transportation is available in nearly every country at an average price of ~$.50 per mile. Given the compelling price-point and utility, robotaxis have traveled 13 trillion vehicle miles and are gaining traction. Autonomous robotaxi platforms charge platform fees or take-rates of 50%+, generate ~50% operating margins, and give asset owner-operators the opportunity to generate reasonable rates of return on capital. The number of autonomous vehicles facilitating this travel is ~100 million, and most of the incremental vehicle production is autonomous-capable. Advanced Battery SystemsDeclining battery costshave ignited aCambrianexplosion in mobility form factors, pushing electrical supply out to end-nodes on networks.Electric vehiclesdominate transport as internal combustion dies. Micro-mobility and aerial systems that include flying taxis enable innovative business models that transform urban landscapes. All these innovations drive fundamental demand for electrical energy at the expense of liquid fuel. They also provide electrical energy more efficiently, reducing the vulnerability of grids, operational expenses, and the capital intensity of transmission and distribution. Oildemand is in decline, and traditional automotive manufacturers and suppliershave been displaced by a smaller number of vertically integrated technology providers.As ridership shifts to electric autonomous platforms, the number ofautonomous capable EVs sold annually is ~74 million, accounting for most of the automotive market. At an average selling price of ~$20,000, EV manufacturers generate $1.4 trillion in annual revenue, ~20% gross margins, and ~10% EBIT margins. With manufacturing consolidation, margins increase. Batteries account for ~20% of the value of EVs. Much like that of EVs, battery manufacturing is capital-intensive and low-margin. Supplying the EV OEMs, battery manufacturers generate revenue of $300 billion per year. Stationary energy storage requires a volume of batteries roughly equivalent to that consumed by EVs, generating another $300 billion in revenue. \nSources: ARK Investment Management LLC, 2024. In the above table, we characterize the convergent technological capabilities that we believe may manifest by 2030 and 2050. We stress that these scenarios, written in the present tense, are possible outcomes\u2014not assured outcomes\u2014and that the future may play out differently. This ARK analysis is based on a range of external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security.Although the scenarios described in the table below are written in present tense, they are forecasted, possible outcomes based on ARK's views. These possible outcomes may not be realized in the future due to a number of uncertainties. The information provided should not be considered investment advice and should not form the basis of any investment decision.CONVERGENCE",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 15
        }
    },
    {
        "page_content": "17Expectations For Artificial IntelligenceTechnology2040 PossibilitiesARK\u2019s 2030 Expectation of ProgressNeural NetworksFed by massive amounts of data, computational systems and software are solving previously unsolvable problems, automating knowledge work, and accelerating the integration of technology into all economic processes. As costs have plummeted, custom software is improving with every AI model enhancement and connecting the world. Learning systems are blazingly fast, their impact as momentous as the introduction of the microprocessor, transforming every sector and region.The cost of training AI models has fallen more than 40,000-fold which, when combined with aggressive investments in AI hardware, has catapulted aggregate AI capability roughly 600,000-fold since 2023. Adopted by 50% of knowledge workers, AI software systems have improved their productivity by 9x on average. Consistent with other software products, enterprises pay 10% of the productivity increase to access the software. Next Gen CloudCloud tools train the AI models that dominate software stacks and the software connections that stitch together the AI-run world. The infrastructure-as-a-service providers, chip manufacturers, and tool-manufacturers that facilitate the training of neural networks have enjoyed a multi-decade demand cycle. Software development has been democratized, and the companies providing API hooks that stitch together interoperable software layers experience unprecedented demand.AI hardware spend of $1.3 trillion supports $13 trillion in AI software sales and accommodates traditional software gross margins of 75%. Three types of customers support the demand for AI hardware--infrastructure-as-a-service providers, software companies, and AI foundation model providers\u2014which should generate 20% cashflow margins, consistent with those of chip manufacturers. \nIntelligent DevicesAI powers a new class of intelligent devices in the home and on the go. Fixed internet-and AI-powered infrastructure exists in homes and other social environments, transforming distribution for all media providers. End-users interface with the world in completely new ways, and data on their consumption preferences spawn new business models and services. Commerce and wagering permeate entertainment experiences, enabling and catalyzing new advertising formats and content monetization. The show is the store. Linear TV is obsolete, as digital curation and direct consumer preference drive visual content. Linear content is ceding ground to interactive experiences, sometimes subtly. AI-mediated glasses and headsets thread through the fabric of everyday life.Consumer spending on intelligent device hardware continues its uptrend to ~$60 per internet user per year. Time spent connected grows dramatically to half of waking leisure hours, or 20 trillion globally. Digital experiences continue to monetize at a discount to in-person experiences and yield $0.25 per hour spent online in revenue to platform providers.Between device spend and digital entertainment experiences, $5.4 trillion in revenue accrues to intelligent devices, entertainment, and social platforms. Advertising and commerce  comprise 80% of that revenue. \nSources: ARK Investment Management LLC, 2024. In the above table, we characterize the convergent technological capabilities that we believe may manifest by 2030 and 2050. We stress that these scenarios, written in the present tense, are possible outcomes\u2014not assured outcomes\u2014and that the future may play out differently. This ARK analysis is based on a range of external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security.Although the scenarios described in the table below are written in present tense, they are forecasted, possible outcomes based on ARK's views. These possible outcomes may not be realized in the future due to a number of uncertainties. The information provided should not be considered investment advice and should not form the basis of any investment decision.CONVERGENCE",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 16
        }
    },
    {
        "page_content": "18Expectations For RoboticsTechnology2040 PossibilitiesARK\u2019s 2030 Expectation of ProgressReusable RocketsReusable rockets are inexpensive and have spawned new business models. Low-earth orbit constellations connect every smartphone user on earth to a censor-resistant data feed. Hypersonic point-to-point travel is becoming a reality, disrupting long-haul flight, transforming military asset delivery, and shrinking global supply chains. Extra-planetary human exploration has begun ramping.Led by SpaceX\u2019s Starship launch volumes, a 40,000 strong satellite network is in orbit, facilitating direct-to-satellite communications for nearly all smartphones and delivering broadband-type speeds to ships, RVs, airplanes, and rural residents in developed and developing countries. Given the relative ease with which customers can be on-boarded\u2014a power outlet, an antenna, and a clear path to the sky\u2014most customers are engaged in an addressable market totaling $130 billion annually. Adaptive RoboticsAdaptive robots powered by artificial intelligence are transforming the economy. The cost of humanoid robots that are backward-compatible with existing infrastructure has dropped below that of human manufacturing labor for many applications. Previously intractable household tasks are submitting to automation at price points that create compelling end-markets. Fleets of robots grow more performant with every AI software upgrade. A virtuous circle of fleet data generation and AI model training drives performance forward. Manufacturing productivity growth accelerates as a wider array of physical goods submit to technologically-driven cost declines. Robots continue to penetrate the service sector as well. The economy has entered a period of undeniable and unprecedented explosive growth.Adaptive robots have penetrated manufacturing processes enough to increase productivity by 15%, and annual unit sales of humanoid robots have grown to 10% of the number of humans in the manufacturing workforce. Less expensive robots in human form-factors have begun to populate households, particularly in developed countries. While still limited in capability, these robots address a third of household chores, their sticker prices justified by the time that household members save. Robot manufacturers enjoy margins at the higher end of capital equipment suppliers, thanks to software.\n3D Printing3D printing has removed design barriers and reduced cost, weight, and time to production, dramatically transforming traditional manufacturing methods.Healthcare tools created with 3D printing are personalized and custom-made, resulting in better experiences for both patients and doctors. Lighter 3D-printed aerospace parts reduce global emissions and give flight to new aircraft both for earth and outer space. Replacement parts across industries are printed on demand at a fraction of previous costs, ultimately short-circuiting supply-chain shortfalls. 3D printing enables artificial intelligence to design parts once impossible to manufacture. 3D printing continues to dominate the prototyping market and has penetrated substantial parts of the intermediate tooling market, enabling low-cost design iterations across injection molding and metal casting applications. Most important to industry growth, 3D printing has begun to see meaningful uptake into end-use applications across aerospace and automotive, markets that collectively sell more than $4 trillion in equipment per year. Across all industries, nearly $900 billion in end-use parts could adopt 3D printing, though that penetration remains in the teens. Sources: ARK Investment Management LLC, 2024. In the above table, we characterize the convergent technological capabilities that we believe may manifest by 2030 and 2050. We stress that these scenarios, written in the present tense, are possible outcomes\u2014not assured outcomes\u2014and that the future may play out differently. This ARK analysis is based on a range of external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security.Although the scenarios described in the table below are written in present tense, they are forecasted, possible outcomes based on ARK's views. These possible outcomes may not be realized in the future due to a number of uncertainties. The information provided should not be considered investment advice and should not form the basis of any investment decision.CONVERGENCE",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 17
        }
    },
    {
        "page_content": "19\n19Artificial IntelligenceScaling Global Intelligence And Redefining Work\nSources: ARK Investment Management LLC, 2024. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Frank Downing Director of Research,Next Generation InternetJozef SojaResearch Associate Research By:BIG IDEAS 2024",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 18
        }
    },
    {
        "page_content": "20With superhuman performance on a wide range of tests, AI models like GPT-4 should catalyze an unprecedented boom in productivity. Jolted by ChatGPT\u2019s \u201ciPhone\u201d like moment, enterprises are scrambling to harness the potential of artificial intelligence (AI). AI promises more than efficiency gains, thanks to rapidly falling costs and open-source models. If knowledge worker productivity were to quadruple by 2030, as we believe is likely, growth in real GDP could accelerate and break records during the next five to ten years.  \nSources: ARK Investment Management LLC, 2024. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.ARTIFICIAL INTELLIGENCE",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 19
        }
    },
    {
        "page_content": "21\nPre-ChatGPT AveragePost-ChatGPT Average\n020406080100120140160180\nQ4'21Q1'22Q2 '22Q3'22Q4'22Q1'23Q2 '23Q3'23Number of MentionsThe Number of AI Mentions Tripled On Earnings CallsAlphabetAppleAmazonMetaMicrosoft\n0102030405060708090100\n012345Monthly Active Users(Millions) YearsChatGPTUsers Hit 100 Million Users In Two MonthsChatGPTWeChatTikTokInstagramYouTubeFaceBookChatGPTDelighted Consumers And Amazed EnterprisesBuilding on years of progress since Google invented transformer architecture in 2017, ChatGPT catalyzed the public\u2019s understanding of generative AI. No longer a tool just for developers, ChatGPT\u2019ssimple chat interface enabled anyone speaking any language to harness the power of large language models (LLMs). In 2023, enterprises scrambled to understand and deploy generative AI.\n*values between 0 and 100 million users are estimatesSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of data sources, which are available upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.ARTIFICIAL INTELLIGENCE",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 20
        }
    },
    {
        "page_content": "22AI Already Has Boosted Productivity SignificantlyCoding assistants like GitHub Copilot and Replit AI are early success stories that have boosted the productivity and job satisfaction of software developers. AI-powered assistants are increasing the performance of knowledge workers and, interestingly, benefiting underperforming workers relatively more than high performers. \nWithout Gen AIWith Gen AITask Speed1.25x\nWithout Gen AIWith Gen AITask Quality\nTask Quality, Top 50th Percentile of WorkersTask Quality, Bottom 50th Percentile of Workers1.17x1.43xProductivity of Consultants Using Gen AI In 2023\nSources: ARK Investment Management LLC, 2024. The data used to analyze productivity were collected from several different studies with varying numbers of participants and definitions of task quality. The sources used are Dell\u2019Acqua et al. 2023 and GitHub 2022. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Without CopilotWith CopilotProductivity of Developers On Coding Tasks Using GithubCopilot in 20232.2xARTIFICIAL INTELLIGENCE",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 21
        }
    },
    {
        "page_content": "23\n0%10%20 %30%40%50 %60%70%80%90%100%\nUSABO* Semifinal 2020GRE VerbalSAT EBRWGRE WritingAP Environmental ScienceUnif orm Bar ExamSAT MathAP US HistoryLSATAP US GovernmentAP Art HistoryAP StatisticsAP BiologyAP MacroeconomicsAP PsychologyAP MicroeconomicsGRE QuantitativeAP ChemistryAP Physics 2AP World HistoryAMC 12AP Calculus BCAP English LanguageAP English LiteratureAMC 10Codeforces RatingPercentileGPT-3.5, GPT-4, and Claude 2 Results on Professional and Academic ExamsGPT-3.5GPT-4Claude 2GPT-4 VisionFoundation Models Are Improving Across DomainsWith larger training datasets and more parameters, GPT-4 outperforms GPT-3.5 significantly. Increasingly, foundation models are becoming \u201cmultimodal\u201d\u2014supporting text, images, audio, and video\u2014and are not only more dynamic and user friendly, but also more performant.\n*USA Biology Olympiad, a prestigious national competition testing high school students in biology. Sources: ARK Investment Management LLC, 2024, based on data from OpenAI and Anthropic as of Jan. 9, 2024. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.ARTIFICIAL INTELLIGENCE",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 22
        }
    },
    {
        "page_content": "24\nText-To-Image Models Are Reinventing Graphic Design Eight years after researchers at the University of Toronto introduced the first modern text-to-image model, the output from image models now rivals that of professional graphic designers. A human designer can create an image\u2014like a herd of elephants walking across a green grass field\u2014in several hours for several hundred dollars. Text-to-image models can produce the same graphic in seconds for pennies. Professional apps like Adobe Photoshop and consumer apps like Lensa and ChatGPT are integrating image models into their products and services. \nFebruary 2016alignDRAW\nFebruary 2022Midjourney v1\nNovember 2022Midjourney v4December 2023Midjourney v6Sources: ARK Investment Management LLC, 2024. Images sourced from Masimov et al. 2016 and Midjourney. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.A herd of elephants walking across a green grass field\nARTIFICIAL INTELLIGENCE",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 23
        }
    },
    {
        "page_content": "25The Cost Of Authoring The Written Word Has CollapsedOver the past century, the cost of authoring written content has been relatively constant in real terms. During the past two years, as the writing quality of LLMs has improved, the cost has collapsed.\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of data sources as of Jan 9, 2024, which are available upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.ARTIFICIAL INTELLIGENCEGPT4 32k$0.16Median GRE Analytic WritingClaude 2$0.04Top Decile GRE Analytic Writing$0$0$1$10$100$1,00019021905190819111914191719201923192619291932193519381941194419471950195319561959196219651968197119741977198019831986198919921995199820012004200720102013201620192022Cost Per 1000 Words Written, 2023 Dollars, Log ScaleTheCost of AuthoringWritten Content\nPost 1997 assumes constant words per employed writer over time$0.10",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 24
        }
    },
    {
        "page_content": "26AI Training Performance Is Improving RapidlyOther Algorithmic Innovations\u2022Llama2 suggests superior writing ability of LLMs is fundamentally driven by reinforcement learning from human feedback (RLHF)\u2022Optimized prompts can outperform human prompts by over 50%\u2022Speculative Decoding speeds up inference 2-3x on certain models\u2022Flash Attention 2 results in a 2.8x training speedup in GPT modelsAI researchers are innovating across training and inference, hardware, and model designsto increase performance and lower costs.Model Training Performance GainsIncreaseDecreaseTotal\n2023 PerformanceMoore\u2019s Law Predicted ImprovementNVIDIA\u2019s Outperformance of Moore\u2019s LawChinchilla Optimal ScalingOther Software Innovations2024 Performance ForecastAlgorithmic OptimizationsMoore\u2019s LawAccelerator Optimizations>5x\nBase = 1x\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of data sources, including Benaich 2023, Touvron et al. 2023, Yang et al. 2023, Leviathan et al. 2022, and Dao 2023, which are available upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.ARTIFICIAL INTELLIGENCE",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 25
        }
    },
    {
        "page_content": "27Training Costs Should Continue To Fall 75% Per YearAccording to Wright\u2019s Law, improvements in accelerated compute hardware should reduce AI-relative compute unit (RCU) production costs by 53% per year, while algorithmic model enhancements could lower training costs further by 47% per year. In other words, the convergence of hardware and software could driveAI training costs down by 75% at an annual rate through 2030.\n 0.000 0.000 0.001 0.010 0.100 1.000\n 0 1 100 10,000 1,000,000TFS-Days*(Log Scale)\nCumulative RCUs Produced(Millions) (Log Scale)AI Software Training Cost Using Neural NetworksEstimated Compute\n*TFS-Days is a measure of compute required to train a model. Wright\u2019s Law states that for every cumulative doubling of units produced, cost will fall by a constant percentage. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of data sources as of Jan. 9, 2024, which are available upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results. $0.01 $0.10 $1.00 $10.00 $100.00 $1,000.00 $10,000.00 $100,000.00\n 0 1 100 10,000 1,000,000 100,000,000$ / RCU(Log Scale)\nCumulative RCUs Produced(Millions) (Log Scale)AI Training Hardware CostActual $ / RCUPredicted $ / RCUARTIFICIAL INTELLIGENCEActual Compute",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 26
        }
    },
    {
        "page_content": "28\nGPT-3\nGPT-3GPT-3.5 TurboGPT-4-32k\nGPT-4 Turbo $- $0.01 $0.02 $0.03 $0.04 $0.05 $0.06 $0.07 $0.08\n11/18/20219/1/202211/6/20233/14/202311/6/2023Date of Price ChangeGPT-3 and GPT-4 API Inference CostsPer 1,000 Tokens\n92% Annualized Cost Decline86% Annualized Cost DeclineAs Production Use Cases Emerge, AI Focus Is Shifting To Inference CostsAfter focusing initially on LLM training cost optimization, researchers now are prioritizing inference costs. Based on enterprise scale use cases, inference costs seem to be falling at an annual rate of ~86%, even faster than training costs. Today, the inference costs associated with GPT-4 Turbo are lower than those for GPT-3 a year ago.GPT-4-32k:Context Window:32k TokensSpeed: 12 Tokens/SecGPT-4 Turbo:Context Window:128k Tokens, \u21914xSpeed: 44 Tokens/Sec, \u21914x\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of data sources, including Patel and Kostovic 2023, and ARK Investment Management LLC 2023, which are available upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.ARTIFICIAL INTELLIGENCE",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 27
        }
    },
    {
        "page_content": "29The Open-Source Community Is Competing With Private ModelsChallenging closed-source models from OpenAIand Google, the open-source community and its corporate champion, Meta, are democratizing access to generative AI. On balance, the performance of open-source models is improving faster than that of closed-source models, helped recently by models from China.\nNote: The chart\u2019s trendlines are fit to the most performant open- or closed-source models on 5-Shot MMLU (Massive Multitask Language Understanding) at the time of their release. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of data sources as of Jan. 9, 2024, which are available upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.ARTIFICIAL INTELLIGENCEGPT-2 1.5B (OpenAI, Fine-Tuned)GPT-3 (OpenAI, Fine-Tuned)Chinchilla 70B (Alphabet)PaLM 540B (Alphabet)Flan-PaLM (Alphabet)Claude 1.3 (Anthropic)GPT-4 (OpenAI)\nFlan-T5-XXL (Alphabet)LlaMA 65B (Meta)LlaMA 2 70B (Meta)Falcon 180 (TII, UAE)Yi-34B (01.AI, China)Qwen-72B (Alibaba, China) GPT-3.5 (OpenAI)PaLM-2 (Alphabet)Flan-PaLM 2 (Alphabet)Claude 2 (Anthropic)Grok-1 (X.ai)Gemini Ultra (Alphabet)\nMixtral 8x7B (Mistral)\n - 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n10/27/20185/15/201912/1/20196/18/20201/4/20217/23/20212/8/20228/27/20223/15/202310/1/20234/18/2024Absolute Log Error MMLU PerformanceOpen Source vs Private Models 5-Shot MMLU PerformancePrivateOpen SourcePrivate (Doesn't Outperform Previous Models on 5-Shot MMLU)Open Source (Doesn't Outperform Previous Models on 5-Shot MMLU)\nAverage Human Performance",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 28
        }
    },
    {
        "page_content": "30\n0102030405060708090100\nUSABO*Semifinal 2020Uniform BarExamSATAdvancedSommelierWinoGrande(commonsense)ScoreSelect GPT-4 Benchmark ResultsHuman Avg.GPT-4Language Model Performance Advances Require Nuanced TechniquesGPT-4 performs significantly better than the average human on standardized education tests, from the SAT to the Advanced Sommelier exam. Yet, it lags human-level capability in commonsense reasoning, as measured by WinnoGrande. Stanford\u2019s framework\u2014Holistic Evaluation of Language Models (HELM)\u2014is one of the most comprehensive, continuously updated evaluation methodologies, having tested over 80 models against a combination of 73 scenarios and 65 metrics. HELM Evaluation MetricsAccuracyComparison with ground truth dataCalibrationProbability distribution assessmentRobustnessStress testing with perturbed inputsFairnessPerformance across diverse groupsBiasAnalysis of decision patterns for skewToxicityDetection rate of harmful contentEfficiencyResource usage during task execution*USA Biology Olympiad, a prestigious national competition testing high school students in biology. Sources: ARK Investment Management LLC, 2024, based on data from Life Architect 2023 and Bomasani et al. 2023 as of Jan. 9, 2024. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.ARTIFICIAL INTELLIGENCE",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 29
        }
    },
    {
        "page_content": "31\n051015202530\nGPT-3Training TokensLlama 2Training TokensGPT-4Training TokensTokens Posted On XAnnual EstimateSpoken LanguageTokensAnnual EstimateTokens (Trillions)Leading LLM Training Set  vs. Language Token Stock\nGPT-3Training TokensLlama 2Training TokensGPT-4Training TokensSpoken Language TokensAnnual EstimateTokens Posted On XAnnual EstimateUntapped Data Sources\u202230 quadrillion words spoken annually\u2022Speech-to-text tools that capture the estimated 80+ trillion words spoken daily.\u2022Synthetic data that augments primary data. \u2022Autonomous taxis, trucks, drones, and other robots that generate large volumes of physical world data.Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of data sources as of Jan 9, 2024, which are available upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.~40 Quadrillion TokensComputing power and high-quality training data appear to be the primary contributors to model performance. As models grow and require more training data, will a lack of fresh data cause model performance to plateau? Epoch AI estimates that high-quality language/data sources like books and scientific papers could be exhausted by 2024, though a larger set of untapped vision data still exists.Will LLMs Run Out Of Data, Limiting Their Performance?ARTIFICIAL INTELLIGENCE",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 30
        }
    },
    {
        "page_content": "32\n0%5%10%15%20%25%\nBusiness EmailEmail MarketingIT Service ManagementCRMIT IncidentResponseSmart TransportationAnalyticsCloud-Based SecurityPlatforms% of Value CapturedTake-Rate Of Notable Enterprise Software SolutionsCustomized AI Offerings Should Enjoy More Pricing Power As open-source alternatives emerge and costs decline, software vendors tailoring AI to end-use applications should be able to monetize them more readily. Conversely, simple generative AI applications are likely to commoditize rapidly.\n\u2022Horizontal, Commoditized Tools\u2022< 5% Value Captured\u2022Example: AI Meeting Summaries\u2022Verticalized, Highly Differentiated Tools\u202220%+ Value Captured\u2022Example: Autonomous Ride-hailLow Value CaptureHigh Value Capture\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of data sources, including Sirohi 2023 and McKinsey & Co. 2023 as of Jan. 9, 2024, which are available upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.ARTIFICIAL INTELLIGENCE",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 31
        }
    },
    {
        "page_content": "33\n1%10%20%2.5$0.7T$7T$14T4.5$1.3T$13T$26T6.5$1.9T$19T$37TAccelerating The Growth Of Knowledge Worker Productivity Represents A Potential Multi-Trillion Dollar OpportunityAI Total Addressable Market (TAM) Forecast In 2030Software Vendor Value Capture % Of Productivity GainArtificial intelligence has the potential to automate most tasks in knowledge-based professions by 2030, dramatically increasing the average worker's productivity. Software solutions that automate and accelerate knowledge work tasks should be prime beneficiaries.Productivity Uplift (Multiple)CAGR = Compound Annual Growth Rate. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of data sources, including McKinsey & Co. 2023, which are available upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results. $- $2 $4 $6 $8 $10 $12 $14 $16 $18 $20$ TrillionsImpact of AI on Software Growth2.5x Uplift4.5x Uplift6.5x Uplift\n16% Annual Growth Rate46%CAGR54%CAGR\n34%CAGR201620232030 ForecastARTIFICIAL INTELLIGENCE",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 32
        }
    },
    {
        "page_content": "34\n34\nGrowing The Role Of Bitcoin In Investment PortfoliosBitcoin Allocation\nSources: ARK Investment Management LLC, 2024. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Yassine Elmandjra Director of Digital Assets David Puell Research Associate Research By:BIG IDEAS 2024",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 33
        }
    },
    {
        "page_content": "35\nSources: ARK Investment Management LLC, 2024 Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Important InformationBitcoin is a relatively new asset class, and the market for bitcoin is subject to rapid changes and uncertainty. Bitcoin is largely unregulated and bitcoin investments may be more susceptible to fraud and manipulation than more regulated investments. Bitcoin is subject to unique and substantial risks, including significant price volatility and lack of liquidity, and theft. Bitcoin is subject to rapid price swings, including as a result of actions and statements by influencers and the media, changes in the supply of and demand for bitcoin, and other factors. There is no assurance that bitcoin will maintain its value over the long term.The information provided on the following slides is based on ARK\u2019s research and is not intended to be investment advice. ARK researches the utility of bitcoin as an investment in order to determine its potential future value as presented on the following slides. This material does not constitute, either explicitly or implicitly, any provision of services or products by ARK, and investors should determine for themselves whether a particular investment management service is suitable for their investment needs. ARK strongly encourages any investor considering an investment in bitcoin or any other digital asset to consult with a financial professional before investing. All statements made regarding bitcoin are strictly beliefs and points of view held by ARK and are not recommendations by ARK to buy, sell or hold bitcoin. Historical results are not indications of future results.Important Terms and ConceptsThe research presented on the following slides contains some terms and concepts that may not be familiar to some readers, so below we provide explanations to help provide a basis for evaluating the research.\u2022Sharpe Ratio is a well-known and well-reputed measure of risk-adjusted return on an investment or portfolio, which indicates how well an investment performs in comparison to the rate of return on arisk-free investment such as U.S. government treasury bonds or bills. Sharpe ratio is calculated by first calculating the expected return on an investment portfolio or individual investment and then subtracting the risk-free rate of return. Normally, a higher Sharpe Ratio indicates good investment performance, given the risk, while a Sharpe Ratio less than 1 is considered less than good. Sharpe ratio is used in our research to determine, hypothetically, at what allocation percentage bitcoin would maximize the risk-adjusted return of an overall portfolio consisting of other commonly used asset classes.\u2022Efficient Frontier is the set of optimal portfolios that offer the highest expected return for a defined level of risk or the lowest risk for a given level ofexpected return. In other words, it graphically represents portfolios that maximize returns for the risk assumed. Portfolios that lie below the efficient frontier are considered sub-optimal because they do not provide enough return for the level of risk, and portfolios that cluster to the right of the efficient frontier are  also considered sub-optimal because they have a higher level of risk for the defined rate of return. The Efficient Frontier chart is used in this section to illustrate that the simulated portfolio we constructed with an allocation to bitcoin lies along the efficient frontier as compared to the portfolios consisting of single asset classes which would be considered sub-optimal. \u2022Compound Annual Growth Rate (\u201cCAGR\u201d) is the average annual amount an investment grows over a period of years assuming profits are reinvested during the period. In other words, it breaks an investment's total return over a number of years into a single average rate. CAGR is typically used to compare assets or portfolios over a longer time period by using an average as opposed to analyzing each year individually as returns from year to year may be uneven. We use CAGR in our research to determine the expected return of a portfolio or asset class over a period of years, typically 5 years.\u2022Standard Deviation is a measure of risk, or volatility, in a portfolio by indicating how much the investment will deviate from its expected return. An investment with higher volatility means a higher standard deviation, and therefore more risk. We use standard deviation to determine the amount of return that would be commensurate with certain levels of risk. BITCOIN ALLOCATION",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 34
        }
    },
    {
        "page_content": "36Digital Assets Like Bitcoin Are A New Asset ClassBitcoinCommodities (Including Gold)Real EstateBondsEquities(Including Emerging Markets) HistoryCreated during the Global Financial Crisis in 2009 by an individual or group under the pseudonym Satoshi NakamotoOrigins trace back thousands of years to commodities like gold being used as a store of valueEarliest known private property rights took shape in ancient Greece and RomeEarliest known bond was issued by the city of Venice in the 12thcentury, but the concept of debt/lending can be traced back to ancient MesopotamiaOrigins trace back to the 1600s with the establishment of the Amsterdam Stock ExchangeInvestabilityHighly liquid and accessible to anyone with access to the internet. Traded on crypto exchanges and through spot ETFsFairly liquid and accessible through physical coins and ETFs through banks and brokers.Illiquid, purchased directly or through REITsHighly liquid. Traded on bond markets, accessible through brokersHighly liquid. Traded on stock exchanges, accessible through brokersBasis Of ValueTied to demand for a decentralized, independent monetary system powered by open-source softwareTied to supply and demand, influenced by global economic conditionsTied to interest rates, property markets, and local economic factorsTied to interest rate policies and credit riskTied to expectations of future cash flowCorrelation Of ReturnsLow correlation with traditional asset classesTypically inversely correlated with asset classes, especially during economic uncertaintyTypically low to moderate correlation with stocks and bondsInversely correlated recently, but not always throughout economic history, with equitiesCorrelated with the health of global economy and market sentimentGovernanceDecentralized and community-driven, leveraging open-source software for decision makingGoverned by mining regulationGoverned by local and national property lawsGoverned by issuance terms set by government or corporationsGoverned by company management and regulated by government agenciesUse CasesScarce digital store of value, its currency native to the internetIndustrial activity, wealth preservation, and hedgingPersonal residence, rental incomeFixed income investment, with regular interest payments and return of principal at maturityCompany ownership, often with voting rights and dividendsAccording to ARK\u2019s research, bitcoin has emerged as an independent asset class worthy of a strategic allocation in institutional portfolios.\nSources: ARK Investment Management LLC, 2024 For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security or cryptocurrency. Past performance is not indicative of future results.BITCOIN ALLOCATION",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 35
        }
    },
    {
        "page_content": "37\n-10%0%10%20%30%40%50%60%70%80%\nLast 7 YearsLast 6 YearsLast 5 YearsLast 4 YearsLast 3 YearsCAGR (%)Annualized Returns Across Major Asset Classes*BitcoinGoldCommoditiesReal EstateBondsEquitiesEmerging MarketsBitcoin Has Outperformed Every Major Asset Over Longer Time HorizonsDuring the last seven years, bitcoin\u2019s annualized return has averaged ~44%, while that of other major assets has averaged 5.7%.\nAverage Bitcoin CAGR: ~44%Average Asset Class CAGR: 5.7%\n*Asset classes are represented by the following instruments: SPDR S&P 500 ETF Trust (SPY, equities), Vanguard Total Bond Market Index Fund Investor Shares (VBMFX, bonds), Vanguard Real Estate Market Index Fund Investor Shares (VGSIX, real estate), SPDR Gold Trust (GLD, gold), iShares S&P GSCI Commodity-Indexed Trust ETF (GSG, commodities), and Vanguard Emerging Markets Stock Index Fund Investor Shares (VEIEX, emerging markets). The performance used to represent each asset class reflects the net asset value (NAV) performance of each ETF/fund for the time periods shown. **\u201cLast 6 Years\u201d includes 2018, 2021, and 2022; \u201cLast 3 Years\u201d includes 2021 and 2022, all years of market downturn or relatively low returns for bitcoin. Sources: ARK Investment Management LLC, 2024, based on data and calculation from PortfolioVisualizer.com, with bitcoin price data from Glassnode, as of December 31, 2023. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security or cryptocurrency. Past performance is not indicative of future results. BITCOIN ALLOCATION\n****",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 36
        }
    },
    {
        "page_content": "38Generally, Bitcoin Investors With A Long-Term Time Horizon Have Benefited Over TimeBitcoin\u2019s volatility can obfuscate its long-term returns. While significant appreciation or depreciation can occur over the short term, a long-term investment horizon has been key to investing in bitcoin.Instead of \u201cwhen,\u201d the better question is \u201cfor how long?\u201dHistorically, investors who bought and held bitcoin for at least 5 years have profited, no matter when they made their purchases.Bitcoin Realized Returns\u201cTime, Not Timing\u201d*\n*Adage first put forth in this configuration by Mizuho Financial Group. Sources: ARK Investment Management LLC, 2024, based on data from Glassnode as of December 31, 2023. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security or cryptocurrency. Past performance is not indicative of future results.\n500    1,000    1,500    2,000    2,500    3,000    3,500    4,000    4,500Days HeldDate of Investment2011201220132014201520162017201820192020202120222023BITCOIN ALLOCATION",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 37
        }
    },
    {
        "page_content": "39\nBitcoinGoldCommoditiesReal EstateBondsEquitiesEmerging MarketsBitcoin0.20.10.40.260.410.23Gold0.2-0.030.280.460.260.34Commodities0.1-0.030.42-0.120.430.5Real Estate0.40.280.420.570.860.68Bonds0.260.46-0.120.570.480.46Equities0.410.260.430.860.480.73Emerging Markets0.230.340.50.680.460.73AVERAGE0.270.250.210.530.350.530.49Bitcoin's Correlation To Traditional Assets Is Low\nHigh correlation: coefficient value lies between \u00b1 0.66 and \u00b11 Moderate correlation: coefficient value lies between \u00b1 0..4 and \u00b1 0.66 Low correlation: coefficient value lies below \u00b1 0.4Asset Class Correlation Matrix1,2(12-Month As Of December 2023)Historically, bitcoin\u2019s price movements have not correlated highly to those of other asset classes. During the past five years, the correlation of bitcoin\u2019s returns relative to traditional asset classes has averaged only 0.27.\n[1] A correlation of 1 connotes that assets perfectly move in tandem; 0 means their movement is completely independent from each other; -1 suggests that they move in perfectly opposite directions. [2] Asset classes are represented by the following instruments: SPDR S&P 500 ETF Trust (SPY, equities), Vanguard Total Bond Market Index Fund Investor Shares (VBMFX, bonds), Vanguard Real Estate Market Index Fund Investor Shares (VGSIX, real estate), SPDR Gold Trust (GLD, gold), iShares S&P GSCI Commodity-Indexed Trust ETF (GSG, commodities), and Vanguard Emerging Markets Stock Index Fund Investor Shares (VEIEX, emerging markets). The performance used to represent each asset class reflects the net asset value (NAV) performance of each ETF/fund for the time periods shown. Sources: ARK Investment Management LLC, 2024, based on data and calculation from PortfolioVisualizer.com, with bitcoin price data from Glassnode, as of December 31, 2023. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security or cryptocurrency. Past performance is not indicative of future results. BITCOIN ALLOCATION",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 38
        }
    },
    {
        "page_content": "40Bitcoin Could Play An Important Role In Maximizing Risk-Adjusted ReturnsFocused on the volatility and return profiles of traditional asset classes, ARK\u2019s research suggests that a portfolio seeking to maximize risk-adjusted returns1 would have allocated 19.4% to bitcoin in 2023.Simulated Optimal Portfolio Allocation Targets By Year2,3 (Rolling 5-Year As Of End Of Every Year6)BitcoinGoldCommoditiesBondsEquities20150.5%0%0%82.5%16.9%20160.9%0%0%62.1%36.9%20170.9%0%0%58.7%40.3%20182.4%0%0%77.3%20.2%20193.9%1.4%0%70.4%24.2%20204.3%4.1%0%75.6%15.8%20214.7%7.3%0%65.3%22.6%20226.2%52.8%9.1%0%31.8%202319.4%40.7%9.6%0%30.3%[1] Measurement of returns of an asset against its risk (in this case, volatility). [2] Real Estate and Emerging Markets are calculated out of these tangency portfolios given their low participation in maximizing risk-adjusted returns relative to the other asset classes included in this table. [3] Asset classes are represented by the following instruments: SPDR S&P 500 ETF Trust (SPY, equities), Vanguard Total Bond Market Index Fund Investor Shares (VBMFX, bonds), Vanguard Real Estate Market Index Fund Investor Shares (VGSIX, real estate), SPDR Gold Trust (GLD, gold), iShares S&P GSCI Commodity-Indexed Trust ETF (GSG, commodities), and Vanguard Emerging Markets Stock Index Fund Investor Shares (VEIEX, emerging markets). The performance used to represent each asset class reflects the net asset value (NAV) performance of each ETF/fund for the time periods shown. [4] This simulation, also known as \u201cefficient frontier\u201d, is a set of theoretical investment portfolios expected to provide the highest returns at multiple levels of risk. [5] The dots under the efficient frontier in the chart represent portfolios comprised of a single asset class. [6] 5 years were used since, in our view, they represent a sample of a long-term time horizon. Sources: ARK Investment Management LLC, 2024, based on data and calculation from PortfolioVisualizer.com, with bitcoin price data from Glassnode, as of December 31, 2023. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security or cryptocurrency. Past performance is not indicative of future results. \nExpected ReturnStandard Deviation2023 Simulated Portfolio Optimization3,4,5Based On Monthly Asset Class Returns (No Limit, Rolling 5-Year6)\nBondsCommoditiesEquitiesGoldReal EstateEmerging MarketsBitcoinBitcoin19.4%Equities30.2%Gold40.7%Commodities9.6%2023 Tangency PortfolioHigh\nLowHighBITCOIN ALLOCATION",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 39
        }
    },
    {
        "page_content": "41On A 5-Year Rolling Basis, An Allocation To Bitcoin Would Have Maximized Risk-Adjusted ReturnsDuring The Past 9 YearsAccording to our analysis, in 2015, the optimal allocation to maximize risk-adjusted returns1\u2014on a 5-year time horizon3\u2014would have been 0.5%. Since then, on the same basis, the average allocation to bitcoin would have been 4.8%, and in 2023 alone, 19.4%.\nOptimal Allocation: 4.8% On Average\n[1] Risk-adjusted returns given by the Sharpe ratio, which divides expected returns minus the risk-free rate by the standard deviation of the asset. [2] For asset class representation in this calculation, please refer to the previous slide. [3] 5 years were used since, in our view, they represent a sample of a long-term time horizon.. Sources: ARK Investment Management LLC, 2024, based on data and calculation from PortfolioVisualizer.com, with bitcoin price data from Glassnode, as of December 31, 2023. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security or cryptocurrency. Past performance is not indicative of future results. 0.5%0.9%0.9%2.4%3.9%4.3%4.7%6.2%19.4%\n0%5%10%15%20%25%\n201520162017201820192020202120222023Allocation Into Bitcoin By Year To Maximize Risk-adjusted Returns2(Maximization By Sharpe Ratio, Rolling 5-Year Time Horizon3,4)BITCOIN ALLOCATION",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 40
        }
    },
    {
        "page_content": "42\n$0$500,000$1,000,000$1,500,000$2,000,000$2,500,000\n1% Allocation4.8% Average Allocation(Average Maximum Sharpe Ratio 2015-2023,Rolling 5-Year Time Horizon)19.4% Allocation(Maximum Sharpe Ratio 2023,Rolling 5-Year Time Horizon)Price Potential (USD)Hypothetical Impact of Institutional Investment On The Price Of Bitcoin1,2What Would Be The Impact Of An Optimal Allocation Into Bitcoin?\n~$120,000~$2,300,000\n~$550,000\n[1] This chart was calculated by dividing each percentage allocation of the estimated global investable asset base of $250 trillion USD (Chung 2021) by the fully diluted expected bitcoin supply of 21 million. When dividing the investable asset base by the bitcoin supply of 19.5 million as of 12/31/2023, the price potential increases to ~$127k (1% allocation), ~$615k (4.8% allocation), and ~$2.5 million (19.4% allocation). [2] Asset classes are represented by the following instruments: SPDR S&P 500 ETF Trust (SPY, equities) and Vanguard Total Bond Market Index Fund Investor Shares (VBMFX, bonds). The performance used to represent each asset class reflects the net asset value (NAV) performance of each ETF/fund for the time periods shown. Sources: ARK Investment Management LLC, 2024, based on data and calculation from PortfolioVisualizer.com, with bitcoin price data from Glassnode, as of December 31, 2023. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security or cryptocurrency. Past performance is not indicative of future results. Allocations from the $250 trillion global investable asset base into bitcoin would have a significant impact on the price. BITCOIN ALLOCATION",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 41
        }
    },
    {
        "page_content": "43\n43BitcoinIn 2023\nSources: ARK Investment Management LLC, 2024. Information as of December 31, 2023. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Demonstrating Resilience And Recovery After Challenges In 2022Yassine Elmandjra Director of Digital Assets David Puell Research Associate Research By:BIG IDEAS 2024",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 42
        }
    },
    {
        "page_content": "44\n$15,000$20,000$25,000$30,000$35,000$40,000$45,000\nJan-23Feb-23Mar-23Apr-23May-23Jun-23Jul-23Aug-23Sep-23Oct-23Nov-23Dec-23Bitcoin Price, 2023In 2023, Bitcoin\u2019s Price Surged 155%, Increasing Its Market Cap To $827 Billion\nSources: ARK Investment Management LLC, 2024, based on data from Glassnode as of December 31, 2023. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security or cryptocurrency. Past performance is not indicative of future results.Genesis Files For Bankruptcy\nCoinbase Unveils Base ProtocolSilicon Valley Bank And Signature Bank Collapse\nBitcoin Transactions Reach Record High As Ordinals SurgeARK, 21Shares File For Bitcoin ETFSEC Charges Coinbase For Operating As An Unregistered Securities Exchange\nBlackRock Files For Bitcoin ETFRipple Labs Notches Landmark Win In SEC CasePayPal Launches USD StablecoinCourt Rules SEC Must Review Grayscale\u2019s Bitcoin ETF Bid\nEl Salvador Launches First Government Backed Bitcoin Mining PoolSam Bankman-Fried Found Guilty Of Seven CountsBinance CEO CZ Steps Down And Pleads Guilty In Settlement With DOJBITCOIN IN 2023",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 43
        }
    },
    {
        "page_content": "45\n0.1110100\n0.010.1110100100010000100000\nJan-11Jan-12Jan-13Jan-14Jan-15Jan-16Jan-17Jan-18Jan-19Jan-20Jan-21Jan-22Jan-23\nOn-Chain Market Mean Ratio (AVIV) And ThresholdPrice And On-Chain Market Mean (USD)Bitcoin\u2019s Break Above Its True Market Mean Signals The Onset Of A Bull MarketBTC PriceOn-Chain Market MeanOn-Chain Market Mean Ratio (AVIV)Risk-on/Risk-off ThresholdAn original ARK metric, the on-chain market mean has been a reliable demarcation point between risk-on and risk-off bitcoin markets. Historically, when the price of bitcoin crosses above the market mean, it typically indicates the early stages of a bull market.Bitcoin\u2019s Price Crossed Above Its On-Chain Market Mean For The First Time In ~4 Years\nSources: ARK Investment Management LLC, 2024, based on data from Glassnode as of December 31, 2023. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security or cryptocurrency. Past performance is not indicative of future results.BITCOIN IN 2023",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 44
        }
    },
    {
        "page_content": "462023 Provided Important Answers To The Crises In 2022Entity2022 CrisisLuna, USTLUNA\u2019s collapse led high profile hedge fund Three Arrows Capital (3AC) into a liquidity crisis, forcing it into bankruptcy.Crypto lending platform Celsius froze withdrawals and then filed for bankruptcy.After Coindesk exposed the fraudulent financial entanglement between trading firm Alameda and FTX, FTX suffered a bank run and collapsed.BlockFi\u2019s exposure to FTX forced it into bankruptcy.With significant loans to 3AC, crypto lender Genesis declared bankruptcy.Algorithmic stablecoin UST collapsed, causing a significant sell-off in its sister cryptocurrency, LUNA, erasing over 60 billion USD in market value.*The Monetary Authority of Singapore banned 3AC\u2019s co-founders from capital markets activity for nine years, and a court in the British Virgin Islands froze their assets.A bankruptcy court approved a restructuring plan for Celsius that will return assets to customers and establish a new company focused on mining and staking. CEO Alex Mashinsky faces criminal charges for allegedly misleading customers. The Southern District of New York convicted Sam Bankman-Fried on seven counts of fraud related to the collapse of FTX. A bankruptcy court granted the FTX estate approval to sell its assets.BlockFi received court approval to liquidate, with partial in-kind repayment to creditors. Crypto lender Genesis reached a settlement with parent company DCG, involving $620 million in repayments. The SEC is suing Genesis for selling unregistered securities.Founder Do Kwon was arrested and faces eight indictments in Manhattan\u2019s U.S. District Court, while his startup, Terraform Labs, faces SEC civil charges for orchestrating a multi-billion-dollar securities fraud.2023 ResolutionThree Arrows CapitalCelsius NetworkFTXBlockFiGenesis*This data point is sourced from Corva 2022. Sources: ARK Investment Management LLC, 2024. Information as of December 31, 2023. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security or cryptocurrency. Past performance is not indicative of future results.BITCOIN IN 2023",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 45
        }
    },
    {
        "page_content": "47\n15,00020,00025,00030,00035,00040,00045,000\n60708090100110120\nJanFebMarAprMayJunJulAugSepOctNovDec\nBitcoin Price (USD)KBW Regional Banking IndexAs Regional Banks Collapsed, Bitcoin\u2019s Price Appreciated ~40%KBW Regional Banking IndexBitcoin Price (USD)Bitcoin Was A Safe Haven During The Regional Banking CollapseIn early 2023, during the historic collapse of US regional banks, bitcoin\u2019s price appreciated more than 40%, highlighting its role as a hedge against counterparty risk.Silicon Valley, Signature, Silvergate, and First Republic Bank collapsed during the regional banking crisis\nSources: ARK Investment Management LLC, 2024, based on data from Bloomberg and Glassnode as of December 31, 2023. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security or cryptocurrency. Past performance is not indicative of future results.BITCOIN IN 2023",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 46
        }
    },
    {
        "page_content": "48The Surge In Inscriptions Signaled A Role For The Bitcoin Network Beyond Transaction SettlementLaunched in January 2023, Bitcoin Inscriptions introduced a unique numbering system for each satoshi, the smallest unit of bitcoin, based on its position in the blockchain. Each satoshi is identifiable and immutable, allowing users to inscribe their data, images, or text.Unlike other blockchains that require smart contracts for NFTs1, Bitcoin Inscriptions are on the base layer of the Bitcoin blockchain.Ordinals2 have sparked debate about the impact of Inscriptions on transaction sizes and block space. In our view, Ordinals are a product of the free market and represent healthy innovation on Bitcoin.0102030405060\nJan-23Apr-23Jul-23Oct-23Cumulative Inscriptions (Millions)Bitcoin Inscriptions3Audio, Image, Video, OtherText/BRC-20\n[1] Short for Non-Fungible Token, it is tokenized metadata via unique identification codes recorded on a blockchain. [2] Refers to the creation of non-fungible tokens (NFTs) in the Bitcoin network by making Inscriptions, where metadata such as images or videos are attached to individual satoshis (the smallest unit of account). [3] BRC-20: A tokenstandard that enables the minting and transaction of fungibletokensvia the Ordinals protocol on the Bitcoin network. Sources: ARK Investment Management LLC, 2024, based on data from Glassnode as of December 31, 2023. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security or cryptocurrency. Past performance is not indicative of future results.BITCOIN IN 2023",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 47
        }
    },
    {
        "page_content": "49Bitcoin\u2019s Fundamentals Didn\u2019t Skip A Beat During The Crisis in 2022 And Continued Apace In 2023\nLong-Term Holder Supply 4(BTC, Millions)Bitcoin Network StatsPriceHash Rate2(EH/s3, 14-Day Average)BTC Addresses With Non-Zero Balance3 (Millions)Market Cost Basis1($ Billions)1Supply Of BTC Last Moved >1 Year Ago (%)2023$42,225$427.7523.270.2%51.714.82022$16,553$380.7254.366.5%43.314.1Transaction Count5 (Non-Inscriptions Related, Thousands)367.5256.20100200300400500600\nApr 22Jul 22Oct 22Jan 23Apr 23Jul 23Oct 23Exahashes/sBitcoin\u2019s Hash Rate2, A Proxy for Network Security, Hit An All-Time High In 2023\n[1] The on-chain volume-weighted average price of the market, calculated by aggregating the value of all bitcoins in circulation at the time when they last moved. Also known as realized price or realized cap. [2] The estimated computational power mining within and providing security to the Bitcoin network. [3] Number of addresses in the Bitcoin network with a balance larger that zero. [4] Bitcoin supply last moved 155 days ago or more, the threshold at which the possibility of a bitcoin remaining unmoved increases drastically. [5] Number of transactions between two addresses of the Bitcoin network. Sources: ARK Investment Management LLC, 2024, based on data from Glassnode as of December 31, 2023. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security or cryptocurrency. Past performance is not indicative of future results.BITCOIN IN 2023",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 48
        }
    },
    {
        "page_content": "50CME*Surpassed Binance As The World\u2019s Largest Bitcoin Futures Exchange \nAfter outpacing the CME, FTX\u2019s market share of open interest collapsed in late 2022.CME\u2019s open interest surpassed Binance\u2019s for the first time.As the demand for more regulated and secure infrastructure increased following the contagion in 2022, bitcoin\u2019s market dynamics shifted more to the US.\n*Short for Chicago Mercantile Exchange. Sources: ARK Investment Management LLC, 2024, based on data from Glassnode as of December 31, 2023. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security or cryptocurrency. Past performance is not indicative of future results.$0$1$2$3$4$5\nSep 22Oct 22Nov 22Dec 22Jan 23Feb 23Mar 23Apr 23May 23Jun 23Jul 23Aug 23Sep 23Oct 23Nov 23Dec 23$ BillionsBitcoin Futures Open Interest Hit a Record $4.5 Billion on the CMECMEBinanceFTXBITCOIN IN 2023",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 49
        }
    },
    {
        "page_content": "51Bitcoin Is Evolving Into A Reliable Risk-Off Asset\nBitcoin operates on a decentralized network, independent of any single entity, government, or central bank. Its distributed, open-source nature protects it against arbitrary asset seizure and counterparty risk.Despite its short-term volatility, bitcoin has delivered significant long-term price appreciation. By design, scarcity increases the probability of capital preservation. Bitcoin's historically low correlation with traditional asset classes is increasing its role as a source of diversification. Adding a non-correlated asset to portfolios potentially increases returns per unit of risk and provides a buffer against market downturns.Global investors can access and trade bitcoin 24/7, which is increasingly important in times of risk-off uncertainty.Bitcoin\u2019s supply will be capped at 21 million coins. As with gold, scarcity characterizes bitcoin\u2019s role as a safe-haven asset.Safety & Capital PreservationDiversificationLong-TermInvestment HorizonLiquidity &AccessibilityInflation HedgeWith increasing macroeconomic uncertainty and less trust in traditional \u201dflights to safety,\u201d bitcoin has become a viable alternative.Evaluating Bitcoin As A Risk-Off Asset\nSources: ARK Investment Management LLC, 2024. Information as of December 31, 2023. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security or cryptocurrency. Past performance is not indicative of future results.BITCOIN IN 2023",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 50
        }
    },
    {
        "page_content": "52Major Catalysts Await Bitcoin In 2024Bitcoin HalvingThe Bitcoin halving occurs approximately every 4 years, cutting the reward for mining new bitcoin blocks in half. Historically, each halving event has coincided with the beginnings of a bull market. Expected in April 2024, this halving will reduce bitcoin\u2019s inflation rate from ~1.8% to ~0.9%.Bitcoin Spot ETF LaunchOn January 11, 2024, the launch of spot bitcoin ETFs set the stage for Bitcoin\u2019s growth, by offering investors a more direct, regulated, and liquid way to gain exposure. Bitcoin spot ETFs are traded on major stock exchanges, allowing investors to buy and sell shares through their existing brokerage accounts, and should reduce the learning curve and operational complexities associated with direct investments in bitcoin.Institutional AcceptanceThanks to its continued resilience and performance, the shift in perception of bitcoin\u2014from a speculative instrument to a strategic investment in a diversified portfolio\u2014should characterize its evolution in 2024. Exemplifying this evolution, Larry Fink, CEO of BlackRock, has shifted his stance from bitcoin skepticism to its potential as a \"flight to quality.\"Regulatory DevelopmentsThe bankruptcies of FTX and Celsius have advanced the push for more transparent and open global crypto regulation, including the potential passage of a US bill establishing a regulatory framework for cryptocurrencies, and the implementation of Europe's Markets in Crypto-Assets (MiCA) regulation, which mandates licensing for crypto wallet providers and exchanges in the EU.\n01020\n20 0920 1520 2120 27Units Of Bitcoin (Million)Bitcoin\u2019s Circulating SupplyBTC Supply (units)BTC Supply CapExpected BitcoinIssuanc eSources: ARK Investment Management LLC, 2024, based on data from Glassnode as of December 31, 2023. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security or cryptocurrency. Past performance is not indicative of future results.BITCOIN IN 2023",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 51
        }
    },
    {
        "page_content": "53\n53SmartContractsPowering The Internet-NativeFinancial SystemSources: ARK Investment Management LLC, 2024. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Research By:Frank DowningDirector of Research, Next Generation InternetBIG IDEAS 2024",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 52
        }
    },
    {
        "page_content": "54Deployed on public blockchains,smart contracts offer a global, automated, and auditable alternative to rent-seeking intermediaries and legacy financial infrastructure.In the aftermath ofthe \u201ccrypto crisis\u201d in 2022, several digital asset solutions gained traction, including stablecoins,tokenized treasury funds, and scaling technologies.According to ARK\u2019s research, as the value of on-chain financial assets increases, the market value associated with decentralized applications could scale 32% at an annual rate, from $775billion in 2023 to $5.2 trillion in 2030.\nPublic blockchains are digital asset ledgers openly available for participants to access and are not controlled by a single entity. Smart Contracts are programs that exist on a blockchain and execute computer code when specific conditions are met. Sources for stablecoin usage, treasury issuance, and core development are provided in the corresponding slides that follow. Sources: ARK Investment Management LLC, 2024, based on a range of external sources, which are available upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.SMART CONTRACTS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 53
        }
    },
    {
        "page_content": "55Smart Contracts Are The Foundation Of The Internet Financial SystemIn their infancy, smart contracts are powering a novel financial system that is native to the internet. Ignited by Ethereum, the largest smart contract blockchain, multiple networks are supporting on-chain activity and vying for market share.\nNOTE: Networks represented are smart contract Layer 1 blockchains with >$10 million in 2023 transaction fees. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which are available upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Transaction FeesTop 6 Smart Contract Networks, 2023EthereumTronBNB ChainAvalancheSolanaPolygon PoSSmart ContractNetworkMarket Value2023e Price Performance2023Ethereum$           274 billion+90%BNB Chain$             49 billion+28%Solana$             44 billion+924%Avalanche$             14 billion+254%Tron$               9 billion+120%Polygon PoS$               9 billion+28%$3.7BillionSMART CONTRACTS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 54
        }
    },
    {
        "page_content": "56StablecoinsHighlight The Value Proposition Of Smart ContractsGiven hyperinflation in emerging markets and an increase in global instability, the demand for stablecoinsoffering digital access to the US dollar is soaring. During the past three years, the number of daily active stablecoinaddresses globally has increased at an annual rate of 93%, from 171 thousand to 1.2 million. In 2023, stablecointransfer volumes surpassed those of Mastercard.\n$2 $9 $10 $15 \n$0$2$4$6$8$10$12$14$16\nPaypalMastercardStablecoinsVisaTotal Transfer Volume, 2023(Trillions)\nNOTE: Stablecoin Daily Active Addresses are averaged for each month displayed in chart. Transfer volume estimates are used where Q4 2023 data is not yet available at time of publication. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which are available upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.USD\n - 200,000 400,000 600,000 800,000 1,000,000 1,200,000 1,400,000\nJan-21Apr-21Jul-21Oct-21Jan-22Apr-22Jul-22Oct-22Jan-23Apr-23Jul-23Oct-23StablecoinDaily Active AddressesTronBNB ChainEthereumAvalanche C-ChainPolygon PoSSolanaETH L2sSMART CONTRACTS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 55
        }
    },
    {
        "page_content": "57Traditional Financial Assets Are Moving On-ChainTokenization allows treasurers to track, trade, and collateralize funds more easily on public blockchains than in traditionalfinancial markets. In 2023, tokenized treasury funds jumped more than 7-fold to $850 million. Early funds launched on the Stellar blockchain, but Ethereum became the largest market for tokenized treasuries in 2023.\n $- $100 $200 $300 $400 $500 $600 $700 $800 $900\n1-Jan-231-Feb-231-Mar-231-Apr-231-May-231-Jun-231-Jul-231-Aug-231-Sep-231-Oct-231-Nov-231-Dec-231-Jan-24MillionsValue Of Tokenized Treasury FundsStellarEthereumPolygonSolana\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which are available upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.SMART CONTRACTS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 56
        }
    },
    {
        "page_content": "58\nPoS MergeWithdrawals Enabled\n - 5 10 15 20 25 30 35\nDec-20Mar-2 1Jun-21Sep-21Dec-21Mar-2 2Jun-22Sep-22Dec-22Mar-2 3Jun-23Sep-23Dec-23Number Of Ether (Millions)Staked Ether \nLaunchFailed validator upgradeBlock propogation bugTransaction spamTransaction spamSoftware bugNode misconfigurationDeduplication error\n050100150200250300350\nMar-2 0Jun-20Sep-20Dec-20Mar-2 1Jun-21Sep-21Dec-21Mar-2 2Jun-22Sep-22Dec-22Mar-2 3Jun-23Sep-23Dec-23Number Of DaysSolana Network UptimeDevelopers RefinedProtocols During The Bear MarketIn the face of crises and their aftermath in 2022, core developers advanced technical roadmaps and hardened protocols to support the next bull market. Ethereum moved successfully to Proof-of-Stake (PoS)*consensus, and Solana hit a new record for continuous uptime.\n*Proof-of-Stake is a method of securing public blockchains, in which network participants who wish to validate transactions on the network pledge or \u201cstake\u201d their assets at risk of loss if they fail to operate within the network\u2019s rules. Chart data end 12/31/23. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which are available upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.SMART CONTRACTS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 57
        }
    },
    {
        "page_content": "59Layer 2 Networks Have Scaled Transactions In The Ethereum EcosystemSince early 2021, more than 20 Layer 2 (L2)*networks have launched, enabling Ethereum to scale average daily transactions at lower fees by 4x. Despite their early success, most L2 networks are controlled centrally. The proliferation of L2s has complicated user and developer experiences.\n*L2 networks aggregate transactions and settle the resulting state changes to a base-layer smart contract network like Ethereum, typically at higher throughput and lower cost compared to the base network. L2 transaction count is based on data available on Artemis Dashboard: Arbitrum, Base, Linea, Optimism, Polygon zkEVM, Scroll, StarkNet, zkSync Era, Zora Network. Chart data end 12/31/23. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which are available upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.012345\nQtr12019Qtr2Qtr3Qtr4Qtr12020Qtr2Qtr3Qtr4Qtr12021Qtr2Qtr3Qtr4Qtr12022Qtr2Qtr3Qtr4Qtr12023Qtr2Qtr3Qtr4Transaction Count (Millions)Average Daily TransactionsEthereum MainnetLayer 2 NeworksSMART CONTRACTS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 58
        }
    },
    {
        "page_content": "60Lower Costs Are Boosting On-Chain Engagement As transaction costs have declined, on-chain engagement\u2014as measured by the ratio of daily active addresses (DAUs) to monthly active addresses (MAUs)\u2014has increased. \nNote: DAU / MAU traditionally refers to a measure of unique users. For this analysis, we are using a measure of unique addresses as an approximation for users. They are correlated but not equivalent. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which are available upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.$6.28 $0.27 $0.13 $0.16 $0.003 $0.28 \n $0.00 $0.01 $0.10 $1.00 $10.00\n0%2%4%6%8%10%12%\nEthereumOptimismArbitrumBaseSolanazkSync\nAverage Fee (2023)DAU As A Percent Of MAUEngagement Relative To Transaction FeesDAU / MAUSMART CONTRACTS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 59
        }
    },
    {
        "page_content": "61Monolithic Chains Like Solana Offer An Alternative To Vertical ScalingSmart contract network designs offer tradeoffs. By prioritizing base-layer decentralization, the Ethereum ecosystem became more complex as it scaled. By prioritizing scalability in a single layer, Solana maintained a simple architecture for users and app developers and has gained traction. \nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which are available upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.L1 EthereumL1Solana+Simplifies the environment for developers and users-Raises L1 validation costs+Maximizes composability and interoperability-Potentially requires L2s to maximally scale+ Lowers fees and increases throughput for base layer transactions-Requires apps depend L1 execution environmentL2OptimismL2Base\u2026DAPP 1DAPP 2DAPP 3\u2026DAPP 1DAPP 2DAPP 3\u2026\n+Minimizes L1 validation cost -Requires asset bridging between L1 and L2, fragmenting liquidity+Supports multiple approaches for scaling, encouraging flexibility & innovation-Increases complexity for developers and users+Leverages the network effect and liquidity advantages of Ethereum mainnet-Introduces additional reliability and security considerations across L2sL2ArbitrumVertical ScalingHorizontal ScalingSMART CONTRACTS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 60
        }
    },
    {
        "page_content": "62Smart Contracts Could Collapse The Cost Of Financial Services\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which are available upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.20002020Global Financial Services Revenue\n20002020TrillionsValue Of Global Financial Assets 3.6x3.3x$510T\n$140T$17T\n$5T3.3% agg. take rateActivityCost To CurrentSystemCost LoweringSolutionKnow Your Customer Verification$1,500-$3,000+ per individual verificationUnified digital identity verifiable across institutionsNasdaq Listing Fee$270k per listing + $52-$180k annuallyDirect DEX listing with global distributionGlobal Anti-Money Laundering Compliance$274 billion cost to the global financial system annuallyAuditable provenance of funds on global ledgerEconomic Impact of Financial Regulatory ComplianceSMART CONTRACTSThe value of financial assets globally ballooned from $140 trillion in 2000 to $510 trillion in 2020, thanks to a combinationofglobal economic growth, increased financialization, and expanding equity multiples. The operating cost of the global financial system increased in tandem with the value of financial assets. At $20 trillion in total annual revenue, the aggregate financial services industry\u2019s take rate has been 3.3% relative to the value of all financial assets. Smart contracts could lower this dragon the economy materially.",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 61
        }
    },
    {
        "page_content": "63Smart Contract Networks Could Generate Fees Of $450 Billion In 2030\n$775 $5,300\n $- $1,000 $2,000 $3,000 $4,000 $5,000 $6,000\n20232030EstimateTrillionsSmart Contract Protocol Market Value\n37% CAGR$1 $20 $11 $8 $450 \n $- $50 $100 $150 $200 $250 $300 $350 $400 $450 $500\n20202021202220232024202520262027202820292030BillionsGross Smart Contract Fee RevenueActualsForecast\n78% CAGR1SMART CONTRACTSSmart contracts could facilitate the origination, ownership, and management of on-chain assets for a fraction of traditional financial costs. If financial assets were to migrate to blockchain infrastructure at a rate similar to the adoption of the internet, and the take rates associated with decentralized financial services were a third those of traditional financial services, smart contracts could generate annual fees of more than $450 billion and create more than $5 trillion in market value, increasing at compound annual rates of 78% and 32%, respectively, through 2030.\n12020-2021 data approximated using top 20 all-time fee generating protocols from Token Terminal Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which are available upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 62
        }
    },
    {
        "page_content": "64\n64Digital Consumers\nSources: ARK Investment Management LLC, 2024. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Transitioning Toward Digital LeisureBIG IDEAS 2024Nicholas Grous Associate Portfolio Manager Andrew Kim Analyst Research By:",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 63
        }
    },
    {
        "page_content": "65\n\u2022Connected TV (CTV) Advertising should grow 17% at a compound annual rate, from $25 billion in 2023 to $73 billion in 2030.\u2022Social Commerce should grow 32% at an annual rate, from $730 billion today to over $5 trillion in 2030.\u2022Sports Betting should remain turbocharged by the legalization of online/mobile betting.\u2022AI-assisted Video Game Creation is the new wave in gaming, building on user-generated platforms like Roblox, which has hostedmore than ~470 million experiences globally\u201452x the combined number of PC, consoles, and mobile games.\u2022AI-enabled Hardware could redefine personal wearable computing, especially if virtual reality (VR) continues to face challenges.According to ARK\u2019s research, spending on digital leisure should take share from physical options and grow 19% at an annual rate during the next seven years, from $7 trillion in 2023 to $23 trillion in 2030. Several trends are accelerating the shift to digital leisure: DIGITAL CONSUMERS\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 64
        }
    },
    {
        "page_content": "66Artificial Intelligence Could Lower The Average Workweek And Stimulate Digital ConsumptionDuring the 80 years between the Second Industrial Revolution through the end of World War II, labor hours per worker decreased 0.5% at an annual rate globally. Generative AI could lower labor hours per worker by 1.3% on average, from 5.0 hours per day in 2022 to 4.5 hours in 2030.*As a result, consumers might devote more time to online entertainment, potentially increasing the share of total waking hoursspent online from 40% in 2023 to 49% in 2030.\n*To calculate global daily working hours, we divide total annual hours of labor per worker by the total days of the year. **The chart illustrating daily allocation of online vs. offline time captures total daily waking hours, including those allocated to labor or education. The chart captures hours generated by internet users only. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.0. 01.02.03.04.05.06.07.08.09.010.0\n18701910195019902030Daily HoursGlobal Labor Hours Per Worker Per Day* \n2nd Industrial RevolutionGenerativeAI Revolution-0.5% per year-1.3% per year31%39%49%0%20%40%60%80%100%\n201020202030Share Of Waking HoursGlobal Online And Offline Time**OnlineOffline\nForecastForecastDIGITAL CONSUMERS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 65
        }
    },
    {
        "page_content": "67Streaming TV Is Displacing Linear TVIn just two years, streaming\u2019s share of overall TV consumption increased more than 10 percentage points to 39% as of July 2023, surpassing the shares of cable and broadcast, respectively. Ad spend on connected TV (CTV) is following eyeballs and is likely to grow 17% in real terms at an annual rate, from $25 billion in 2023 to $73 billion in 2030. If so, ad spend on CTV should surpassthat on linear TV in 2027.\n*The share of streaming, cable, and broadcast do not add up to 100%, as we exclude the portion of consumption that Nielsen categorizes as \u201cOther,\u201d which includes time spent on unmeasured sources like video-on- demand (VOD), audio streaming, gaming, and other device use. **We define linear TV as traditional TV delivered via cable, satellite, or over-the-air. We define connected TV as streamed TV delivered over-the-top through smart TVs, streaming media devices, video game consoles, and other modern hardware. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources,, including Nielsen, Insider Intelligence, and MAGNA Global, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results. $- $20 $40 $60 $80\n2018'19'20'21'22'23E'24E'25E'26E'27E'28E'29E'30E$ BillionsUS TV Ad Spend**Connected TVLinear TV\n0%5%10%15%20%25%30%35%40%45%May-21Jul-21Sep-21Nov-21Jan-22Mar-2 2May-22Jul-22Sep-22Nov-22Jan-23Mar-2 3May-23Jul-23Sep-23Nov-23Share Of US TV TimeUS TV Viewership Share* StreamingCableBroadcast\nDec-23DIGITAL CONSUMERS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 66
        }
    },
    {
        "page_content": "68\nPinterest0.4B\nSocial Commerce Merchants Can Sell to Anyone\u2014Anytime, AnywhereSocial media platforms are increasing their monetization of global audiences with e-commerce. Thanks to omnichannel solutions\u2014both physical and digital\u2014social commerce could grow 32% at an annual rate, from $730 billion today to over $5 trillionin2030.\nYouTube2.8B MAUs*\nInstagram1.5B\nTikTok1.1B\nShopify\nBusiness Launch \u2013 Omnichannel Selling \u2013 Payment Processing \u2013 MarketingAnalytics and Management \u2013 Logistics & Shipping \u2013 Business Funding*We estimate each platform\u2019s monthly active users (MAUs) across its iOS and Android mobile apps. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, including Sensor Tower, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.\nFacebook2.2B\nWhatsApp1.8B\n0%5%10%15%20%25%30%\n $- $2 $4 $6 $8 $10 $12 $14 $16 $18\n2018'19'20'21'22'23E'24E'25E'26E'27E'28E'29E'30EShare Of E-commerceGross Merchandise Value($ Trillions)Global Social Commerce SalesARK ForecastSocial Commerce (LHS)Traditional E- commerce (LHS)Social Commerce Share (RHS)DIGITAL CONSUMERS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 67
        }
    },
    {
        "page_content": "69Mobile Sports Betting Continues To Grow And Consolidate In The USThanks to legalization and consumer adoption, the winners in online sports betting are pulling away from the pack. As online sports betting surged 35% during 2023, DraftKings and FanDuel offered superior user experiences that helped take share from other sportsbooks. DraftKings and FanDuel grew their share of national deposits to 75% in 2023, while the long tail of sportsbooks lost 8 percentage points of share.\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, including Yipit Data, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.0%25%50%75%100%\n $- $100 $200 $300 $400 $500\n2018'19'20'21'22'23E'24E'25E'26E'27E'28E'29E'30E% of Total Volume$ BillionsUS Online Sports Betting VolumeLegalized States (LHS)Future Legalizations (LHS)Online Penetration (RHS)\n124% CAGR22% CAGR\n0%20%40%60%80%100%Jan-20Apr-20Jul-20Oct-20Jan-21Apr-21Jul-21Oct-21Jan-22Apr-22Jul-22Oct-22Jan-23Apr-23Jul-23Oct-23Sportsbook ShareNational Deposit Market ShareDraftKings and FanDuelAll Other SportsbooksDIGITAL CONSUMERS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 68
        }
    },
    {
        "page_content": "70\n$0.0016 $0.07 $0.15 $0.23 $1.40 \n $0.001 $0.010 $0.100 $1.000 $10.000Net Monetization Per HourGross Platform Monetization Rates**Text-based AIStreamed AudioVideo GamesStreamed VideoDating AppsOnline Experiences Are Becoming More Immersive And MonetizableHistory suggests that deeper immersion leads to higher monetization. After computer graphics expanded the market beyond text-based adventure games in the 1980s, gaming revenue soared 19% at an annual rate, from $6 billion in 1985 to $24 billion in 1993.Now, multimodal AI\u2014text, images, audio, and video\u2014are creating more immersive and interactive experiences that should expand the market.\n*\u201dText games\u201d refer to both text-based and spreadsheet-based games. \u201cAll other games\u201d exclude arcade game releases. Gaming revenue captures PC and console gaming revenue only **We estimate various platforms\u2019 ability to monetize on direct consumer spend only.. ***Revenue figures have been inflation-adjusted to 2023 US Dollars. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results. $- $5 $10 $15 $20 $25 $30 $35 $40 $45\n0%20%40%60%80%100%1975'77'79'81'83'85'87'89'91'93'95'97'99'01'03'05Gaming Revenue($2023 Billions)***Share Of Games ReleasedVideo Games Evolution*Text Games (LHS)All Other Games (LHS)Gaming Revenue (RHS)$10$1$0.1$0.01$0.001DIGITAL CONSUMERS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 69
        }
    },
    {
        "page_content": "71\nAtari introduces the Atari 2600.Apple launches the first iPhone.Generative AI lowers the cost of UGC.\n1.E-041.E-031.E-021.E-011.E+001.E+011.E+021.E+03\n1975'78'81'84'87'90'93'96'99'02'05'08'11'14'17'20Cumulative Number(Log Scale)Number Of Video Game ReleasesTraditional GamesMobile GamesRoblox Experiences\n*We normalize the cost of 3D asset generation by each model\u2019s CLIP R-Precision scores. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, including Nichol et al. 2022, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Thanks To AI-Assisted Creation, Gamers Could Become DevelopersAI-assisted game creation on user-generated content (UGC) platforms could cause an explosion in gaming content.According to our research, after normalizing for output quality, the cost of generating a single 3D asset has dropped ~99% at an annual rate to less than $0.06 since 2021. AI should democratize content creation and accelerate the growth in UGC. Roblox already has delivered more than ~470 million experiences globally, 52x the combined number of PC, console, and mobile app games.\n$1E-02$1E-01$1E+00$1E+01$1E+02$1E+03\nOct-21Feb- 22May-22Aug-22Dec-22Mar-2 3Dollar Cost per 3D AssetDate of PublicationCost Decline In Generative AI For 3D Assets*>99% Annualized Cost Decline\n'22108107106105104103102109$1,000$100$10$1$0.1$0.01(Log Scale)DIGITAL CONSUMERS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 70
        }
    },
    {
        "page_content": "72\nThe Market For Virtual Reality Is NascentDespite significant headset improvements, including Apple's Vision Pro,developers have not flocked to support virtual reality (VR). Without compelling usecases,adoptionhas been slow. MetaQuest,for example, is offeringonly 2,200 apps\u2014a fractionof the 553,000 the iPhone boasted fiveyears after its launch. As a result, Meta has sold only 27 million Quest units, 18% of the 146million iPhonesApple sold cumulatively five years after launch.\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.020406080100120140160\n12345Cumulative Units(Millions)\nYears After LaunchApple iPhone vs. Meta QuestHeadset ShipmentsApple iPhoneMeta Quest\n01101001,000\n12345Units(Thousands) (Log Scale)Years After LaunchiOS vs. Meta Quest AppsiOS AppsMeta Quest Apps\nDIGITAL CONSUMERS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 71
        }
    },
    {
        "page_content": "73AI Could Redefine Personal ComputingAs computing transitioned, hardware cycles compressed from 35 years for the personal computer to 20 years for the smartphone,causing the consolidation of software players. More rapid adoption of AI-enabled hardware could accelerate the consolidation of software providers. \nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, including DeGusta 2012, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.20 35 \n010203040AI-EnabledHardwareSmartphoneComputer\nYearsTime To Penetrate 75% Of The US Population\n?Software ExpansionOperating System ConsolidationWindows, Commodore 64 OS, Atari TOS, Amiga, Linux, MacOS\u2026Windows, MacOS, LinuxBB OS, Windows Phone, iOS, Symbian, Android, Palm OS\u2026iOS, Android?GPT-4, Claude 2, Mixtral, Llama 2, Grok\u2026DIGITAL CONSUMERS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 72
        }
    },
    {
        "page_content": "74Digital Consumption Is Outpacing Economic GrowthGlobally, consumers spent 20% of their $34 trillion leisure budget on digitally-facilitated goods and services in 2023. Based onthe shift toward digital leisure, real digital revenue* could increase 16% at an annual rate during the next seven years, from ~$1.8 trillion to $5 trillion, and account for 43% of all leisure spending in 2030.\n*We define digital platform revenue as the gross revenues of US sports betting, global video game software and services, global digital video, and global digital audio. We also include net e-commerce platform revenue and net NFT creator fees and platform revenue on a global basis. **Direct includes spend across e-commerce, video game software, digital video, digital audio, NFTs, and US mobile sports betting. Indirect includes spend across all digital ads. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.0%5%10%15%20%25%30%35%40%45%50%\n $- $5 $10 $15 $20 $25\n2018'19'20'21'22'23E'24E'25E'26E'27E'28E'29E'30EShare of Leisure Spend$ TrillionsGlobal Digital Leisure Spend**Direct (LHS)Indirect (LHS)Share of Total Leisure Spend (RHS)\n $- $1 $2 $3 $4 $5 $6\n2018'19'20'21'22'23E'24E'25E'26E'27E'28E'29E'30E$ TrillionsGlobal Digital Platform RevenueDirectIndirectDIGITAL CONSUMERS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 73
        }
    },
    {
        "page_content": "75\n75DigitalWallets\nSources: ARK Investment Management LLC, 2024. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Closing The Loop With Two-Sided NetworksAndrew KimAnalystResearch By:BIG IDEAS 2024",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 74
        }
    },
    {
        "page_content": "76Vertical software refers to a suite of solutions tailored to the needs of specific industries. Leading vertical software platforms are expanding rapidly into financial services for consumers and merchants. With two-sided networks, such software could facilitate closed loop transactions from consumer to merchant, merchant to employee, and employee to merchant. ARK believes that digital wallets on these platforms will enable fully closed payment ecosystems. Block, Shopify, and Toast are compelling platforms likely to use digital wallets as the nucleus of their consumer, merchant, and employee ecosystems. According to our research, closed loop consumer payments, merchant banking, and employee payroll/payments could increase their revenues by 22-33% at an annual rate during the next seven years, from $7 billion in 2023 to $27-$50 billion in 2030.*\n*In this exercise, we forecast the core revenues of Block, Shopify, and Toast to grow 22% at an annual rate over the next seven years. Summing the mentioned revenue opportunities on top of our core revenues forecast increases the annual growth rate from 22% to 33% over the next seven years. We primarily model Block\u2019s historical revenue and future revenue opportunity across its Square merchant ecosystem and do not incorporate Cash App or Afterpay revenues independent of Square. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.DIGITAL WALLETS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 75
        }
    },
    {
        "page_content": "77Vertical Software Platforms Are Consolidating Financial ServicesIn addition to enabling core business operations, vertical software providers like Block, Shopify, and Toast are consolidating financial services for merchants. With digital wallets at their core, andpartnering with sponsor banks and fintech companies or activating their own banking charters, vertical platforms should eliminate myriad merchant interactions with less efficient legacy financial institutions.Savings**\n*We consider Block\u2019s Cash App and Toast\u2019s MyToast mobile app as consumer digital wallets, and we consider Shopify\u2019s Shop mobile app and Toast\u2019s Toast Takeout mobile app as digital wallets in their early stages. **We consider Shopify Balance as both a checking and savings vehicle for merchants. ***Given xtraCHEF\u2019s invoice automation features, we believe Toast will soon offer direct bill pay on its platform. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.ToastShopifyBlockCheckingDebit CardCredit CardWorking Capital FinancingPayrollBill Pay***Digital Wallet\nConsumer*Merchant\nDIGITAL WALLETS\n",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 76
        }
    },
    {
        "page_content": "78Vertical Software Platforms Are Consolidating Consumer ServicesVertical software platforms are not only enabling vast merchant networks but also building consumer networks using digital wallets. By scaling merchant and consumer networks simultaneously, vertical software platforms are becoming operating systemsfor these two-sided networks.\n*We consider Block\u2019s Cash App and Toast\u2019s MyToast mobile app as consumer digital wallets, and we consider Shopify\u2019s Shop mobile app and Toast\u2019s Toast Takeout mobile app as digital wallets in their early stages. **We consider the Toast Pay Card a form of consumer debit cards. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Debit Card**\nToastShopifyBlockSavingsBuy Now Pay LaterPersonal LendingCheckingDigital WalletE-commercePayments\nConsumer*MerchantLoyalty\nDIGITAL WALLETS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 77
        }
    },
    {
        "page_content": "79Two-Sided Networks Can Close The Financial Loop Between Consumers And MerchantsClosed-loop payment ecosystems incorporate in-network money transfers in three ways: from consumers to merchants, from merchants to employees, and from employees\u2014cum consumers\u2014to merchants. To build these payment ecosystems, platforms must have: 1) large and engaged two-sided networks, 2) end-to-end visibility over merchant operations and finances, and 3) vertical industry expertise. \nSources: ARK Investment Management LLC, 2024. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Vertical Software PlatformConsumerMerchants\nEmployeesIn-Network Consumer Digital WalletIn-Network Merchant Digital WalletIn-Network Payroll Service Provider$$$$DIGITAL WALLETS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 78
        }
    },
    {
        "page_content": "80\nSteps:3PSP Take Rate:2.3%Steps: 9PSP Take Rate: 1.1%Digital Wallets Are Likely To Disintermediate Consumer-To-Business (C2B) Payment EcosystemsTransactions funded with digital wallet balances bypass banks and card networks, saving interchange fees for payment facilitators, merchants, and consumers. In ARK\u2019s view, vertical software platforms with scaled consumer and merchant ecosystems will leverage digital wallets to facilitate closed-loop transactions.*\n*Payment processes and associated fee estimates are rendered for illustrative purposes only. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.BEFORE: C2B Card Payment AuthorizationAFTER: C2B Closed-Loop Payment Authorization\nConsumerPayment Service ProviderIssuing BankCard Network\nMerchantAcquiring Bank\n123456\n789Issuing BankCard NetworkAcquiring BankConsumer Digital WalletPayment Service ProviderMerchantIn-NetworkIn-Network23FundingSource1Transaction AuthorizationTransaction SettlementDIGITAL WALLETS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 79
        }
    },
    {
        "page_content": "81Closed-Loop Payment Volume In The US Could Increase 24-Fold By 2030According to ARK\u2019s research, total C2B digital wallet payments should increase 20% at an annual rate during the next seven years, from ~$2 trillion in 2023 to ~$7 trillion in 2030. As a percent of the total, closed-loop payments should increase from ~4% to 25%, taking the payments revenue forecast for Block\u2019s Square, Shopify, and Toast from $3.5 billion to ~$21 billion, a 29% annualized rate of gain.**\n*We define closed-loop transactions as any consumer-to-buyer (C2B) digital wallet transaction that does not involve third-party issuers or card networks except for digital wallet balance top-ups. **Closed-loop payment revenue is represented on a gross basis and will be shared between the software platforms and all enabling market participants such as other fintechs or sponsoring financial institutions. We primarily model Block\u2019s historical revenue and future revenue opportunity across its Square merchant ecosystem and do not incorporate Cash App or Afterpay revenues independent of Square. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, including Worldplay 2019, 2020, 2021, 2022, 2023, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.0%5%10%15%20%25%30%\n $- $1 $2 $3 $4 $5 $6 $7 $8\n2018'19'20'21'22'23E'24E'25E'26E'27E'28E'29E'30EPercent of  Personal ConsumptionC2B Volume ($ Trillions)US Digital Wallet Transaction Volume*Open LoopClosed Loop% of PCE (RHS)20% CAGR\n $- $5 $10 $15 $20 $25\n2019'20'21'22'23E'24E'25E'26E'27E'28E'29E'30EC2B Payment Revenue ($ Billions)US Payment Revenue For Block's Square, Shopify, And ToastNet Payment RevenueClosed Loop Payment Revenue Opportunity29% CAGRDIGITAL WALLETS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 80
        }
    },
    {
        "page_content": "82\nSteps:3 + 2 = 5PSP Take Rate:2.3% 2.4%Steps: 9 + 7 = 16PSP Take Rate: 1.1%\n*Payment processes and associated fee estimates are rendered for illustrative purposes only. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.BEFORE: Status Quo Merchant SettlementAFTER: Closed-Loop Merchant Settlement\nMerchantPayment Service ProviderConsumer Bank AccountCard Network\nMerchant Bank AccountAcquiring Bank\n10Issuing BankCard NetworkAcquiring BankMerchantDigital WalletPayment Service ProviderIn-Network5Consumer Digital Wallet4Transaction AuthorizationTransaction SettlementDIGITAL WALLETSDigital Wallets Could Disintermediate Merchant BankingVertical software platforms can serve merchants with financial services. With digital wallets, these platforms not only enhance convenience but also monetize deposits, reducing the number of steps from payment authorization to merchant settlement from 16 to 5 and more than doubling the platform take rate.*\n141315121611",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 81
        }
    },
    {
        "page_content": "83Merchant Digital Wallet Revenue Could Double By 2030If net deposit yields were to equal those of large commercial banks, the merchant banking revenue associated with Block\u2019s Square, Shopify, and Toast could scale 40% at an annual rate during the next seven years, from $700 million in 2023 to $7 billion in 2030. At $7 billion, the three platforms would 5x their share of total commercial payments revenue in the US from ~0.3% todayto ~1.6% in 2030.\n*Our incremental banking revenue forecast intends to capture both net interest income and noninterest revenue associated with merchant deposits and lending that are not already included in our forecast for Block, Shopify, and Toast\u2019s working capital financing business. Both line items are represented on a gross basis and will be shared between the software platforms and all enabling market participants such as other fintechs or sponsoring financial institutions. This forecast does not directly include revenue from instant transfers, corporate card issuance and spend management, or bill pay. We primarily model Block\u2019s historical revenue and future revenue opportunity across its Square merchant ecosystem and do not incorporate Cash App or Afterpay revenues independent of Square. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, including McKinsey & Company 2018, 2019, 2020, 2021, 2022, 2023, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results. $- $1 $2 $3 $4 $5 $6 $7 $8\n2019'20'21'22'23E'24E'25E'26E'27E'28E'29E'30EGross Revenue ($ Billions)Addressable US Merchant Banking Revenue For Block's Square, Shopify, And Toast*Merchant LendingIncremental Banking RevenueDIGITAL WALLETS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 82
        }
    },
    {
        "page_content": "84Digital Wallets Could Disintermediate The Payroll Banking OpportunityVertical software platforms probably will use digital wallets to link merchants directly to employees, adding monetization opportunities with little to no cost of customer acquisition.*\n*Payment processes and associated fee estimates are rendered for illustrative purposes only. **In this example, we assume the PSP offers a first-party or white-labeled third-party payroll solution. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.BEFORE: Status Quo Payroll\nPayment Service ProviderACH Network\nEmployee Bank AccountMerchant Bank AccountMerchantPayroll Service ProviderSteps:9 + 7 + 5 = 21PSP Take Rate:1.1%AFTER: Closed-Loop Payroll\nIn-NetworkIn-NetworkMerchant Digital WalletPayment Service ProviderEmployee Digital Wallet8Payroll Service Provider**ACH NetworkMerchant Bank AccountSteps:3 + 2 + 3 = 8PSP Take Rate:2.4% 2.8%\nTransaction AuthorizationTransaction SettlementDIGITAL WALLETS192018172176",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 83
        }
    },
    {
        "page_content": "85Employee Digital Wallets Represent A Potential $25 Billion Revenue OpportunityLike their consumer counterparts, employee digital wallets could evolve into full-scale financial apps customized for specific industries. Employee payroll and payments could become compelling monetization streams for Block, Shopify, and Toast. According to our research, employee digital wallets could generate $25 billion of gross revenue on the $1 trillion in addressable payroll opportunities in 2030.* If these platforms were to capture 100% of this opportunity, employee digital wallet revenue could grow 123% at an annual rate during the next seven years.\n~$100M$5B$6B$15B $- $5 $10 $15 $20 $25 $30\n2023E2030EGross Revenue ($ Billions)US Employee Digital Wallet Revenue Opportunity For Block, Shopify, And Toast**Payroll Software RevenueEmployee Debit RevenueEmployee Credit Revenue123% CAGR\n*Our forecasted ~$1 trillion in annual payroll aggregates our forecasts for Block, Shopify, and Toast\u2019s merchant base, employee base, and average payroll across addressable verticals such as retail, accommodations and food services, other consumer services, professional services, and other consumer entertainment. **All revenue is represented on a gross basis and will be shared between the software platforms and all enabling market participants such as other fintechs or sponsoring financial institutions. Payroll software revenue does not include float revenue, and we do not adjust for duplicate employee debit and credit revenues that may already be embedded in consumer payment revenue. We primarily model Block\u2019s historical revenue and future revenue opportunity across its Square merchant ecosystem and do not incorporate Cash App or Afterpay revenues independent of Square. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.DIGITAL WALLETS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 84
        }
    },
    {
        "page_content": "86Digital Wallets Could Generate $23 Billion In Vertical Software RevenueAccording to ARK\u2019s research, the core revenue of Block\u2019s Square, Shopify, and Toast should increase 22% at an annual rate duringthe next seven years, from $7 billion in 2023 to $27 billion in 2030. Closed loop consumer payments, merchant banking, and employee payroll/payments could generate an additional $23 billion, accelerating revenue growth from 22% to 33% at an annual rate by 2030.\n*Core revenues include software revenue, net open-loop payment revenue, merchant lending revenue, and revenue attributable to all other extant business lines. Merchant banking revenue includes both net interest income and noninterest revenue attributable to merchant deposits and lending not already captured by our forecast for Block, Shopify, and Toast\u2019s working capital financing business. All revenue segments excluding net open-loop payment revenue, closed-loop payment revenue, and employee payment revenue are represented on a gross basis, and all revenue will be shared between the software platforms and all enabling market participants such as other fintechs or sponsoring financial institutions. We use our status quo forecasts the software platforms\u2019 net take rates to estimate net employee payment revenue and do not explicitly estimate incremental cost synergies from employee closed-loop payments. We view all revenue segments except for core revenues not as explicit forecasts but as addressable opportunities in the US for Block\u2019s Square, Shopify, and Toast. We primarily model Block\u2019s historical revenue and future revenue opportunity across its Square merchant ecosystem and do not incorporate Cash App or Afterpay revenues independent of Square. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results. $- $10 $20 $30 $40 $50 $60\n2019'20'21'22'23E'24E'25E'26E'27E'28E'29E'30E$ BillionsUS Revenue Opportunities For Block\u2019s Square, Shopify, And Toast*Core RevenuesClosed Loop PaymentsMerchant BankingPayroll SoftwareEmployee Payments33% CAGRDIGITAL WALLETS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 85
        }
    },
    {
        "page_content": "87\n87Precision TherapiesCuring Disease More Efficiently And Less Expensively\nSources: ARK Investment Management LLC, 2024. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Alexandra UrmanAnalystRong GuoResearch Associate Research By:Pierce Jamieson AnalystBIG IDEAS 2024",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 86
        }
    },
    {
        "page_content": "88During the past twenty years, new modalities for precision therapies, CRISPR gene editing, RNA therapeutics and targeted protein degradation have proliferated. Innovative therapies powered by artificial intelligence(AI), CRISPR gene editing, and new sequencing technologies have increased returns on research and development (R&D), whileenabling undruggable targets to become druggable.Increasingly, precision therapies are becoming multiomic and curative, with mechanisms of action spanning DNA, RNA, proteins, and more. Based on ARK's research, the enterprise value of companies focused on precision therapies could appreciate 28% at an annual rate during the next seven years, from ~$820 billion in 2023 to ~$4.5 trillion by 2030.\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.PRECISION THERAPIES",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 87
        }
    },
    {
        "page_content": "89\nSmall MoleculesProteinsAntibioticsSmall PeptidesProdrugsTILsMonoclonal Antibodies Antibody-Drug ConjugatesASOsRNA AptamerssiRNA/RNAiZFNsCAR-TTALENSiPSCsCRISPR/Cas9mRNA Vaccines\u03b3\u03b4T-cellsTPDsBase/Prime Editors*\n1900192019401960198020002020Aspirin (1899)Penicillin (1928)Insulin (1921)Oxytocin  (1953)Recombinant DNA (1972)Polymerase Chain Reaction (1982)Sanger Sequencing (1977)Watson & Crick (1953)Sequencing by Synthesis (1996)Single Molecule Real Time Sequencing (2011)New Therapeutic Modalities Are ProliferatingDuring the last thirty years, the number of therapeutic modalities with entirely new mechanisms of action has proliferated. Not only have they expanded the number of treatable diseases, but they have also improved efficacy and safety. In 2023, more than 25% of clinical trials were harnessing new therapeutic modalities.Discovery of New Modalities Based On Investigational New Drug Application Approval\n*This timeline is not exhaustive.Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of external sources, including Biomedtracker, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.PRECISION THERAPIES",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 88
        }
    },
    {
        "page_content": "90Precision Therapies Could Reverse The Downtrend In Returns On Research And Development (R&D)Given regulatory bottlenecks and legacy drug discovery methods, the return on therapeutic R&D has been falling for nearly 25 years. According to our research, novel therapeutic modalities and R&D methods, coupled with regulatory approval of \u201cprecision\u201d therapies, could reverse the downward trend in return on investment in the pharmaceutical industry.\n$0$20$40$60$80$100$120$140$160$180\n1981 to 19851986 to 19901991 to 19951996 to 20002001 to 20052006 to 20102011 to 20152016 to 20202021 to 2023*BillionsAverageAnnualR&DAndIncrementalRevenueAttributableToDrugsReleasedR&D Devoted to Drugs ReleasedIncremental Revenue Yield\n*Shorter time frame. Dataimpacted due to COVID.Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of external sources, including Biomedtracker and Ycharts, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.PRECISION THERAPIES",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 89
        }
    },
    {
        "page_content": "91Precision Therapies Could Reverse The Downtrend In Returns On R&DGiven regulatory bottlenecks and legacy drug discovery methods, returns on therapeutic R&D declined on balance for ~35 years through 2020. Regulations permitting novel therapeutic modalities and R&D methods enabling \u201cprecision\u201d therapies could reverse the downtrend during the next five to ten years.\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of external sources, including Biomedtracker and Ycharts, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.00. 20. 40. 60. 811.21.41.61.8\n1981 to 19851986 to 19901991 to 19951996 to 20002001 to 20052006 to 20102011 to 20152016 to 20202021 to 2023*2030 forecastRatioRatio Of Incremental Revenue To Related R&D SpendPRECISION THERAPIES",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 90
        }
    },
    {
        "page_content": "92\n0123\nGeneralPrecisionDuplicative Trials per Unique TargetPrecision Therapies Are Reducing The Number Of Duplicative Trials Precision Therapies Are Helping Treat Diseases That Were Previously UndruggablePrecision therapies, including RNA-based medicines and \u201ctargeted protein degraders\u201d (TPDs), are expanding not only the number of druggable proteins in the human genome, but also the number of treatable tissue types.\nAdvanced precision therapy trials are testing a wider variety of biological targets than was possible with status quo treatments, lowering the number of duplicative trials by 77%. As a result, scientists are testing more biological targets per dollar of R&D, increasing the probability of identifying unique and successful therapies.The human genome contains ~20,000 protein-coding genes, of which only 864 (4.3%) are associated with drugs that the FDA has approved. Human Protein Atlas estimates that 79% (~15,800) of human proteins are undruggable. Our research indicates that TPDs and adjacent technologies could treat 56% (~11,200) of human protein-coding genes.-77%4%17%56%79%44%\n0%20%40%60%80%100%\nConventionalTPD-Enabled% of Human Protein-Coding GenesTPDs Are Expanding The Druggable ProteomeFDA ApprovedDruggableUndruggable\nData are as of December of 2023Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.PRECISION THERAPIES",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 91
        }
    },
    {
        "page_content": "93The Value Of Curing Rare Diseases Like Sickle Cell Anemia Is HighAmong precision therapies, gene editing medicines like CRISPR-Cas9 have the potential to cure rare genetic diseases such as Sickle Cell Disease (SCD). SCDis an inherited red blood cell disorder that affects more than 100,000 people in the US and 20million people globally, primarily in Africa. Today, therapeutics account for ~16% of the total spent on treating SCD disease in the US,but they have done little more than manage symptoms, as the life expectancy of SCD patients is only 56% that of the general population.\n$0.0$0.5$1.0$1.5$2.0$2.5\n1357911131517192123252729313335373941434547495153MillionsAgeSCD Healthcare Costs Over Average Patient LifetimeTherapeuticsOther Costs\n*Quality of Life Years Gained = Health Utility * DurationFor Health Utility, 0 means dead and 1 means full healthData are as of December of 2023.Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.TherapeuticsOther Costs$0.0$0.5$1.0$1.5$2.0$2.5\nCurrent Direct CostQu ality of Life Years GainedNew Therapy CostMillionsReasonable Cost For Sickle Cell Disease Cure\nForecast*PRECISION THERAPIES",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 92
        }
    },
    {
        "page_content": "94Curing All Rare Diseases Would Be ValuableThe US healthcare system spends approximately $450 billion per year on the treatment of rare diseases. To manage patients with rare diseases throughout their lifetimes, the cost could mount to $20 trillion, of which less than half would be for medication.Theoretically, curing all rare diseases would shift most of the costs to medication, obviating the need for in-and out-patient disease management, underscoring the value of a cure.\n $- $5 $10 $15 $20 $25\n 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51TrillionsYearsAggregate US Rare Disease Healthcare CostsMedication CostsOther Costs\nAvoided Direct CostsValue of QALYs GainedTotal ValueTrillions$0$5$10$15$20$25Forecasted Value of Rare Disease Cures To Healthcare System Over 50 Years\nData are as of December of 2023Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of external sources, including Orphanet 2023, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.* *Quality of Life Years Gained = Health Utility * DurationFor Health Utility, 0 means dead and 1 means full healthPRECISION THERAPIES",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 93
        }
    },
    {
        "page_content": "95Sizing the Opportunity: Precision TherapiesBased on our research, as technologieslike CRISPR gene editing, sequencing, and artificial intelligence (AI) create precision therapies, the enterprise value of precision therapy companies should appreciate at a ~28% compound annual growth rate (CAGR)during the next seven years, from ~$820 billion in 2023 to ~$4 trillion by 2030.\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of external sources, including S&P Capital IQ Data and Biomedtracker, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.$0.0$0.5$1.0$1.5$2.0$2.5$3.0$3.5$4.0$4.5$5.0\n20232030TrillionsPrecision Therapy Enterprise Value Should Appreciate 28% Annual Rate Through 2030Cell TherapiesGene Editing/TherapyRNA TherapeuticsAntibodiesPrecision Small Molecules\nForecastCAGR\n34%46%11%21%16%PRECISION THERAPIES",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 94
        }
    },
    {
        "page_content": "96\n96MultiomicTools And TechnologyTranslating Biological Insights Into Better Healthcare And Economic Value\nSources: ARK Investment Management LLC, 2024. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Pierce Jamieson AnalystNemo MarjanovicResearch Associate Research By:BIG IDEAS 2024",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 95
        }
    },
    {
        "page_content": "97Over the past decade, the number of biological tools and techniques has proliferated, their capability having improved remarkably. Among others, three enabling technologies stand out: high-throughput proteomics, artificial intelligence, and single-cell sequencing. Their convergence is increasing productivity and efficiency, enhancing precision in healthcare applications, and unlocking substantial economic value.According to ARK\u2019s research, these technologies could reduce research and development (R&D) spending per drug by more than 25%, potentially increasing the enterprise value of the precision therapy space 26% at a compound annual rate during the next seven years, from ~$820 billion in 2023 to ~$4.5 trillion in 2030. \nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.MULTIOMIC TOOLS AND TECHNOLOGY",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 96
        }
    },
    {
        "page_content": "98\n01101001,00010,000\n1001,00010,000100,000\n2011201220132014201520162017201820192020202120222023Unique Proteins/ Sample(Depth)Proteins Analyzed Per Hour(Throughput)Throughput (LHS)Depth (RHS)120% CAGR\n40% CAGR\nAI/ML*** TechniquesSingle-Cell ProteomicsSRMXL-MSITRAQTMT*SWATH-MSHRAM**Parallel Reaction Monitoring Real-Time Single Molecule Proteomic SequencingReal-Time SearchIon-Mobility Mass SpecProteomic Throughput And Depth Are Improving Exponentially Advances in mass spectrometry and bioinformatics have improved proteomic analysis dramatically over the past decade, increasing resolution, accuracy, and the capacity to analyze multiple samples simultaneously. Not only have these developments enabled detailed exploration of the proteome in health and disease, but they also have accelerated the discovery of cancer biomarkers and the development of targeted therapies.\n*SRM: Single reaction monitoring; XL-MS: cross-linking mass-spectrometry; ITRAQ: isobaric tagging; TMT: tandem mass spectrometry. **SWATH-MS:sequential window acquisition of all theoretical fragment ion spectra mass spectrometry. ***AI/ML: Artificial Intelligence/Machine Learning. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of external sources, including Peters-Clarke et al. 2023, and Zhang and Cui 2022, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.MULTIOMIC TOOLS AND TECHNOLOGY",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 97
        }
    },
    {
        "page_content": "99Wright\u2019s Law* Has Predicted The Cost Decline Of ProteomicsAs the number of proteomes analyzed by mass spectrometry has increased, costs have dropped dramatically, unlocking new possibilities in medical research and diagnostics. Our research suggests that for untargeted proteomics using mass spectrometry,the cost per sample is declining 23% at an annual rate, or ~11% for each cumulative doubling in the number of proteomes sequenced. Proteomic discoveries are paving the way for the identification of novel biomarkers, enabling the earlier detection and treatment for unique cancer subtypes. \n$1$10$100$1,000$10,000Cost/SampleCumulative Proteomes SequencedWright's Law Has Predicted The Cost Decline For Untargeted Proteomics2011201320212023\n12517522527532537542547 520102011201220132014201520162017201820192020202120222023Number Of TrialsYearUS Trials With Patient Biomarkers+9% CAGR(2010-2021)\nWright\u2019s Law states that for every cumulative doubling of units produced, costs will fall by a constant percentage. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.100101102103104105106107108MULTIOMIC TOOLS AND TECHNOLOGY",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 98
        }
    },
    {
        "page_content": "100\nGene 1Gene 2Gene 3Gene 4\nCell Type1Cell Type2Cell Type3Cell Type4Cancerous mutations are causing overexpression of \u201cGene 4\u201d in \u201cCell Type 3\u201d\n+++----+ExpressionCancerSingle-Cell RNA Sequencing Is Revolutionizing Our Understanding Of CancerWhile traditional gene expressionanalysis using RNA-seq can measure only the expression of genes in a mixture of different cell types, single-cell RNA-seq (scRNA-Seq) can delineate the expression of different cell types in a complex tissue sample. Theoretically, linking gene expression to specific cells increases the accuracy of measuring by 10x and cuts costs per gigabyte by 76%.\nNormalExpressionCancerNo expression changes are evident in cancerous tissue relative to normal cells.\nMultiple Cell Types\nRNA SampleBulk RNA-SeqSingle-Cell RNACancerous Tissue\nSources: ARK Investment Management LLC, 2024. Illustration created with BioRender.com, based on data from Hwang et al. 2018. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results. Forecasts are inherently limited and cannot be relied upon.\nMULTIOMIC TOOLS AND TECHNOLOGY",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 99
        }
    },
    {
        "page_content": "101\n$0$200$400$600$800$1,000\nLegacyPrecisionCost/Drug Approval ($MM)Forecasted Cost Per Approval\n0%5%10%15%20 %25%30%\nLegacyPrecisionPercentForecasted ProbabilityOf Clinical Success2.1x0.5xThe implementation of Artificial Intelligence/ Machine Learning (AI/ML) in the drug discovery process has increased the number of potential active compounds that drug developers can screen from virtual and physical libraries.High-throughput automated workflows like drug microsynthesis and in-vitro/in-vivo assays are critical to leveraging AI-enabled drug discovery.Within the next decade, companies implementing AI/ML drug discovery methods and automated workflows are likely to double their probability of clinical success from Phase 1 to approval. Earlier in the process, eliminating compounds and increasing productivity should cut the cost of a single drug approval in half.\nAverage Cost Per Approval ($MM)$0$100$200$300$400$500$600$700$800$900$1,000Forecasted Reduction In Cost/Approval Attributable To AI/AutomationAI-Enabled Library ScreeningAutomated Preclinical WorkflowsSavings From Probability Of Approval Improvement-$135mm-$45mm-$280mm$440mm$900mmApproval Cost AI And Automation Are Empowering Drug Discovery\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of external sources, including Recursion 2024, , Paul et al. 2010, Schreiber 2022, and Dreiman 2021, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Precision Therapy ApprovalCostMULTIOMIC TOOLS AND TECHNOLOGY",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 100
        }
    },
    {
        "page_content": "102Drug Development Costs Could Drop PrecipitouslyAdvances in fundamental biology, artificial intelligence, automation, and trial design should lower preclinical drug developmentcosts significantly. They enable methods that eliminate less-promising candidates early in the drug development process, prevent downstream misallocation of R&D capital, and create a larger chemical search space early in the discovery phase. During the next decade, companies leveragingthese techniques fully could lower costs per approval by almost 50%, in part by more than doubling the odds of success for those drug candidates that do enter clinical trials.\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.$0$100$200$300$400$500$600$700$800$900$1,000\nLegacyPrecision($ Millions)R&D Cost Per Drug Approval (Including Failures)\n0%5%10%15%20 %25%30%\nLegacyPrecisionProbability Of Clinical SuccessClinical Success Probability Innovative Trial Design+ Single-Cell Biology+ Proteomic Techniques+ Virtual Compound Libraries+ Biomarker Development+ Humanized animal models+ Automated Liquid Handling+ Automated Invivomics+ Automated Microsynthesis+ CRISPR \u201cPerturb-Seq\u201d Screens+ Organ-on-a-chip TechnologyArtificial IntelligenceAutomationFundamental Biology\n+ AI-Enabled Pathway Analysis+ AI-Enabled Toxicity Prediction+ In-Silico Molecular Modeling+ ML-Driven Compound Screens+ Adaptive Clinical Trial Design+ Precision Biomarkers+ Decentralized/Virtual TrialsEfficiency Innovations-48%2.1xMULTIOMIC TOOLS AND TECHNOLOGY",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 101
        }
    },
    {
        "page_content": "103Technological Advances Should Lower R&D Costs For Each DrugDuring the past decade, R&D spending per drug in development has declined by 3% at an annual rate. According to our research, this decline shouldcontinue, if not accelerate, thanks to groundbreaking advancements in fundamental biology, single-cell sequencing, proteomics, automation and artificial intelligence. Together, these efficiencies should contribute $1.5 trillion, or ~40%, to the increase in enterprise value for precision therapies by 2030. \n$5$6$7$8$9$10$11$1220112012201320142015201620172018201920202021202220232024202520262027202820292030Global, $ MillionsProjected Average Annual R&D Spending Per Drug In Development PipelineHistoricalARK ProjectionMa rket Expecta tion\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results. $- $100 $200 $300 $400 $500 $600 $700\n20 0020 0220 0420 0620 0820 1020 1220 1420 1620 1820 2020 2220 2420 2620 2820 30Precision Therapies Annual  Sales($ Billions)Precision Therapy Sales Could Grow 30% Annually Into 2030Historical PT SalesProjectedMULTIOMIC TOOLS AND TECHNOLOGY",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 102
        }
    },
    {
        "page_content": "104\n104Electric Vehicles\nSources: ARK Investment Management LLC, 2024. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Lower Battery Costs Powering EV AdoptionResearch By:Daniel Maguire, ACAResearch Associate Sam KorusDirector of Research, Autonomous Technology& RoboticsBIG IDEAS 2024",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 103
        }
    },
    {
        "page_content": "105\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.After increasing in response to supply chain disruptions, battery costs now are falling in line with Wright\u2019s Law, leading to lower electric vehicle (EV) sticker prices.If robotaxi platforms proliferate, EVs could account for 95-100% of vehicle sales in 2030.ARK forecasts that electric vehicle sales will scale 33% at an annual rate during the next seven years, from roughly 10 million in 2023 to 74 million in 2030.ELECTRIC VEHICLES",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 104
        }
    },
    {
        "page_content": "106Electric Vehicles Continue To Take Share From Internal Combustion Engine Vehicles\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, including EVVolumes.com, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.-2%-4%-15%1%-4%9%70%18%33%113%59%28%\n-40%-20%0%20%40%60%80%100%120%\n201820192020202120222023ePercent ChangeGlobal Vehicle Sales GrowthInternal Combustion EngineBattery Electric VehiclesELECTRIC VEHICLES",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 105
        }
    },
    {
        "page_content": "107The Auto Industry Is Likely To ConsolidateIf EV adoption continues to gain traction, traditional automakers may be forced to restructure and consolidate.\n0%5%10%15%\n20132014201520162017201820192020202120222023ePercentGlobal Battery Electric Vehicle SalesMarket Share*\n*BEV market share is calculated relative to all \u201clight vehicles\u201d, which are vehicles with a maximum Gross Vehicle Weight Rating (GVWR) of < 8,500 lbs. Sources: ARK Investment Management LLC, 2024, based on data from EVVolumes.com 2023, Hawkins 2023, Mihalascu 2023, Shepardson & Klayman 2023, Rosevear 2023, Transport Policy 2023. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.\u2014InsideEVs Nov 2, 2023VW Group Delays EV Battery Plant In Europe Amid \u201cSluggish\u201d EV Demand\u201cThere is for the time being no business rationale for deciding on further sites,\u201d Volkswagen Group CEO Oliver Blume said.\n\u2014The Verge Dec 11, 2023Ford Will Cut Weekly Production Of F-150 Lightning In Response To Slowing Demand\u2014Reuters Oct 17, 2023GM Delays EV Truck Production At Michigan Plant By YearELECTRIC VEHICLES",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 106
        }
    },
    {
        "page_content": "108Wright\u2019s Law Has Modeled The Decline In Battery Costs Accurately\n0%5%10%15%20%25%30%35%40%45%\n20192023Global LFP Cathode Chemistry Share Of Passenger EV SalesAccording to Wright\u2019s Law, for every cumulative doubling in the number of kWh produced, battery costs will fall by 28%. Lithium iron phosphate (LFP) cells are taking share from nickel-rich cells, illustrating the difficulty of forecasting commodity prices as battery chemistries change over time.\n*Combination of modeled and historical data. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, including Bloomberg New Energy Finance 2023, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results. $1 $10 $100 $1,000 $10,000\n 100,000 1,000,000 10,000,000 100,000,000 1,000,000,000 10,000,000,000$/kWh\nCumulative kWhBattery Cost DeclineNickel CellsNickel ForecastLFP CellsLFP ForecastELECTRIC VEHICLES**",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 107
        }
    },
    {
        "page_content": "109Wright\u2019s Law Points To Faster EV Charging RatesThe EV charging rate seems to be a good proxy for overall performance, including efficiency, range, and power. In the past five years, charging rates for 200 miles of range have improved nearly three-fold, from 40 minutes to 12, and could drop another three-fold to 4 minutes over the next five years. As EV charging reaches acceptable rates, manufacturers are likely to optimize for other features, including autonomous driving, safety, and entertainment.1915:2,314 minutes1999:188 minutes2018:40 minutes2021:20 minutes2022:15 minutes2023:12 minutes2027(e):4 minutes001101001,00010,000\n 1 10 100 1,000 10,000 100,000 1,000,000 10,000,000 100,000,000Theoretical Number Of MinutesTo Charge 200 Miles Cumulative Units Of Electric Vehicle ProductionEV Charging Rates For 200 Miles Of RangeHistorical Data Points2027 Forecast\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.ELECTRIC VEHICLES",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 108
        }
    },
    {
        "page_content": "110Many EV Manufacturers Are Struggling To Scale ProfitablyIn the absence of an EV supply chain, Tesla had little choice but to vertically integrate. Now that the supply chain is evolving, other auto manufacturers will reach profitability if they scale. Many are pulling back from the market, however, because the\u2014already profitable\u2014market leaders are cutting prices aggressively.\n*Data may not be exhaustive. \u201cTTM\u201d (trailing twelve months). \u201cBEV\u201d (battery electric vehicle). Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, including Piper Sandler 2023, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Model 3 (China)Model Y (China)\nModel 3 (US)Nio ET5 (China)Model Y (US)\nZeekr001 (China)Model 3 (Europe)Model Y (Europe)\nAudi Q4 e-tron(Europe) - 50,000 100,000 150,000 200,000 250,000 300,000 350,000 400,000 450,000\n $- $50,000 $100,000 $150,000 $200,000 $250,000Units (TTM As Of Aug 2023)PriceGlobal Luxury BEV Unit Sales At Various Price Points*\nNon-luxury top selling BYD models in China for contextELECTRIC VEHICLES",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 109
        }
    },
    {
        "page_content": "111EVs Have Hit Price-Parity With Internal Combustion Engine VehiclesAs battery costs continue to decline, EV prices should fall, potentially driving exponential growth in unit sales.\n*Older data points adjusted to 2023 dollars using CPI. Segment average transaction prices are as of September 2023 as reported by Cox Automotive. Tesla Model Y LR price taken from Tesla website as of December 2023. Sources: ARK Investment Management LLC, 2024, based on data from Cox Automotive 2023.. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.ELECTRIC VEHICLESTesla Model YCompact CarLuxury CarLuxury Compact SUV/CrossoverFull-Size Pickup TruckElectric VehicleTesla Model 3 $25,000 $35,000 $45,000 $55,000 $65,000 $75,000 $85,000\n20122014201620182020202220242026Average PriceUS New Vehicle Transaction PriceNew-Vehicle Transaction Price (Average)New-Vehicle Transaction Price Before Supply Chain BottlenecksLinear (New-Vehicle Transaction Price Before Supply Chain Bottlenecks)\n0%10%20%30%40%50%60%70%80%90%100%\n0255075100125Addressable Revenue Share By MSRP2023 US Dollars (Thousands)*Vehicle Price vs Addressable Market\n$125$100$75$50$25$0",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 110
        }
    },
    {
        "page_content": "112Internal Combustion Engine Vehicles Should Lose Significant ShareIf EVs continue to gain share, as we believe they will, then used cars and new EVs will make more economic sense than new internal combustion engine (ICE) vehicles, perhaps causing a death spiral for incumbent auto manufacturers. As EV and used car prices fall, consumers could delay purchases, waiting for even lower price points.\nNote: Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.ActualForecast20406080100120\n20162017201820192020202120222023e2024202520262027202820292030Units (Millions)Auto Sales Historical EV SalesARK EV ForecastARK ICE ForecastIHS Markit ForecastELECTRIC VEHICLES",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 111
        }
    },
    {
        "page_content": "113\n113Robotics\nSources: ARK Investment Management LLC, 2024. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Generalizing Automation Thanks To The Convergence Of AI Software And HardwareResearch By:Daniel Maguire, ACAResearch Associate Sam KorusDirector of Research, Autonomous Technology& RoboticsBIG IDEAS 2024",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 112
        }
    },
    {
        "page_content": "114The convergence of AI and hardware should enable generalizable robotics.Robots are outperforming humans in factory settings and should do so in many domains. As hardware and software costs decline according to Wright\u2019s Law, AI should continue to improve productivity and create a new market opportunity for generalizable robotics that, at scale, exceeds $24 trillion in revenue annually.\nWright\u2019s Law states that for every cumulative doubling of units produced, costs will fall by a constant percentage. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.ROBOTICS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 113
        }
    },
    {
        "page_content": "115Thanks To AI And Computer Vision, Robots Should Be Able To Operate Cost-Effectively In Unstructured Environments\nThe points in each category represent real world products with the exception of humanoid robots and autonomous vehicles *These figures are estimated costs of humanoid robots that we expect to hit the market. **These figures are for both current operating and future robotaxis. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.$0$50,000$100,000$150,000$200,000Unstructured Environment\nStructured Environment\nInexpensiveExpensiveAutonomous Vehicles**Humanoid Robots*\nCollaborative RobotsWarehouse RobotsHome AppliancesConsumer Drones$600,000$800,000$1,000,000$1,200,000$1,400,000Large Military DronesConstruction RobotsMedical/Surgical RobotsAgTech RobotsTraditional Industrial RobotsROBOTICS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 114
        }
    },
    {
        "page_content": "116Lower Prices Are Stimulating Demand For Industrial Robots\nSources: ARK Investment Management LLC, 2024, based on data from The International Federation of Robotics 2023. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Industrial robot costs have been dropping 50% for every cumulative doubling in production. \n1996199719981999200020012002200320042005200620072008201020122011201320142015201620172018\n20092019202020212022\n - 100,000 200,000 300,000 400,000 500,000 600,000\n$0$20$40$60$80$100$120Unit Sales \nUnit Price eThousandsIndustrial Robots: Price Elasticity Of Demand1996-20022002-20102010-20152016-20182009, 2019 and 202020212022ROBOTICS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 115
        }
    },
    {
        "page_content": "117Increased Performance Is Stimulating Demand For Industrial Robots\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Advances in computer vision and deep learning have improved robot performance 33-fold in seven years. Robots are already surpassing human performance by greater than a factor of two and it\u2019s unclear where the upper limit will be.\n - 200 400 600 800 1,000 1,200\nRobot2015Robot2016Robot2018Robot2019Human2022Robot2022NumberItems Picked And Placed Per HourROBOTICS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 116
        }
    },
    {
        "page_content": "118Collaborative Robots Are Entering The Sweet Spot Of The Adoption Curve\n*S-Curve refers to the typical technology adoption curve, which looks like an \"S\" when plotted over time. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, including International Federation of Robotics 2023 and Citi Research 2023, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.0100200300400500600\n201720182019202020212022Thousand UnitsIndustrial Robot SalesCollaborative RobotsTraditional Robots\n0%2%4%6%8%10%12%\n201720182019202020212022PercentUnit Sales Of Collaborative Robots As A Percent Of Total Industrial Robot SalesCollaborative robots and humans are likely to operate together, whether on the road, in factories, or at home. Historically, S-curves reach tipping points when the adoption of new technologies approaches 10-20% market share.* ROBOTICS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 117
        }
    },
    {
        "page_content": "119Many Companies Are Likely To Deploy More Robots Than Humans\n*Modeled/annualized. Figures denoted with an \u201ce\u201d are ARK estimates. Sources: ARK Investment Management LLC, based on data from Amazon 2023 as of June 26, 2023. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.1 15 30 45 100 140e200 265e350 520 1,082*\n88 117 154 231 341 566 648 798 1,298 1,608 1,541 \n - 200 400 600 800 1,000 1,200 1,400 1,600 1,800\n20132014201520162017201820192020202120222023ThousandsAmazon Robots And EmployeesRobotsEmployees (at start of year)Robots are freeing humans from tedious physical tasks. ROBOTICS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 118
        }
    },
    {
        "page_content": "120\n01020304050607080\nBeforeKiva RobotAfterKiva RobotsMinutesTime From Click To Ship At An Amazon Warehouse\n02468101214\nBeforeAssembly LineAfterAssembly LineHoursTime To Manufacture A Car\n0246810121416\nBeforeWashing MachinesAfterWashing MachinesHoursTime To Do LaundryAutomation\u2019s Impact On Productivity Has Transformed Industries\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.-87%-88%-78%ROBOTICS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 119
        }
    },
    {
        "page_content": "121Generalizable Robotics Represent A Potential $24+ Trillion Global Revenue Opportunity\n*Note: the cells highlighted in green represent what ARK believes to be a reasonable or likely outcome. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.~2.3 Hours of Unpaid Work per Day~2.8 Billion Working Age Population~$10.75 Weighted Average Hourly Wage~$12.5 Trillion Opportunity\u00bd Value Attributed to Free Time vs Paid TimeHousehold RoboticsManufacturing RoboticsARK Forecasts Global Manufacturing GDP At ~$28.5 Trillion In 2030\n~$12+ Trillion Opportunity(Average Of The Green Cells)\n\u00d7=\nRevenue Opportunity*(Billions)Productivity Uplift10%25%50%100%200%400%Take Rate10%286 714 1,429 2,857 5,715 11,430 20%571 1,429 2,857 5,715 11,430 22,860 50%1,429 3,572 7,144 14,287 28,575 57,149 =ROBOTICS\u00d7\u00d7",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 120
        }
    },
    {
        "page_content": "122\n122RobotaxisTransforming Urban Transit Safely And Affordably\nSources: ARK Investment Management LLC, 2024. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Tasha Keeney, CFA Director of Investment Analysis & Institutional StrategiesDaniel Maguire, ACAResearch Associate Research By:BIG IDEAS 2024",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 121
        }
    },
    {
        "page_content": "123\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Thanks to breakthroughs in AI, robotaxis are beginning to revolutionize urban travel and could accelerate the unraveling of the auto loan sector.Safer than human drivers, robotaxis hold the promise of safer and cleaner streets. Robotaxi platform pioneers should enjoy the higher prices associated with early adoption. According to ARK\u2019s research, robotaxi platforms could redefine personal mobility and generate $28 trillion in enterprise value during the next five to ten years.ROBOTAXIS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 122
        }
    },
    {
        "page_content": "124\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Autonomous Ride-Hail Is Likely To Increase Access To Convenient Point-to-Point Transportation\n$1.70 $0.70 $0.70 $0.70 $0.25\n18711934195020162021Cost Per Mile Of A Personally Owned Vehicle(2020 $)\nAdjusted for inflation, the cost of owning and operating a personal car has not changed since the Model T rolled off the first assembly line more than 100 years ago. ARK estimates that autonomous taxis at scale could cost consumers as little as $0.25 per mile, spurring widespread adoption.\n2030ROBOTAXIS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 123
        }
    },
    {
        "page_content": "125Robotaxis are operating in ~20 cities globally, with fully driverless commercial options in at least 7 cities. In 2023, Baiduwas operating at a run rate of 1.6 million autonomous trips,* triple those of Waymo. Cruise has ceased US operations. With accessto50x more driving data than Baidu and 280x more than Waymo, Tesla has a massive data advantage as it prepares to launch its robotaxiservice, the largest AI project in the world.RobotaxiPassenger Trips Annualized At A Rate of ~2 Million Late Last Year\n*This includes only the 55% of rides that are fully autonomous at Baidu. The chart on the right assumes 5 miles per trip for Waymo, Cruise, and Baidu. Tesla miles in righthand chart are FSD miles and still require a human behind the wheel. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.0.00.20.40.60.81.01.21.41.61.8\nBaiduWaymoTeslaCruiseMillion Rides (Annualized)Autonomous RidesRun Rate\n - 100 200 300 400 500 600 700 800\nBaiduWaymoTeslaCruiseMillion Miles (Annualized)Autonomous MilesRun RateROBOTAXIS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 124
        }
    },
    {
        "page_content": "126Autonomous Vehicles Are Safer Than Human-Driven Vehicles\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, including CDC 2024, Kusano 2023, NHTSA 2023, Tesla 2023, 2024, and Zhang, 2023, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.ARK AdjustedARK AdjustedCompany AdjustedCompany AdjustedIn 2015, ARK estimated that the rate of autonomous vehicle accidents would be ~80% lower than that associated with human drivers, reducing the ~40,000 auto-related fatalities per year in the US and the ~1.35 million globally. Current data support our original estimates.In full self driving (FSD) mode on surface streets, a Tesla appears to be ~5x safer than a Tesla in manual mode, and ~16x safer than the national average. Waymo\u2019s autonomous cars are ~2-3x safer than the national average, while Cruise\u2014now sidelined by regulators\u2014seems to have underperformed the national average considerably.ROBOTAXIS3,200 588 476192 43 Tesla in FSD(2023)Human Driven Tesla(avg 2022)Waymo(2023)National Average(2021)Cruise(2023)Miles Between Crashes On Surface Streets Only(Thousands)\nHuman-Driven Tesla(avg 2022)",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 125
        }
    },
    {
        "page_content": "127Autonomous Electric Transport Should Save The ~10,000 US Lives Per Year Lost To Vehicle Emissions\n - 2,000 4,000 6,000 8,000 10,000 12,000\n2024202520262027202820292030Number Of Deaths AvoidedIncremental Lives Saved In The US By Lower Emissions Associated With Electric And Autonomous VehiclesAir pollution from gas-powered passenger vehicles is associated with 9,700 deaths in the US annually. According to ARK\u2019s research, autonomous electric vehicles should prevent ~10,000 deaths in 2030.* \n*This analysis is based on ARK\u2019s autonomous electric vehicle adoption forecast and adjusted for population growth. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, including Thakrar et al. 2020, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.ROBOTAXIS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 126
        }
    },
    {
        "page_content": "128Large Language Models and Generative AI Should Accelerate The Progress In RoboticsTrained by GPT-4 to perform robotics tasks, a neural network performed better than human expert coders on 83% of tasks, with the margin of improvement averaging 52%.Large Language Models (LLMs) enable text-based training, validation, and self-explanations, which should facilitate regulatory approval.Multimodal models can train autonomous vehicles with images and text, which could result in better performance.Generative AI can train and validate autonomous vehicle safety through simulation.\nNote: Yaw is rotation along the vertical axis of an aircraft. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, including Ma et al. 2023 and Wayve 2023, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.2.072.061.661.421.091.061.001.001.240.880.990.561.001.00Task 1Task 2Task 3Task 4Task 5Task 6Task 7Task 8Task 9Human Normalized ScoreLLM-Driven Reinforcement Learning Outperforms Expert Human CodersAcross Various Robotics Tasks, Environments, And MorphologiesEureka (LLM-based reward design with little manual input, zero-shot rewards)L2R (LLM-based reward design with manual reward templates, few-shot examples)Human12.6512.64\n-2.04-1.04Task Legend:Task 1: To open the cabinet doorTask 2: To make the hand spin the object toward a target.Task 3: To make the humanoid run as fast as possible.Task 4: To make the ant run forward as fast as possible.Task 5: To make the shadow hand spin the object toward a target.Task 6: To make the quadruped follow randomly chosen x, y and yaw target velocities.Task 7: To make the quadcopter reach and hover near a fixed position.Task 8: To balance a pole upright on a cart.Task 9: To stabilize a ball on the table-top.ROBOTAXIS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 127
        }
    },
    {
        "page_content": "129\n$0.00$0.50$1.00$1.50$2.00$2.50$3.00$3.50$4.00$4.50Price Per MileRide-Hail Addressable Market* 4.00 3.00 2.00 1.10 0.60 0.50 0.25$4B$30B$100B$1T: Addressable ridership in Western markets at ~$1$2.4T: Non-commuting miles in higher income countries priced at ~$0.60 per mile$2.75T:  Long tail of demand priced at human driven ride-hail prices of $0.50 per mile in lower income countries$5T: Low cost, accessible autonomous travel at $0.25 per mile $34B: Existing addressable market for ride-hail companies in Western markets charging $2-$4 per mile\n1    5 10 030 Price Points ($):At $0.25 cents per mile, autonomous transportation could serve a wider population than human-driven ride-hail does today. In the meantime, based on the value that consumers place on their time, demand at higher price points could be significant.\n*$11 Trillion is the addressable market, not the revenue we expect in 2030, as we do not expect autonomy to penetrate all addressable miles. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Miles (Trillions)Ride-Hail Is Likely To Create An $11 Trillion Addressable MarketROBOTAXIS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 128
        }
    },
    {
        "page_content": "130Platforms Facilitating The First 50% Of Urban Autonomous Miles Should Generate The Bulk Of Earnings\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.0%20%40%60%80%100%120%\n0%10%20%30%40%50%60%70%80%90%100%Share Of EarningsPenetration Of Urban MilesAutonomous Platforms\u2019 Share Of Earnings Potential Vs. Penetration Of Urban MilesROBOTAXIS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 129
        }
    },
    {
        "page_content": "131\nNote: Wright\u2019s Law states that for every cumulative doubling of units produced, costs will fall by a constant percentage. *Motor Vehicle Loans Owned and Securitized data as of Q3 2023. ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, as of January 3, 2024, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Autonomous Electric Vehicle Adoption Could Disrupt The US Auto Loan IndustryDuring the past three years, interest rate hikes have increased new vehicle monthly car loan payments by ~27%, from $581 to $739. As a result, the number of subprime auto loans delinquent by 60+ days recently hit an all-time high.Thanks to Wright\u2019s Law, EV prices should continue to fall, shifting more miles onto electric platforms and decreasing the value of gas-powered vehicles. As a result, the ~$1.6 trillion in auto loans currently sitting on financial institution balance sheets, issued predominantly for gas-powered vehicles, could be at risk over the next 10 years. \n0%1%2%3%4%5%6%1993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023Percent Of LoansAuto Loan 60+ DelinquencySubprimePrime\n$1.6 T$1.3 TAuto Vehicle Fleet Composition(Trillions Of Dollars)*Mo tor Vehicle Loans Owned And Securitized By BanksMo tor Vehicle Loans On Consumer Balance Sheets (ARK Estimate)ROBOTAXIS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 130
        }
    },
    {
        "page_content": "132\n9001001,4003,2004002,2003,8001,90028,100\n$0$5,000$10,000$15,000$20,000$25,000$30,000\nRevenue (Net)EBITEnterprise Value$ BillionsRevenue, Earnings And Enterprise Value 2030 ARK EstimatesAutonomous ElectricAuto ManufacturersFleet OwnersAutonomous Platform ProvidersAutonomous Platform Providers Could Create ~$28 Trillion In Enterprise Value In 2030\nNumbers are rounded. EBIT = Earnings Before Interest and Taxes. Autonomous Platform Operators = Autonomous Ride-hail Companies, such as Waymo or Tesla. The chart on the left includes all publicly listed automakers with available CAPIQ data on Enterprise Value, Revenue, and Operating Income. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.At 15x EBIT in 2030, autonomous platform providers could scale to $28 trillion in enterprise value, or ~9x that of all auto manufactures in 2023. \n$2,750$200$3,100 $- $2,000 $4,000 $6,000 $8,000 $10,000 $12,000 $14,000\nRevenueEBITEnterprise Value$ BillionsRevenue, Earnings, And Enterprise Value 2023 Actual Auto ManufacturersROBOTAXIS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 131
        }
    },
    {
        "page_content": "133\n133Autonomous LogisticsReducing Costs And Reshaping Supply ChainsSources: ARK Investment Management LLC, 2024 Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Tasha Keeney, CFA Director of Investment Analysis & Institutional StrategiesDaniel Maguire, ACAResearch Associate Research By:BIG IDEAS 2024",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 132
        }
    },
    {
        "page_content": "134Autonomous logistics should reduce the cost of moving goods by 15-fold during the next five to ten years. Autonomous drones and robots have made millions of deliveries, while autonomous trucking companies have logged tens of millions of miles and are beginning to remove safety drivers. AI is proving superior to human pilots and drivers, encouraging regulators to allow truly autonomous operations that will change shopping behavior. Autonomous vehicles should impact health care by accelerating the delivery of life-saving supplies, particularly in emerging markets. According to ARK\u2019s research, autonomous delivery revenues could scale from essentially nil today to $900 billion in 2030. \nSources: ARK Investment Management LLC, 2024 Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.AUTONOMOUS LOGISTICS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 133
        }
    },
    {
        "page_content": "135\n$5.40 $0.35 Human-DrivenApp DeliveryDroneDeliveryLocal Small Item Delivery Cost(Per Trip)Autonomous Vehicles That Roll And Fly Could Lower Supply Chain Costs Dramatically\n$2.40 $0.40 Human-DrivenDeliveryRolling IntegratedTraffic Robot DeliveryLocal Batch Delivery Cost(Per Trip)\nNote: Drone price per mile has been updated with our latest assumptions for replacement costs, launching and charging infrastructure, insurance, and labor costs. Fees for drone and robot delivery are shown net of infrastructure costs (outside of charging and launch/land), which we believe could either be born by the drone or robot delivery operators or shared with logistics or retail partners. ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources as of December 7, 2023, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.According to our research, autonomous vehicles should operate at higher utilization rates than human-in-the-loop systems, creating more cost-effective last-mile delivery systems.\n-83%-94%$0.07 $0.03 Human-DrivenDiesel TruckAutonomousElectric TruckTruckload Delivery Cost(Per Ton-Mile)AUTONOMOUS LOGISTICS-57%\nHuman-Driven VehicleRobotDelivery",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 134
        }
    },
    {
        "page_content": "136\n2,500,000 1,900,000 1,500,000 1,500,000 895,000 \nKodiakPony.aiGatikEmbarkAuroraCumulative Miles Traveled\n29,000,000 5,000,000 300,000 70,000 20,000 \nAlibabaStarshipKiwibotServeRoboticsNuroCumulative Number of DeliveriesProprietary Data Is Likely To Determine Commercial Success In Autonomous LogisticsCompanies with more real-world data should have a competitive advantage. Verticalization and manufacturing partnerships also will be critical to success.\nNote: All truck miles traveled are latest available real-world reported miles; Gatik Class 6 trucks have operated commercially without a safety driver in some instances and the dashed navy lines are a representation of this. *Figures estimated based on available data. Robot delivery companies have different package capacities per robot, so some can make more deliveries per run than others. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources as of January 11, 2024, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.*\nFully AutonomousHuman In The Loop890,000 350,000 184,000 150,000 110,000 20,000 200 \nZiplineWingMeituanManna DroneDroneUpMatternetAmazonCumulative Number Of Commercial Flights\nFully AutonomousHuman In The LoopFully AutonomousHuman In The Loop**Drones\nAutonomous Trucks\nRolling Robots\nAUTONOMOUS LOGISTICS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 135
        }
    },
    {
        "page_content": "137AI Pilot Performance Seems Superior To That Of Human PilotsAI pilots have immense data advantages over humans. Zipline drones have logged more commercial flight miles than would have been possible by humans. In head-to-head simulated F-16 dogfights with a human expert fighter pilot, Shield AI won 5-0.*In drone races, AI trained by deep reinforcement learning outperformed professional human pilots 15 out of 25 times, with lap times ~10% faster.\n*Note: Heron Systems, now part of Shield AI, won the dogfight. ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources as of December 7, 2023, which may be provided upon request. Numbers are rounded. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.515010F-16 DogfightSimulatedDrone RacingCompetitionCompetitive WinsAI PilotHuman\n(University of Zurich Researchers vs. International Drone Racing Champions, 2023)(US Defense Advanced Research Projects Agency (DARPA) Challenge, Shield AI vs. Expert F-16 Pilot, 2020)*1,50065,800 769,000\nFAA Airline PilotTraining RequirementCommercial HumanPilot Flight Hours(Career Maximum PerFederal Age andFlight HourRestrictions)Zipline CommercialHours (FleetCumulative)Flight HoursAUTONOMOUS LOGISTICS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 136
        }
    },
    {
        "page_content": "138Boosted during and after COVID, food delivery fees have doubled the average cost of baseline menu orders. Beyond line-of-sight drones without visual observers should reduce food delivery costs dramatically, thanks to recent FAA approvals.Autonomous Drones Should Reduce Food Delivery Costs, Thanks To Regulatory Approvals\nNote: Numbers are rounded. ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources as of December 7, 2023, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.$24$12$0.35With Line of SightWith Visual ObserverAutonomousDrone Delivery Price(10-mile Trip)~50%~90%$0$2$4$6$8$10$12$14$16$18\nMealApp DeliverySelf PickupDrones(at scale)RollingRobots(at scale)Average Dollars Per OrderFood Delivery Costs (Pre-Tax)MealDelivery CostAUTONOMOUS LOGISTICS\nWith Line-Of-Sight",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 137
        }
    },
    {
        "page_content": "139Drones Are Saving LivesIn geographies without road infrastructure, Zipline drones can deliver blood in fewer than 15 minutes, improving the mortality associated with post-partum hemorrhages by 80%.\nSources: ARK Investment Management LLC, 2024, based on data from Jeon et al. 2022. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.0.0%0.5%1.0%1.5%2.0%2.5%3.0%3.5%4.0%\nBefore Drone DeliveryDrone delivery > 30 minutesDrone delivery 15-30 minutesDrone delivery < 15 minutesPostpartum Hemorrhage Mortality RateBefore And After Drone Delivery Of Blood Transfusions In RwandaAUTONOMOUS LOGISTICS\nDrone Delivery > 30 MinutesDrone Delivery 15-30 MinutesDrone Delivery < 15 Minutes",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 138
        }
    },
    {
        "page_content": "140Addressable Market For Last Mile Autonomous Food and Parcel Delivery* \n 1.3TAutonomous Food And Parcel Delivery Could Create A $1-2 Trillion Addressable Market$250B: Addressable Market For Parcel and Food Delivery In High-Cost Markets at $3.50-15.50 Per Trip\n$450B: Additional Demand in High-Cost Markets Based On Consumer\u2019s Value of Time$400B: Additional Demand For Parcel and Food Delivery In Low-Cost Markets at $0.60-1.50 Per Trip$200B: Additional Demand in Low-Cost Markets for Autonomous Delivery of $0.35-0.40 Per TripCumulative Number Of Delivery TripsPrice Per Trip 1.1T700B250B$3.50-$15.50\n$2.00-$2.90$0.60-$1.50$0.35-$0.40\n*Numbers in the graph are rounded. $1-2 Trillion is the addressable opportunity, but total revenues / market size by 2030 will depend on penetration rates, which are detailed in slides below. ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources as of December 7, 2023, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.AUTONOMOUS LOGISTICS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 139
        }
    },
    {
        "page_content": "141Global Autonomous Delivery Revenue Could Reach $900 Billion by 2030\nNote: ARK updates its research models often and most recently adjusted the adoption curve for autonomous technology, which resulted in a lower market forecast than previously estimated. ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources as of December 7, 2023, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Robot and drone food and parcel delivery fees could reach $450 billion in 2030, as affordable technology-enabled delivery reshapes consumer habits. Meanwhile, autonomous trucking revenues could reach $450 billion in 2030, as autonomous trucks coupled with drones and robotics transform the way that businesses transport goods cost effectively and quickly. \n300150450Robots & DronesLast MileTrucksMiddle MileAutonomous Delivery Revenue($ Billions, 2030)ParcelsFoodAll GoodsAUTONOMOUS LOGISTICS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 140
        }
    },
    {
        "page_content": "142Autonomous Logistics Could Double The Enterprise Value Of Precision AgricultureThanks to continued automation and yield improvements enabled by breeding, transgenics, and agricultural biologics, the operating cost per bushel produced\u2014a metric incorporating both cost and yield\u2014could decline by ~30% across major US crops.* Agricultural companies with per-acre business models could generate autonomous platform fees on the cost savings from the technology, achieving software-like margins.As a result, their collective enterprise value could roughly double to ~$600 billion at scale.**\n*This analysis focuses on \u201cMajor Crops\u201d\u2014Corn, Soybean, and Wheat\u2014which ARK defines as the top three crops in the US based on bushel production. Numbers are rounded. **When accounting for different cost compositions and adoption rates globally. This assumes a 50% autonomous platform fee and a 19X EV/EBITDA multiple on autonomous service earnings. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, using the latest available data as of January 4, 2024, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.$0$100$200$300$400$500$600$700\n2023At Scale(ARK Estimate)$ BillionsGlobal Agricultural And Farm Machinery Enterprise Value~2X\n $- $0.50 $1.00 $1.50 $2.00 $2.50 $3.00 $3.50 $4.00 $4.50\n2022At Scale(ARK Estimate)$/BushelUS Farm Operating Cost Per Bushel for Major Crops*Farm Operating Costs Per BushelAutonomous Platform Fee Per Bushel-30%AUTONOMOUS LOGISTICS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 141
        }
    },
    {
        "page_content": "143\n143Reusable RocketsOpening Outer Space For Business\nSources: ARK Investment Management LLC, 2024. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Research By:Daniel Maguire, ACAResearch Associate Sam KorusDirector of Research, Autonomous Technology& RoboticsBIG IDEAS 2024",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 142
        }
    },
    {
        "page_content": "144Reusable rockets are lowering launch costs dramatically, opening outer space for business and creating new services like direct-to-device satellite connectivity. According to ARK\u2019s research, satellite connectivity revenues could reach $130 billion in 2030, still just a fraction of the roughly $2 trillion in telecommunications revenue. Longer term, hypersonic flight point-to-point could generate revenues of ~$35 billion in 2030, and potentially reach $350 billion at scale.\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, including CompaniesMarketcap.com 2024, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.REUSABLE ROCKETS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 143
        }
    },
    {
        "page_content": "145\n $1 $10 $100 $1,000 $10,000 $100,000\n 1 100 10,000 1,000,000 100,000,000$/kg to Low Earth Orbit, 2023 DollarsCumulative Upmass(kg)SpaceX Launch Costs*Historical2023eForecastReusable Rockets Should Lower Launch Costs By An Order Of Magnitude\u2026Or Two!SpaceX\u2019s reusable rocket, Falcon 9, put an end to soaring launch costs. By reusing one Falcon 9 booster 19 times, SpaceX increased its annual launch cadence nearly 60% to 96 in 2023.\n$71 $210 $118 $164 $71  $- $50 $100 $150 $200 $250\n20062015$ Millions, 2015 DollarsRocket Launch CostsSoyuzAtlas VFalcon 9\n*Forecast timeline dependent on the speed of development of SpaceX's Starship. Sources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.REUSABLE ROCKETS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 144
        }
    },
    {
        "page_content": "146SpaceX Is Refurbishing Rockets In Record Time\nSources: ARK Investment Management LLC, 2024, based on data from NASA 2023 and SpaceX 2023a, 2023b as of December 11, 2023. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.When the Space Shuttle cost ~$1.5 billion per launch, industry experts assumed that a reusable rocket would be impossible economically. SpaceX then flipped the script.According to ARK\u2019s research, the first stage of the Falcon 9 cost <$1 million to refurbish. Now, rocket turnaround time should be proportional to the cost required to refurbish a rocket booster, the key metric in tracking launch cost declines. \n050100150200250\n20162018202020222024Number Of DaysFalcon 9 Average Time Between Reuses252 54356\n272125 - 50 100 150 200 250 300 350 400\nSpace Shuttle:Average 1972-2011Space Shuttle:Fastest 1985First SpaceXTurnaroundTimeSpaceX 2021FastestTurnaroundTimeSpaceX 2022FastestTurnaroundTimeSpaceX 2023FastestTurnaroundTimeNumber Of DaysRocket Turnaround TimeREUSABLE ROCKETS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 145
        }
    },
    {
        "page_content": "147Lower Satellite Launch Costs Should Enable Continuous Global Coverage With Low Latency\nSources: ARK Investment Management LLC, 2024. based on data from Starlink 2023, SES/ViaSattelite 2023, NEONE 2023 . Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.While latency precluded geostationary orbit (GEO) satellites from offering a compelling broadband internet solution, now thousands of low-cost, low earth orbit (LEO) satellites can provide service with low latency, continuous global coverage, and direct-to-mobile device connectivity.\nDebris will fall back to earth within ~5 years Debris will fall back to earth within 1,000+ years GEO ~22,000 miles700 ms latencyLEO~300 miles<40 ms latencyREUSABLE ROCKETS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 146
        }
    },
    {
        "page_content": "148\nUpmass Necessary To Maintain A 42,000 Satellite Constellation\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, including Brycetech 2023a, 2023b, 2023c, and McDowell 2024 as of January 23, 2024, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Starship Will Help The StarlinkConstellation Achieve Its Potential Starship\u2019s payload capacity to LEO is ~5x that of the Falcon 9. While impressive, given the five-year life of its satellites, Starship still will have to fly every 3.5 days to maintain its target constellation of 42,000 Starlink satellites. As of January 2024, SpaceX has a constellation of ~5,400 satellites.\n~80% - 200 400 600 800 1,000 1,200 1,400 1,600\n2023ekg ThousandsSpacecraft UpmassSpaceXEx-SpaceX\n3.5Days Between LaunchesStarshipAnnual UpmassCapability By Launch CadenceREUSABLE ROCKETS\n - 5,000 10,000 15,000 20,000\n0. 512.5(2024 Goal)3.8(2023 Cadence)kg Thousands\nDays Between LaunchesFalcon 9Annual UpmassCapabilityBy Launch Cadence",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 147
        }
    },
    {
        "page_content": "149Small Launch Providers Proliferated But May Not Be The Winners In Space\nSources: ARK Investment Management LLC, 2024, based on data from NewSpace Index 2023. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.After capital spending booms, industries tend to consolidate. In the space industry, while launch capability is critical, the larger opportunity could be in the services enabled by low launch costs. Today, only 16 of the 186 small launch providers created since 1996 are operational.\n9181543434216\n020406080100120140160180200NumberStage Of Small Launch Providers Created Since 1996RetiredCancelledConceptDormantIn Development >5 YearsIn Development <=5 YearsOperational\n05101520253019961997199819992001200220032004200520062008200920102011201220132014201520162017201820192020202120222023NumberNew Small Launch Providers FoundedREUSABLE ROCKETS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 148
        }
    },
    {
        "page_content": "150Antenna Costs Continue To Decline\nSources: ARK Investment Management LLC, 2024, based on data from SpaceX as of September 2023. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.SpaceX currently produces user terminals for less than the $599 it charges customers. Lower antenna costs should enable SpaceX to scale Starlinkprofitably. \n - 500,000 1,000,000 1,500,000 2,000,000 2,500,000\n020040060080010001200Number\nDays Since Starlink LaunchStarlinkSubscribers\n$450 2028e $100 $1,000 $10,000\n 1 100 10,000 1,000,000 100,000,000Unit Cost(Log Scale)\nCumulative Production(Log Scale)StarlinkAntenna Costs\nLaunch: February 20212.3 million Dec. 2023REUSABLE ROCKETS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 149
        }
    },
    {
        "page_content": "151ARK believes direct to device capability will be adopted by all telecom operators over time.Satellite Connectivity Revenues Could Exceed $130 Billion Per Year\n*Forecasts. Source: ARK Investment Management LLC, 2024. This ARK forecast is based on a range of data sources, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.0. 001%0. 010%0. 100%1.000%10.000%100.000%\n1997200720172027Satellite Subscribers As Percent Of Cellular Subscribers(Log Scale)Satellite Subscribers As Percent Of Cellular Subscribers\n11 Million$1,620~$18 BillionRVs\n25 Thousand$225,000~$6 BillionCommercial Aircraft Fleet\n100 Thousand$60,000~$6 BillionCruise Ships,Warships,Commercial ShipsRecreational Boats8.5 million$1,620~$14 Billion\nGlobal HouseholdsWithout Access To Broadband600 Million$60~$40 BillionAddressable Subscribers*Annual Revenue*Direct to Device8 billion$6~$48 Billion\nAnnual Addressable Market*\nTotal: ~$132 BillionSpaceX direct to device with T-Mobile, when at scaleREUSABLE ROCKETS\nForecast",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 150
        }
    },
    {
        "page_content": "152By 2030, Hypersonic Flight Could Be A ~$35 Billion Market, Ready To Scale To ~$350 Billion Longer Term\n*Forecast. Sources: ARK Investment Management LLC, 2024. This ARK forecast is based on a range of data sources, including Brycetech and Saic 2021, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.According to the US Department of Transportation, leisure travelers are willing to spend 60%-90% of their estimated hourly household income to save one hour.*Compared to conventional flights that can take 28 hours roundtrip, ARK estimates that hypersonic flights could take just 6 hours, saving each traveler ~22 hours.Given the typical cost and potential time savings, ARK\u2019s research suggests that a first-class passenger should be willing to spend $44,000 roundtrip for a hypersonic flight.If launch costs decline in line with ARK\u2019s expectations, early adopters of hypersonic flight could generate $35 billion revenue by 2030.\n110kg$200/kg to LEO$44,000StarshipPrice2RoundtripTotal number of airline passengers worldwide: 6.7 billion5% of flights are long-haul5% of passengers are first-classNumber of passengers on long-haul flights: ~335 millionNumber of passengers flying first-class: ~16 million50% adoption at maturityNumber of passengers flying hypersonic: ~8 million$44,000 roundtrip ticketAnnual addressable market: ~$350 billionBuilding Blocks Of Addressable Market ForecastREUSABLE ROCKETS",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 151
        }
    },
    {
        "page_content": "153\n1533D PrintingReshaping Manufacturing\nSources: ARK Investment Management LLC, 2024. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Tasha Keeney, CFA Director of Investment Analysis & Institutional StrategiesDaniel Maguire, ACAResearch Associate Research By:BIG IDEAS 2024",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 152
        }
    },
    {
        "page_content": "154In automotive manufacturing, 3D printing has lowered both the part count and the product development timeline dramatically. As a result, automakers can carry less inventory and save on tooling costs. In healthcare, 3D printing is making novel surgeries possible with customized guides, tools, and implants. 3D printing also should provide positive environmental benefits relative to traditional manufacturing. Thus far, companies using 3D printing have benefited more than the 3D printing equipment manufacturers. In the future, data feedback loops could change that dynamic.According to ARK\u2019s research, 3D printing revenues could scale ~40% at an annual rate during the next seven years, from ~$18 billion today to ~$180 billion in 2030. Note numbers are rounded. Sources: ARK Investment Management LLC, 2024. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.3D PRINTING",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 153
        }
    },
    {
        "page_content": "155Thanks To 3D Printing, Automotive Production Has Entered Unchartered Territory\n400 parts \u00e0 1 castingReportedly, Tesla is experimenting with 3D printed sand molds to cast auto underbodies that could substitute one part for 400 parts, lowering automotive development timelines and mold design validation costs by 50% and 97%, respectively. 3D printing could play a role in the production of every car.\n0. 00. 51.01.52.0\nMetal Mold3D Printed Sand Mold$ MillionsDesign Validation Cost-97%\nImagery sourced from Lambert 2022. Sources: ARK Investment Management LLC, 2024, based on data from Shirouzu 2023. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.3-4 Years18-24 MonthsHistoricalAverageAverageWith 3D PrintingVehicle Development Time-50%3D PRINTING\n",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 154
        }
    },
    {
        "page_content": "156Across a range of surgeries, 3D printed tools, guides, and models increased performance, as measured by surgical accuracy and results, by ~40-50% and reduced operating time on average by ~30%.3D Printing Has Played A Role In Medical BreakthroughsIn fewer than 24 hours after identifying the donor, Materialise 3D printed pivotal surgical tools and guides used in the world\u2019s first eye transplant. Speed to operation is critical to preserving donor tissue deprived of blood supply.\n-60%-40%-20%0%20%40%60%80%\nPerformanceTimeDuring Surgeries, 3D Printed Tools, Guides, and Models Shorten Time And Improve AccuracyPreoperative PlanningSurgical Tool/Guide\nNote: Time Savings and Accuracy Improvements Provided by 3D Printed Surgical Guides and Preoperative Planning Aides: bars represent the average percent improvement in time or performance as described in Bergmann et al. 2017 and Woodard et al. 2019, N=6-9 for each sample group. Error bars represent +/- standard error. The above analysis was conducted across medical fields; however, oral maxillofacial surgery and musculoskeletal studies were the most prevalent. +/- Standard ErrorPercent ImprovementDonor\nPatient\nSources: ARK Investment Management LLC, 2024. This ARK analysis is based on a range of underlying data from external sources, including Diment et al. 2017, Meara et al. 2015, and Dobson 2020 as of January 17, 2024, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.3D PRINTING",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 155
        }
    },
    {
        "page_content": "157SpaceX uses 3D printing every day to make parts for Starship\u2019s Raptor engines. Today, the operating margins of SpaceX\u2019s launch and satellite business are superior to those of any 3D printing supplier. Industrial companies benefiting from 3D printing could vertically integrate to sustain their competitive advantages.Thus Far, 3D Printing Has Benefited Users More Than Suppliers\n$108$103-$71$180,000\n$9,000$3,000Enterprise ValueRevenueEBITVelo3D And SpaceX2023 Estimates In ThousandsVelo3DSpaceX\nA SpaceX Super Heavy Booster With 33 Raptor Engines:\nVelo3D is a 3D-printer manufacturer specializing in support-free powder bed fusion. Sources: ARK Investment Management LLC, 2024, based on data from S&P Capital IQ, 2024. SpaceX Heavy Booster Illustration sourced from Ali 2021. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.3D PRINTING",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 156
        }
    },
    {
        "page_content": "158Software-Defined 3D Printers Could Shift Some Economics Back To Printer ManufacturersWith sensor-equipped 3D printers, 3D printing equipment manufacturers can collect data from customer print jobs and improve their fleets of printers in the field with over-the-air software updates. This data feedback loop could help 3D printing companies capture more economics than they do today. While companies may be reluctant to share data, AI-enabled manufacturing solutions should create better outcomes for 3D printing equipment companies and their customers\nEBITDA: Earnings before interest, taxes, depreciation, and amortization. Sources: ARK Investment Management LLC, 2024, based on data from S&P Capital IQ, 2024. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results.Real Time Printing Data From Sensors on Customer Printers:GeometryTemperature MoistureSent To 3D Printer ManufacturerOver-The-Air Updates Improve Each Print\n-30%-20%-10%0%10%20%30%40%50%\nAverage of 3D Printer Manufacturers(Latest 12 Months)Illinois Tool Works(Latest 12 Months)Margin Structure3D Printing Manufacturers Vs. Mature Tools CompanyGross MarginEBITDA Margin3D PRINTING",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 157
        }
    },
    {
        "page_content": "1593D Printing Revenues Could Grow ~40% At An Annual Rate To $180 Billion By 2030\n $- $20 $40 $60 $80 $100 $120 $140 $160 $180 $200\n20232024202520262027202820292030$ Billions3D Printing Revenue Forecast\nNote numbers are rounded. Sources: ARK Investment Management LLC, 2024 and S&P Capital IQ, 2024.  This ARK analysis is based on a range of underlying data from external sources as of January 17, 2024, which may be provided upon request. Forecasts are inherently limited and cannot be relied upon. For informational purposes only and should not be considered investment advice or a recommendation to buy, sell, or hold any particular security. Past performance is not indicative of future results. $- $0.5 $1.0 $1.5 $2.0 $2.5 $3.0 $3.5 $4.0\nAutomobiles andComponentsAerospace andDefenseHealth CareEquipment andSuppliesFootwear$ TrillionsRevenue By IndustrySelect Industries Using 3D Printing(Latest 12 Months As of 1/18/24)\nCompanyExamples:TeslaVolkswagenFordGeneral MotorsBMWSpaceXLockheed MartinNikeAdidasStrykerAlign3D PRINTING",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 158
        }
    },
    {
        "page_content": "160\n160Works CitedBIG IDEAS 2024",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 159
        }
    },
    {
        "page_content": "161WORK CITEDAli, I. 2021. \u201cSpaceX ramps up Raptor engine production as Starship booster engine count will rise to 32 this year.\u201d Teslaoracle.ARK Investment Management LLC. 2023. \u201cBig Ideas 2023.\u201dArtemis Terminal Dashboard. 2023. \u201cChains: Arbitrum, Base, Linea, Optimism, Polygon zkEVM, Scroll, StarkNet, zkSync Era, Zora Network.\u201dBekar, C. et al. 2017. \u201cGeneral Purpose Technologies in Theory, Applications and Controversy: A Review.\u201d CORE.Benaich, N. 2023 \u201cState of AI Report.\u201d Air Street Capital.Biomedtracker. 2023.Bloomberg. 2023.Bomasani, R. et al. 2023. \u201cLanguage Models Are Changing AI: The Need for Holistic Evaluation.\u201d  Center for Research on Foundation Models. Stanford University .Brycetech. 2023a. \u201cGlobal Orbital Space Launches, Q1 2023.\u201d   Brycetech. 2023b. \u201cGlobal Orbital Space Launches, Q2 2023.\u201d  Brycetech. 2023c. \u201cGlobal Orbital Space Launches, Q3 2023.\u201d   CDC. 2024. \u201cRoad Traffic Injuries and Deaths\u2014A Global Problem.\u201d Chung, G. 2021. \u201cGlobal Investable Assets Reach Record $250 Trillion.\u201d Institutional Investor.CompaniesMarketcap.com 2024. \u201cTop publicly traded telecommunication companies by revenue.\u201d Corva, F. 2022. \u201cHow Far We\u2019ve Fallen: Lessons Learned in the Aftermath of the Terra (LUNA) Ecosystem Crash.\u201d Nasdaq.Crafts, N. 2004. \u201cSteam as a General Purpose Technology: A Growth Accounting Perspective.\u201dThe Economic Journal. Cui, M. and Zhang, L. 2022. \u201cHigh-throughput proteomics: a methodological mini-review.\u201dLab Invest.Dao, T . 2023. \u201cFlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning.\u201d  Center for Research on Foundation Models. Stanford University.DeGusta, M. 2012. \u201cAre Smart Phones Spreading Faster than Any Technology in Human History?\u201d MIT Technology Review. Dell'Acqua, F. et al. 2023. \u201cNavigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality.\u201d Working Paper 24-013. Harvard Business School.Diment, L. et al. 2017. \"Clinical efficacy and effectiveness of 3D printing: a systematic review.\" BMJ Publishing Group Ltd.Dobson, G. 2020. \"Trauma of major surgery: A global problem that is not going away.\" National Library of Medicine. Dreiman, G. et al. 2021. \u201cChanging the HTS Paradigm: AI-Driven Iterative Screening for Hit Finding.\u201d SLAS Discov.EVVolumes. 2023. \u201cThe Electric Vehicle World Sales Database.\u201d Gao, Steven. 2024. \u201cOnline Gambling - December 2023.\u201d Yipit Data. Glassnode. 2023.Hawkins, A. J. 2023. \u201cFord will cut weekly production of F-150 Lightning in response to slowing demand.\u201d The Verge.Hwang et al. 2018. \u201cSingle-cell RNA sequencing technologies and bioinformatics pipelines.\u201dExp Mol Med.Insider Intelligence Inc. 2023a. \u201cTime Spent with Connected TV, US.\u201d Insider Intelligence Inc. 2023b. \u201cTime Spent with TV, US.\u201d Insider Intelligence Inc. 2023c. \u201cTV Viewers, US.\u201d Insider Intelligence Inc. 2023d. \u201cConnected TV Households, US.\u201dInsider Intelligence Inc. 2023e. \u201cConnected TV Ad Spending, US.\u201d International Federation of Robotics. 2023. \u201cWorld Robotics: Industrial Robots 2023.\u201dJeon et al. 2022. \u201cLeapfrogging for Last-mile Delivery in Health Care: Drone Delivery for Blood Products in Rwanda.\u201d Available at SSRN.Kusano, K. D. 2023. \u201cComparison of Waymo Rider-Only Crash Data to Human Benchmarks at 7.1 Million Miles.\u201d arXiv.Lambert, F. 2022. \u201cTesla releases impressive footage of robot pulling still smoking casting from Giga Press.\u201d Electrek.Leviathan, Y. et al. 2022 \u201cFast Inference from Transformers via Speculative Decoding.\u201d arXiv.Life architect. 2023. \u201cAI + IQ testing (human vs AI).\u201dMa et al. 2023 \u201cEureka: Human-Level Reward Design via Coding Large Language Models.\u201d arXiv.Magna Global. 2023. \u201cGlobal Advertising Forecast - December 2023 Release.\u201d S&P Global Market Intelligence.Masimov, E. et al. 2016. \u201cGenerating Images from Captions with Attention.\u201d ICLR Conference Paper.McDowell, J. 2024. \u201cStarlink Launch Statistics.\u201d Jonathan\u2019s Space Pages. McKinsey & Company. 2018. \u201c2018 McKinsey Global Payments Report.\u201d McKinsey & Company. 2019. \u201c2019 McKinsey Global Payments Report.\u201d McKinsey & Company. 2020. \u201c2020 McKinsey Global Payments Report.\u201d McKinsey & Company. 2021. \u201c2021 McKinsey Global Payments Report.\u201d McKinsey & Company. 2022. \u201c2022 McKinsey Global Payments Report.\u201d McKinsey & Company. 2023a. \u201c2023 McKinsey Global Payments Report.\u201d McKinsey & Company. 2023b. \u201cThe Global Banking Annual Review 2023: The Great Banking Transition.\u201d McKinsey & Company. 2023. \u201cEconomic potential of generative AI.\u201dMcKinsey & Co. 2023. \u201cUnleashing developer productivity with generative AI.\u201dMcKinsey Global Institute. 2017. \u201cA Future that Works.\u201dMeara, J.G. et al. 2015. \"Global Surgery 2030.\" The Lancet Commission on Global Surgery. Metaculus. 2023. \u201cWhen will the first general AI system be devised, tested, and publicly announced?\u201dMidjourney. 2023. ",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 160
        }
    },
    {
        "page_content": "162WORK CITEDMihalascu, D. 2023.\u201d \u201cVW Group Delays EV Battery Plant In Europe Amid \u2018Sluggish\u2019 EV Demand.\u201d Inside EVs. Nasa. 2023. \u201cSpace Shuttle Launches.\u201dNHTSA. 2023. \u201cNHTSA Estimates for 2022 Show Roadway Fatalities Remain Flat After Two Years of Dramatic Increases.\u201dUnited State Department of Transportation.Nichol, A. et al. 2022. \u201cPoint-E: A System for Generating 3D Point Clouds from Complex Prompts.\u201d ArXiv. Nielsen. 2021a. \u201cThe Gauge Shows Streaming Is Taking a Seat at the Table.\u201d Nielsen.Nielsen. 2021b. \u201cThe Gauge Shows Seasonality, Sports and Streaming Content Drive Shifts in Viewing.\u201d Nielsen. Nielsen. 2022. \u201cStreaming Claims Largest Piece of TV Viewing Pie in July.\u201d Nielsen. Nielsen. 2023a. \u201cStreaming Grabs a Record 38.7% of Total TV Usage in July, with Acquired Titles Outpacing New Originals.\u201d Nielsen. Nielsen. 2023b. \u201cSports and News Boost Broadcast and Cable Viewing in August; Streaming Declines as Kids Head Back to School. Nielsen. 2023c. \u201cSports Gave Broadcast Channels a Second Straight Month of Viewing Gains in September.\u201dNielsen. 2023d. \u201cSports Continues to Fuel Broadcast Gains in October; Streaming Surrenders Almost a Full Share Point.\u201dNielsen. 2023e. \u201cBroadcast Programming Sets the Table for Thanksgiving.\u201dNielsen. 2024. \u201cTV Usage Rises in December; Video Gaming Drives Big Gains.\u201dNEONE. 2023. \u201cWhy is satellite latency high?\u201dO\u2019Mahony, M. and Timmer, M. P. 2009. \u201cOutput, Input and Productivity Measures at the Industry Level: The Eu Klems Database.\u201dThe Economic Journal.OpenAI. 2024.Orphanet. 2023Patel D. and Kostotovic, A. 2023. \u201cTPUv5e: The New Benchmark in Cost-Efficient Inference and Training for <200B Parameter Models.\u201d SemiAnalysis.Paul et al. 2010. \u201cHow to improve R&D productivity: the pharmaceutical industry's grand challenge.\u201dNat Rev Drug Discov.Peters-Clarke, T. et al. 2023. \u201cInstrumentation at the Leading Edge of Proteomics.\u201d ChemRxiv.PortfolioVisualizer. 2023.Recursion Pharmaceuticals. 2024. \u201cDecoding Biology to radically improve lives.\u201dRosevear, J. 2023. \u201cFord will postpone about $12 billion in EV investment as buyers become more cautious.\u201d CNBC.Schreiber, K. 2022. \u201cMachine Learning in Drug Discovery Symposium: What\u2019s Holding Back AI in Drug Discovery?\u201d The Broad Institute.Sensor Tower. n.d. \u201cMonthly Active Users by Unified Apps.\u201d Sensor Tower. Accessed January 2, 2024. SES/ViaSatellite. 2023. \u201cGEO, MEO, and LEO: How orbital altitude impacts network performance in satellite data services.\u201dS&P Capital IQ. 2024. \u201cESSENTIAL INTELLIGENCE: S&P CAPIT AL IQ PLA TFORM.\u201dShepardson, D. & Klayman, B. 2023. \u201cGM delays EV truck production at Michigan plant by year.\u201d Reuters.Shirouzu, T. 2023. \u201cTesla reinvents carmaking with quiet breakthrough.\u201d The Japan Times.Sirohi, A. 2023. \u201cWhat is the Average Email Marketing ROI?\u201d Constant Contact.SpaceX. 2023a.\u201dCores.\u201d Reddit.SpaceX. 2023b. \u201cB1049.\u201d Reddit.Anthropic. 2024Starlink. 2023. \u201cStarlink Specifications.\u201d Tesla. 2023. \u201cTesla Investor Day: Master Plan 3.\u201d Keynote. Tesla. 2024. \u201cTesla Vehicle Safety Report.\u201dThakrar et al. 2020 \u201cReducing Mortality from Air Pollution in the United States by Targeting Specific Emission Sources.\u201d Environ. Sci. Technol. Lett.Token Terminal. 2023. Touvron, H. et al. 2023. \u201cLlama 2: Open Foundation and Fine- Tuned Chat Models.\u201d arXiv.Transport Policy. 2023. \u201cVehicle Definitions.\u201d Wayve. 2023. \u201cLINGO-1: Exploring Natural Language for Autonomous Driving.\u201dWorldpay Inc. 2019. \u201c2019 Global Payments Report.\u201d Worldpay. 2019. Worldpay Inc. 2020. \u201c2020 Global Payments Report.\u201d Worldpay Inc. 2021. \u201c2021 Global Payments Report.\u201d Worldpay Inc. 2022. \u201c2022 Global Payments Report.\u201d Worldpay Inc. 2023. \u201c2023 Global Payments Report.\u201d Yang, C. et al. 2023. \u201cLarge Language Models as Optimizers.\u201d arXiv.Ycharts. 2023. Zhang, L. 2023. \u201cCruise\u2019s Safety Record Over 1 Million Driverless Miles.\u201d Cruise.",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 161
        }
    },
    {
        "page_content": "163DISCLOSURE\u00a92021-2026, ARK Investment Management LLC. No part of this material may be reproduced in any form, or referred to in any other publication, without the express written permission of ARK Investment Management LLC (\u201cARK\u201d).Please note, companies that ARK believes are capitalizing on disruptive innovation and developing technologies to displace older technologies or create new markets may not in fact do so and/or may face political or legal attacks from competitors, industry groups, or local and national governments. ARK aims to educate investors and to size the potential opportunity of Disruptive Innovation, noting that risks and uncertainties may impact our projections and research models. Investors should use the content presented for informational purposes only, and be aware of market risk, disruptive innovation risk, regulatory risk, and risks related to Deep Learning, Digital Wallets, Battery Technology, Autonomous Technologies, Drones, DNA Sequencing, CRISPR, Robotics, 3D Printing, Bitcoin, Blockchain Technology, etc. Cryptocurrency Risk. Cryptocurrencies (also referred to as \u201cvirtual currencies\u201d and \u201cdigital currencies\u201d) are digital assets designed to act as a medium of exchange. Cryptocurrency is an emerging asset class. There are thousands of cryptocurrencies, the most well-known of which is bitcoin. Cryptocurrency generally operates without central authority (such as a bank) and is not backed by any government. Cryptocurrency is not legal tender. Federal, state and/or foreign governments may restrict the use and exchange of cryptocurrency, and regulation in the U.S. is still developing. The market price of bitcoin and other cryptocurrencies have been subject to extreme fluctuations. Similar to fiat currencies (i.e., a currency that is backed by a central bank or a national, supra-national or quasi-national organization), cryptocurrencies are susceptible to theft, loss and destruction. Cryptocurrency exchanges and other trading venues on which cryptocurrencies trade are relatively new and, in most cases, largely unregulated and may therefore be more exposed to fraud and failure than established, regulated exchanges for securities, derivatives and other currencies. Cryptocurrency exchanges may stop operating or permanently shut down due to fraud, technical glitches, hackers or malware, which may also affect the price of cryptocurrencies. Cryptocurrency Tax Risk. Many significant aspects of the U.S. federal income tax treatment of investments in bitcoin and other cryptocurrencies are uncertain and still evolving.The content of this presentation is for informational purposes only and is subject to change without notice. This presentation does not constitute, either explicitly or implicitly, any provision of services or products by ARK and investors are encouraged to consult counsel and/or other investment professionals as to whether a particular investment management service is suitable for their investment needs. All statements made regarding companies or securities are strictly beliefs and points of view held by ARK and are not endorsements by ARK of any company or security or recommendations by ARK to buy, sell or hold any security. Historical results are not indications of future results. Certain of the statements contained in this presentation may be statements of future expectations and other forward-looking statements that are based on ARK's current views and assumptions and involve known and unknown risks and uncertainties that could cause actual results, performance or events to differ materially from those expressed or implied in such statements. The matters discussed in this presentation may also involve risks and uncertainties described from time to time in ARK's filings with the U.S. Securities and Exchange Commission. ARK assumes no obligation to update any forward-looking information contained in this presentation. Certain information was obtained from sources that ARK believes to be reliable; however, ARK does not guarantee the accuracy or completeness of any information obtained from any third party. ARK and its clients as well as its related persons may (but do not necessarily) have financial interests in securities or issuers that are discussed. ARK Investment Management LLC For more research on disruptive innovation visit www.ark-invest.com163\n",
        "metadata": {
            "source": "pdf/ark.pdf",
            "page": 162
        }
    },
    {
        "page_content": "The Role of AR and VR Technologies in Education\nDevelopments: Opportunities and Challenges\n1stHadi Ardiny\nIntelligent, Distributed and\nAutonomous Systems (IDAS) Laboratory, IUST\nTehran, Iran\nhadiardiny@yahoo.com2ndEsmaeel Khanmirza\nIntelligent, Distributed and\nAutonomous Systems (IDAS) Laboratory, IUST\nTehran, Iran\nkhanmirza@iust.ac.ir\nAbstract \u2014Technology has been growing fast and noticeably\nin\ufb02uencing different aspects of life such as education. Studies\nhave revealed that (AR) and virtual reality (VR) have strong\npotentials for helping students to improve their skills and\nknowledge. In fact, bridging AR/VR and education can bring\nteaching and learning experiences in an attractive and effective\nway.\nIn this review paper, we initially present an introduction to and\na de\ufb01nition of AR/VR. We then brie\ufb02y study ongoing research\nand latest products in AR/VR, that have pedagogical values and\npotentials to improve educational systems. We then highlight the\ncapabilities and limitations of AR/VR to identify what AR/VR\ncan provide for learners and teachers.\nIndex Terms \u2014Augmented reality, Virtual reality, Education,\nTechnology, Opportunities, Challenges\nI. I NTRODUCTION\nIn recent years, technology has been growing fast and\nnoticeably in\ufb02uencing different aspects of life; our thinking,\nhabits, social activities, and lifestyle are all changed in differ-\nent ways compared to a few years ago. Regardless of positive\nand negative impacts of technology, a purpose of technology\nis typically to increase productivity in the industries, to ease\nlife, or to improve education. Accordingly, the development\nof education systems and learning methods is always a part of\nresearch programs, which includes the use of new technologies\nto take into account educational issues. Imperfections and\nchallenges in current education systems such as accessibility,\nfunding, autonomy, one-size-\ufb01ts-all approach, and big changes\nin future jobs indicate teachers need to employ new methods\nfor improving education [1, 2, 3]. Integrating technology into\neducation allows to facilitate learning methods and improve\nlearning performance by creating and managing appropriate\ntechnological materials [4]. In addition, this integration pro-\nmotes students\u2019 skills to learn how to use new technologies\nin their future life. Jobs and future requirements change at\na fast speed, so students must be prepared to adapt to new\nenvironments and be proactive.\nEmerging and advanced technologies such as robotics, ar-\nti\ufb01cial intelligence (AI), cloud computing, and 3D printing\nare reshaping education systems. For instance, search engines\ncan answer a lot of questions that people might need to\nThe research was supported by the Iranian National Elites Foundation and\nIUST.memorize too much information to \ufb01nd solutions for them,\nso students need to learn to be differentiated from automated\nmachines. In this research, we address virtual reality (VR)\nand augmented reality (AR) technologies for improvements of\nlearning processes. In Dale\u2019s Cone of Experience that shows\nthe progression of learning experiences from the bottom of\nthe cone (learning by doing direct experiments) to the top of\nthe cone (learning through abstracts), learners involve more\nthe bottom rather than to be just spectators at the abstract\nlevel [5]. At the bottom, learners have opportunities to sense\nand understand their new knowledge in real life with learning\ncontexts. AR and VR technologies can be embedded at the\nlowest level of the cone; they can enrich environments where\nstudents can learn with the help from the most of their \ufb01ve\nsenses. Furthermore, students will be able to discover new\nknowledge, motivate to learn, develop their own experiences\nwith the help of AR/VR [6, 7]. In this research after the\nde\ufb01nitions of AR and VR in Section II, we present these\npotentials along with AR/VR products in Section III. Section\nIV provides a discussion on AR/VR and gives an overview\nof opportunities and challenges in the implementation of\nthese two technologies for learners and teachers. Finally, we\nconclude in Section V.\nII. D EFINITIONS\nAR and VR technologies are related but they are different\nthings. Before we begin to de\ufb01ne these terms precisely, \ufb01rst\nwe need to know what real andvirtual are. In Figure 1 which\npresented by Milgram et al. [8], the virtual environment and\nreal environment are two ends of a spectrum. Regardless of a\ncomplicated concept of reality in the philosophy, we consider\na real-world environment where it is limited to laws of physics\nand things in real-world can be sensed directly as they actually\nexist. In contrast, a virtual world is a computer-based simulated\nenvironment [9] which may or may not follow laws of physics\nsuch as time, gravity, material properties and so on. In a wider\nde\ufb01nition, participants are totally immersed in a virtual world\nthat synthesized by computers. In contrast, participants are\ndirectly presented in the real world [8]. The spectrum between\nreal and virtual environment was called Reality-Virtuality (RV)\ncontinuum [8] and the different types of AR or VR can be\nplaced on this continuum. Schnabel et al. [10] presented a\n978-1-7281-0127-9/18/$31.00 \u00a92018 IEEE\nProceedings of the 6th RSI\nInternational Conference on Robotics and Mechatronics (IcRoM 2018)\nOctober 23-25, 2018, Tehran, Iran\n482\nAuthorized licensed use limited to: Chalmers University of Technology Sweden. Downloaded on February 05,2024 at 17:06:14 UTC from IEEE Xplore.  Restrictions apply. ",
        "metadata": {
            "source": "pdf/test.pdf",
            "page": 0
        }
    },
    {
        "page_content": "Fig. 1. A representation of a Reality-Virtuality (RV) Continuum [8]\nclassi\ufb01cation to indicate the differences in the different types\nof AR and VR. As illustrated in Figure 2, from the left\nto right, the degree of reality decreases; so a more detailed\nclassi\ufb01cation of AR and VR is shown on the continuum\nbased on their attributes, activities, and designs. Although this\nFig. 2. A different classi\ufb01cation ranging from the real environment to the\nvirtual environment [10]\nclassi\ufb01cation is more precise, most products in AR/VR do not\napply this terminology. Therefore, we address research and\nproducts according to a more general framework. In Figure\n3, the RV continuum is divided into two main subcategories\nAR and VR technologies; it is more convenient to study the\nresearch in the literature.\nFig. 3. A general representation for AR and VR classi\ufb01cation\nA. AR (augmented reality)\nAccording to the framework that is shown in Figure 3, AR\ncan be de\ufb01ned as an interactive experience in the real world\nenvironment where the computer-generated information and\nelements are linked to the real world. The computer-generated\ninformation is virtual content that is synthesized with the\nhelp of multiple sensors (i.e., camera, microphone, GPS) and\nhaptic devices. The AR productions can take place in three\nsteps: \ufb01rst, all real-world data is collected by various sensors.\nSecond, this information is then analyzed and additional\ninformation from different information sources. Finally, the\ngained information is displayed as digital elements.B. VR (virtual reality)\nIn contrast to AR, VR takes place within an arti\ufb01cial\nenvironment and a participant becomes a part of this arti\ufb01cial\nworld as an immersive or a non-immersive member. The\npeople can interact and manipulate computer-generated objects\nin a virtual environment with the help of some gadgets like\nhaptic devices. In addition, VR gadgets have in\ufb02uenced VR\ncontent and enhances VR capabilities for better experiences.\nFor instance, smell, wind, sounds, heat, body movement detec-\ntion are elements that potentially create VR experiences more\nreal and interesting.\nIII. AR/VR P RODUCTS\nIn the literature, AR/VR products tend to create new ex-\nperiences in several \ufb01elds, from education to games. Here,\nwe studied VR/AR products in two categories: hardware and\nsoftware.\nA. AR/VR hardware\nVR and AR technologies need to mix real environment\ninformation and computer-generated objects properly to create\ndesirable experiences but participants never are able to have a\nperfect feeling of VR/AR content without tools and gadgets.\nThe AR/VR tools work based on human perceptions and can\nengage the several human senses. The tools can be visual,\nauditory, haptic, olfactory devices and the external devices like\nposition system can incorporate in these tools.\nThe visual perception in VR can be done either by a\nhead-mounted display (HMD), in a space with back-projected\nstereo projection screens around (e.g., CA VE1), or a desktop\ndisplay. An HMD is a box worn on the head and consists\nof a one or two small displays in front of one or each eye.\nThe Google Cardboard is a low-cost HMD developed by\nGoogle [11]. A smartphone is placed into the back of lenses\nto present content to viewers. This cardboard is also linked\nto a software development kit (SDK) that provide a designing\nplatform to simply produce VR content for the Android and\niOS operating systems. The HTC Vive, Sony PlayStation VR,\nand Samsung Gear VR are other HMD products that are\nmostly targeted for games and entertainment purposes (Table\nI). In education, price, quality, and user-friendly are important\nfactors for student use. The Google Cardboard seems to be a\ngood choice for education but the quality of a display is also\ncan be taken into account. Because, side effects of VR such\nas dizziness and eye fatigue may encourage students to use\nbetter ones, therefore the expensive HMD may be considered\nfor education. The HMD is also developed for AR applica-\ntions. AR HMDs look like eyeglass with components such\nas camera, IMU, microphone. Users see a real environment\noptically and computer-generated elements appear on the glass\nsimultaneously. Although AR HMDs are relatively costly for\neducation but it can revolutionize classrooms by presenting\n3D models and other interesting demonstrations (Table I).\n1Cave automatic virtual environment\n483\nAuthorized licensed use limited to: Chalmers University of Technology Sweden. Downloaded on February 05,2024 at 17:06:14 UTC from IEEE Xplore.  Restrictions apply. ",
        "metadata": {
            "source": "pdf/test.pdf",
            "page": 1
        }
    },
    {
        "page_content": "TABLE I\nTHE SPECIFICATIONS OF POPULAR AR/VR HMD S\nName AR/VR Price ($) Weight (gr) Type SoftwarePosition\ntracking\nsystemSensors Other features\nOculus Rift VR 399 470 Tethered Oculus YesAccelerometer,\ngyroscope,\nmagnetometer6DoF dual\ncontrollers,\nhand trackers,\nbuilt-in headphones\nPlayStation VR VR 300 610 Tethered Console YesAccelerometer,\ngyroscope6DoF dual\ncontrollers\nHTC vive VR 650 555 Tethered SteamVR YesAccelerometer,\nstructured light,\ngyroscope6DoF dual\ncontrollers,\ncamera\nSamsung\nGear VRVR 93 318 Mobile Android NoAccelerometer,\nproximity sensor,\ngyroscopeHandheld remote,\ntouchpad\nGoogle Cardboard VR 5 and up 96 Mobile Android and iOS No - Easy to use\nMicrosoft\nHoloLensAR \u223c3000 579 TetheredWindows\nmixed realityIMU, cameras\nlight ambient sensor\nmicrophone arrayClicker\nVuzix Blade AR \u223c1000 85 Tethered AndroidHead motion\ntracking sensors,\ncamerasRemote control\napp for\nAndroid & iOS\ndevice,\nvoice control,\ntouch pad\nEpson Moverio\nBT-300AR 700 69 Tethered AndroidGPS,\ngeomagnetic,\naccelerometer,\ngyroscopic\nillumination sensorRemote control\nIn contrast to HMD, the displays can be placed far from\nparticipants\u2019 eyes. For instance, CA VE is an immersive virtual\nreality environment and takes place inside a room whose walls,\nceil and \ufb02oor may have projection screens. Users typically\ninteract through input devices such as joysticks or gloves with\nvirtual objects on the screens. CA VE can be a suitable choice\nfor education because a number of students can experience VR\nonce. It can decrease the cost of a school (not individual) in\nrespect to HMDs and can help a teacher to conduct all students\nduring experiments but users cannot move around freely.\nThe CA VE or HMDs are still inaccessible and expensive\nfor teachers and students. Desktop VR is a low-cost visual\nperception that can be easily applied by users; the users just\nneed to watch a desktop computer monitor and interact with\nVR objects by a controlling device (i.e., a computer mouse)\n[12, 13].\nVR cabins are usually simulators that look like CA VE but\nthey can provide motions in the three axes. These cabins are\nbasically employed in education and job training, for instance,\ncockpit simulator for pilot training [14], vehicle simulator for\ndriver training [15] and so on.\nThe haptic technology allows to produce more attractive\nVR content; haptic devices can reconstruct and simulate the\neffects of real force feedback phenomena in VR. For instance,\nTeslasuit is wearable in which haptic feedback system is\nembedded; it covers the entire body and transfer hits and\nstrikes to body [16]. Gloves and other small wearable devices\nwere presented to make touch feeling and transfers the force tohands (i.e., HaptX2, unlimitedhand3). Handling users\u2019 motions\nin minimal physical space causes to develop a walking system\nfor virtual worlds (i.e., KATVR4, virtuix5, vrgochair6).\nB. AR/VR software\nDuring the last few decades, several AR/VR applications\nhave been produced to be used in several \ufb01elds such as adver-\ntisement, tourism, maintenance, and training. In the education\n\ufb01elds, applications have been developed for adoption AR/VR\ninto learning processes and attractive ones were presented\nin mathematics, physics, astronomy, biology and other scien-\nti\ufb01c subjects. In physics, virtual electricity and Newton labs\nwere presented by Zspace to learn electricity concepts and\ntroubleshooting circuits. Newtons Park was designed to teach\nNewtonian Mechanics by building simulations and interacting\nwith data. Moreover, zSpace applications support teaching a\nwide range of learning objectives from learning Newton laws\nto learn anatomy [17]. The Hololens was used to show heat\nconduction of metals for an introductory laboratory course in\nthermodynamics [18]. Chemistry lab may include dangerous\nexperiments which may happen when a student combine\nwrong elements. VR helps students to learn chemistry without\nusing real materials and give more understanding of what are\n2www.haptx.com\n3www.unlimitedhand.com\n4www.katvr.com\n5www.virtuix.com\n6www.vrgochair.com\n484\nAuthorized licensed use limited to: Chalmers University of Technology Sweden. Downloaded on February 05,2024 at 17:06:14 UTC from IEEE Xplore.  Restrictions apply. ",
        "metadata": {
            "source": "pdf/test.pdf",
            "page": 2
        }
    },
    {
        "page_content": "atoms and molecules [19] (i.e., MEL7, Labster8). Real chem-\nical components were also replaced with virtual ones with the\nhelp of AR technology and more tangible interaction in real\nenvironment was provided for learner [20, 21]. In astronomy,\nricher AR/VR content were introduced than other \ufb01elds. The\nstudies have shown that students misunderstand some concepts\nin astronomy [22] and some of these misconceptions have\nremained for several years [23]. AR/VR astronomy products\naim to change conventional teaching methods and content, then\nhelp students to clarify their misconceptions. DVREMS was\ndesigned to teach earth motion in elementary school students\n[12]. Mintz et al. [24] presented a dynamic 3D model of the\nsolar system and learners travel through it. In the literature,\nsimilar VR content were presented to enhance astronomical\nknowledge in students [25, 26, 27, 28]. In addition, the\ncommercial applications were presented, for instance, Apollo\n119, Astronomy VR10, and Star Chart11. The AR technology\nhelps students to understand some astronomy concepts as\nwell, for instance, relationships between planets [29]. Fleck\nand Simon [30] compared AR and physical astronomical\nmodels for learner. They highlighted that AR learning methods\nimproved signi\ufb01cantly astronomical learning than physical\nlearning method. Reed et al. [31] designed a new sandbox\nthat mixed with AR to explain gravity. AR/VR tours and\njourneys were attractive parts of these technologies that en-\nhanced historical and geographical knowledge of students. For\ninstance, students just need to turn on their VR HMD and then\ntravel to ancient Iran, Rome, or Egypt (i.e., unimersiv12) [32].\nIn addition, the learners can virtually travel to oceans, visit\nother countries to familiar with other cultures and so on (i.e.,\nGoogle earth VR13, destinations14). The AR technology was\nalso used in museums and heritage places to help visitors and\nlearners with additional information simultaneously on objects\nor visiting places [33, 34]. In biology and anatomy subjects,\nVR was used to train surgeons and to improve their level of\ncompetence before they operate on real patients [35]. Travel\ninside the human body (i.e., body VR15) and representing\n3D models of organs [36] can reshape learning processes.\nAR in Human Anatomy Atlas 2018 Edition was a perfect 3D\nanatomy reference for learning in the healthcare system [37].\nAR can provide additional information on books. The\nexperiments, videos, and other materials can be shown since\na device is placed in front of pages of books, posters,\n\ufb02ashcards (i.e., PAMPAM \ufb02ashcards16). Google Expeditions\nis an immersive education application that was designed for\neducation. Learners can get new experiences without leaving\nthe classroom; for instance, they can swim with sharks,\n7www.melscience.com/vr\n8www.labster.com\n9www.immersivevreducation.com/apollo-11-vr/\n10www.play.google.com\n11www.play.google.com\n12www.unimersiv.com\n13www.oculus.com\n14www.steamcommunity.com\n15www.oculus.com\n16www.cafebazaar.ir/app/com.majazkadeh.animals/?l=enturn the classroom into a museum, visit outer space, and\nmore [38]. The Google also provides a package to support\nAR/VR. The Google Cardboard as a cheap HMD, Google\nJump to produce 3D-360 video with cameras, Google VR to\nvisit places, and provide a platform for developers to build\nVR content. The researchers and companies are developing\nplatforms for AR/VR developers to facilitate production of\nAR and VR content. Google VR provides SDKs (software\ndevelopment kit) allowing to build new VR content. These\nSDKs are available for the Android, iOS, Unity, Unreal that\nallow developers to produce VR content for several platforms\n[11]. The SDKs (i.e., google VR, GOpenVR, and SteamVR)\nmay be linked with the game engines to create 3D elements\nand environments for AR/VR. ARCore17is a platform for\nproducing AR content. It detects images and track the position\nof the camera relative to the world. Then it builds models\non the images [11]. ARToolKit, Vuforia, Wikitude, EasyAR,\nand DeepAR are similar SDKs that are employed for AR\nproduction.\nIV. O PPORTUNITIES AND CHALLENGES\nResearch has revealed that AR/VR technologies are highly\nbene\ufb01cial to education and could help students to develop their\nskills and knowledge in a more effective way [39, 40, 41]\nAR/VR systems could present educational content in attractive\nways and enhance students motivation and interest. Not only\nstudents enjoy from AR/VR learning but also they follow\nlearning processes and then AR/VR systems help them to\nachieve more accurate knowledge [13, 42]. AR/VR systems\nprovide a better understanding of educational dif\ufb01culties that\nhave been addressed in the educational literature [43]. For\ninstance, some students cannot perceive 3D models or some\nstudents cannot imagine invisible phenomena such as the spin-\nning of the earth [44, 45]. AR/VR allows learners to see 3D\nmodels, to manipulate objects virtually, to \ufb01gure out unobserv-\nable phenomena, to experience abstract concepts (e.g., travel\nin wormholes). These virtual experiences can deeply promote\nstudents thinking [46] and correct their misconceptions [42].\nRecent progress in AR/VR hardware allows the interaction\nand integration between several senses and an around envi-\nronment with the help of multi-sensory devices. The sensory\nintegration provides learners to construct meaning from ex-\nperiences [13]. The importance of collaborative learning in\ndistant has been already stated in the literature [47]. The VR\ntechnology potentially provides instant communication and\nstudents can attend in a virtual classroom at the same time.\nThey can discuss, receive immediate comments from others\nand feel a sense of being in the same places as their classmates\n[48].\nAR/VR products help students to overcome learning bar-\nriers and improve students\u2019 knowledge, skills, thinking, and\nunderstanding. However, there are challenges and drawbacks\nto apply AR/VR as an educational tool in the most classrooms\nin the world. The \ufb01rst problem is the cost of implementing\n17www.developers.google.com/ar/\n485\nAuthorized licensed use limited to: Chalmers University of Technology Sweden. Downloaded on February 05,2024 at 17:06:14 UTC from IEEE Xplore.  Restrictions apply. ",
        "metadata": {
            "source": "pdf/test.pdf",
            "page": 3
        }
    },
    {
        "page_content": "AR/VR systems. As we have seen, HMDs are relatively\nexpensive. In other cases, computers and display systems for\ndemonstrating the VR/AR products are also needed that may\nmake a challenge for many schools. The second challenge is a\nlack of realism in VR or AR simulations. Quality of graphics\ndisplay impresses a feeling of a user, it can improve visual\nperception and provide a richer experience [49, 50]. The third\nchallenge is health issues and physical effects on students.\nHMDs are relatively heavy that may cause wearers feel fatigue\nafter a long period of time. Lenses in an HMD may obstruct\nthe view, although they are close to the eye in modern versions.\nAnother side effect that is not limited to HMDs is simulator\nsicknesses and it appears mostly on VR experiences. The\nsimulator sickness usually results from mismatching between\nthe visual perception and sense of movement. Symptoms of\nsimulator sickness may include nausea, disorientation, and\ndiscomfort [13]. Fourth challenge is inherited from hardware\nlimitations. Although recent hardware developments have im-\nproved AR and VR demonstrations, the limitations may avoid\nhaving a great level of user experiences. Lack of precision,\nGPS errors and navigation problems, latency between sensors\u2019\ndata and the effects to the visual system are common problems\nthat can cause students\u2019 frustration and interruption to their\nexperiences.\nV. C ONCLUSION\nIn this paper, we have attempted to show the capabilities\nand limitations of AR and VR technologies for education.\nTechnologies progress fast and impress our lifestyles. Re-\nsearchers are interested in addressing the issues and drawbacks\nof education systems with the help of potentials of emerging\ntechnologies and the latest scienti\ufb01c achievements. AR/VR\nsystems provide opportunities to increase students\u2019 motiva-\ntions and engage them more in learning processes. In addition,\nAR/VR allows the students to have virtual experiences where\ntraditional learning methods are impossible or expensive.\nHowever, there are still some challenges for AR/VR to be\nused by most learners in the world. These technologies are in\ntheir infancy but they have a great potential to overcome the\nbarriers.\nREFERENCES\n[1]D. Teferra and P. G. Altbachl, \u201cAfrican higher education:\nChallenges for the 21st century,\u201d Higher Education ,\nvol. 47, no. 1, pp. 21\u201350, Jan 2004.\n[2]C. Luna Scott, \u201cThe futures of learning 3: What kind of\npedagogies for the 21st century?\u201d 2015.\n[3]C. B. Frey and M. A. Osborne, \u201cThe future of em-\nployment: How susceptible are jobs to computerisation?\u201d\nTechnological Forecasting and Social Change , vol. 114,\npp. 254 \u2013 280, 2017.\n[4]R. C. Richey, K. Silber, and D. Ely, \u201cRe\ufb02ections on the\n2008 aect de\ufb01nitions of the \ufb01eld,\u201d TechTrends , vol. 52,\nno. 1, pp. 24\u201325, 2008.\n[5]E. Dale, Audiovisual methods in teaching . New York\nDryden Press: Dryden Press, 1969.[6]C. E. Baukal, F. B. Ausburn, and L. J. Ausburn, \u201cA\nproposed multimedia cone of abstraction: Updating a\nclassic instructional design theory.\u201d Journal of Educa-\ntional Technology , vol. 9, no. 4, pp. 15\u201324, 2013.\n[7]S. J. Lee and T. C. Reeves, \u201cA signi\ufb01cant contributor to\nthe \ufb01eld of educational technology,\u201d Educational Tech-\nnology , vol. 47, no. 6, pp. 56\u201359, 2007.\n[8]P. Milgram, H. Takemura, A. Utsumi, and F. Kishino,\n\u201cAugmented reality: A class of displays on the reality-\nvirtuality continuum,\u201d in Telemanipulator and telepres-\nence technologies , vol. 2351. International Society for\nOptics and Photonics, 1995, pp. 282\u2013293.\n[9]R. A. Bartle, Designing virtual worlds . New Riders,\n2004.\n[10]M. A. Schnabel, X. Wang, H. Seichter, and T. Kvan,\n\u201cFrom virtuality to reality and back,\u201d Proceedings of\nthe International Association of Societies of Design Re-\nsearch , vol. 1, p. 15, 2007.\n[11]Google, \u201cGoogle cardboard,\u201d 2014. [Online]. Available:\nhttps://vr.google.com/\n[12]C. H. Chen, J. C. Yang, S. Shen, and M. C. Jeng, \u201cA\ndesktop virtual reality earth motion system in astronomy\neducation,\u201d Journal of Educational Technology & Soci-\nety, vol. 10, no. 3, 2007.\n[13]C. Christou, \u201cVirtual reality in education,\u201d in Affective,\ninteractive and cognitive methods for e-learning design:\ncreating an optimal education experience . IGI Global,\n2010, pp. 228\u2013243.\n[14]K.-U. Doer, J. Schiefel, and W. Kubbat, \u201cVirtual cock-\npit simulation for pilot training,\u201d DARMSTADT UNIV\n(GERMANY) INSTITUTE FORFLIGHT MECHANICS\nAND CONTROL, Tech. Rep., 2001.\n[15]W.-S. Lee, J.-H. Kim, and J.-H. Cho, \u201cA driving simula-\ntor as a virtual reality tool,\u201d in Robotics and Automation,\n1998. Proceedings. 1998 IEEE International Conference\non, vol. 1. IEEE, 1998, pp. 71\u201376.\n[16]\u201cTeslasuit, ultimate tech in smart clothing,\u201d 2015.\n[Online]. Available: https://teslasuit.io/\n[17]\u201czspace applications.\u201d [Online]. Available:\nhttps://zspace.com/apps/\n[18]M. Strzys, S. Kapp, M. Thees, P. Klein, P. Lukowicz,\nP. Knierim, A. Schmidt, and J. Kuhn, \u201cPhysics holo. lab\nlearning experience: using smartglasses for augmented\nreality labwork to foster the concepts of heat conduction,\u201d\nEuropean Journal of Physics , vol. 39, no. 3, p. 035703,\n2018.\n[19]M. Limniou, D. Roberts, and N. Papadopoulos, \u201cFull\nimmersive virtual environment cavetm in chemistry ed-\nucation,\u201d Computers & Education , vol. 51, no. 2, pp.\n584\u2013593, 2008.\n[20]S. Singhal, S. Bagga, P. Goyal, and V . Saxena, \u201cAug-\nmented chemistry: Interactive education system,\u201d Inter-\nnational Journal of Computer Applications , vol. 49,\nno. 15, 2012.\n[21]L. Charton, N. Cuotto, and M. Jakobsson, \u201cLmn-lab: Ar\nelements for chemistry laboratory practices,\u201d in Proceed-\n486\nAuthorized licensed use limited to: Chalmers University of Technology Sweden. Downloaded on February 05,2024 at 17:06:14 UTC from IEEE Xplore.  Restrictions apply. ",
        "metadata": {
            "source": "pdf/test.pdf",
            "page": 4
        }
    },
    {
        "page_content": "ings of SIDeR 2016 . IEEE, 2016.\n[22]J. Dunlop, \u201cHow children observe the universe,\u201d Publi-\ncations of the Astronomical Society of Australia , vol. 17,\nno. 2, pp. 194\u2013206, 2000.\n[23]J. Parker and D. Heywood, \u201cThe earth and beyond:\nDeveloping primary teachers\u2019 understanding of basic\nastronomical events,\u201d International Journal of Science\nEducation , vol. 20, no. 5, pp. 503\u2013520, 1998.\n[24]R. Mintz, S. Litvak, and Y . Yair, \u201c3d-virtual reality in\nscience education: An implication for astronomy teach-\ning,\u201d Journal of Computers in Mathematics and Science\nTeaching , vol. 20, no. 3, pp. 293\u2013305, 2001.\n[25]W. Tarng and H. Liou, \u201cThe application of virtual reality\nin astronomy education,\u201d Adv. Technol. Learn. , vol. 4,\nno. 3, pp. 160\u2013169, 2007.\n[26]A. D. Weigel and C. D. Moraitis, \u201cVirtual reality as-\ntronomy education using aas worldwide telescope and\noculus rift,\u201d in American Astronomical Society Meeting\nAbstracts , vol. 229, 2017.\n[27]S. Klimenko, \u201cVirtual planetarium: Learning astronomy\nin virtual reality,\u201d in Proceedings of EdMedia Innovate\nLearning 2004 , L. Cantoni and C. McLoughlin, Eds.\nLugano, Switzerland: Association for the Advancement\nof Computing in Education (AACE), 2004, pp. 2154\u2013\n2157.\n[28]J. Detlefsen, \u201cThe cosmic perspective: Teaching middle-\nschool children astronomy using ego-centric virtual real-\nity,\u201d Master\u2019s thesis, Master Thesis, Aalborg University,\n2014.\n[29]B. E. Shelton and N. R. Hedley, \u201cUsing augmented real-\nity for teaching earth-sun relationships to undergraduate\ngeography students,\u201d in Augmented Reality Toolkit, The\nFirst IEEE International Workshop , vol. 8. IEEE, 2002.\n[30]S. Fleck and G. Simon, \u201cAn augmented reality environ-\nment for astronomy learning in elementary grades: an\nexploratory study,\u201d in Proceedings of the 25th Conference\non l\u2019Interaction Homme-Machine . ACM, 2013, p. 14.\n[31]S. Reed, S. Hsi, O. Kreylos, M. Yikilmaz, L. Kellogg,\nS. Schladow, H. Segale, and L. Chan, \u201cAugmented reality\nturns a sandbox into a geoscience lesson,\u201d Eos, vol. 97,\n2016.\n[32]D. A. Guttentag, \u201cVirtual reality: Applications and im-\nplications for tourism,\u201d Tourism Management , vol. 31,\nno. 5, pp. 637\u2013651, 2010.\n[33]V . Vlahakis, M. Ioannidis, J. Karigiannis, M. Tsotros,\nM. Gounaris, D. Stricker, T. Gleue, P. Daehne, and\nL. Almeida, \u201cArcheoguide: an augmented reality guide\nfor archaeological sites,\u201d IEEE Computer Graphics and\nApplications , vol. 22, no. 5, pp. 52\u201360, 2002.\n[34]O. Choudary, V . Charvillat, R. Grigoras, and P. Gurdjos,\n\u201cMarch: mobile augmented reality for cultural heritage,\u201d\ninProceedings of the 17th ACM international conference\non Multimedia . ACM, 2009, pp. 1023\u20131024.\n[35]D. Ota, B. Loftin, T. Saito, R. Lea, and J. Keller, \u201cVirtual\nreality in surgical education,\u201d Computers in biology and\nmedicine , vol. 25, no. 2, pp. 127\u2013137, 1995.[36]D. T. Nicholson, C. Chalk, W. R. J. Funnell, and S. J.\nDaniel, \u201cCan virtual reality improve anatomy education?\na randomised controlled study of a computer-generated\nthree-dimensional anatomical ear model,\u201d Medical edu-\ncation , vol. 40, no. 11, pp. 1081\u20131087, 2006.\n[37]\u201cHuman anatomy atlas offers augmented reality,\u201d 2018.\n[Online]. Available: https://www.visiblebody.com\n[38]\u201cGoogle expeditions,\u201d 2015. [Online]. Available:\nhttps://support.google.com/edu/expeditions/answer/6335093?hl=en\n[39]N. Sayed, H. H. Zayed, and M. I. Sharawy, \u201cArsc:\nAugmented reality student card an augmented reality\nsolution for the education \ufb01eld,\u201d Computers & Education ,\nvol. 56, no. 4, pp. 1045\u20131061, 2011.\n[40]J. K. Crosier, S. Cobb, and J. R. Wilson, \u201cKey lessons\nfor the design and integration of virtual environments in\nsecondary science,\u201d Computers & Education , vol. 38, no.\n1-3, pp. 77\u201394, 2002.\n[41]H. Kaufmann, D. Schmalstieg, and M. Wagner, \u201cCon-\nstruct3d: a virtual reality application for mathematics\nand geometry education,\u201d Education and information\ntechnologies , vol. 5, no. 4, pp. 263\u2013276, 2000.\n[42]S. Sotiriou and F. X. Bogner, \u201cVisualizing the invisible:\naugmented reality as an innovative science education\nscheme,\u201d Advanced Science Letters , vol. 1, no. 1, pp.\n114\u2013122, 2008.\n[43]R. Arends and S. Castle, Learning to teach . McGraw-\nHill New York, 1991, vol. 2.\n[44]H.-K. Wu, J. S. Krajcik, and E. Soloway, \u201cPromoting\nunderstanding of chemical representations: Students\u2019 use\nof a visualization tool in the classroom,\u201d Journal of\nResearch in Science Teaching: The Of\ufb01cial Journal of the\nNational Association for Research in Science Teaching ,\nvol. 38, no. 7, pp. 821\u2013842, 2001.\n[45]L. Kerawalla, R. Luckin, S. Selje\ufb02ot, and A. Woolard,\n\u201cmaking it real: exploring the potential of augmented\nreality for teaching primary school science,\u201d Virtual\nreality , vol. 10, no. 3-4, pp. 163\u2013174, 2006.\n[46]T.-Y . Liu, T.-H. Tan, and Y .-L. Chu, \u201cOutdoor natural\nscience learning with an r\ufb01d-supported immersive ubiq-\nuitous learning environment.\u201d Journal of Educational\nTechnology & Society , vol. 12, no. 4, 2009.\n[47]V . Thurmond and K. Wambach, \u201cUnderstanding interac-\ntions in distance education: A review of the literature,\u201d\nInternational journal of instructional technology and\ndistance learning , vol. 1, no. 1, 2004.\n[48]T. Monahan, G. McArdle, and M. Bertolotto, \u201cVirtual\nreality for collaborative e-learning,\u201d Computers & Edu-\ncation , vol. 50, no. 4, pp. 1339\u20131353, 2008.\n[49]H. T. Chong, C. K. Lim, and K. L. Tan, \u201cChallenges\nin virtual reality system: A review,\u201d in AIP Conference\nProceedings , vol. 2016, no. 1. AIP Publishing, 2018.\n[50]H.-K. Wu, S. W.-Y . Lee, H.-Y . Chang, and J.-C. Liang,\n\u201cCurrent status, opportunities and challenges of aug-\nmented reality in education,\u201d Computers & education ,\nvol. 62, pp. 41\u201349, 2013.\n487\nAuthorized licensed use limited to: Chalmers University of Technology Sweden. Downloaded on February 05,2024 at 17:06:14 UTC from IEEE Xplore.  Restrictions apply. ",
        "metadata": {
            "source": "pdf/test.pdf",
            "page": 5
        }
    },
    {
        "page_content": "Sj\u00e4lvk\u00f6rande\nbilar\noch\nbilar\nmed\nassistanssystem\nur\nett\nh\u00e5llbarhetsperspektiv\nProjektarbete\ni\nkursen\nFFR102\nH\u00e5llbar\nutveckling\n-\nkritiska\nperspektiv\noch\nm\u00f6jliga\nl\u00f6sningar\nGamal\nAllan\nForsell\nOskar\nLong\nRicky\nTenggren\nPer\nChalmers\ntekniska\nh\u00f6gskola\n2023-05-29/slutversion\n",
        "metadata": {
            "source": "pdf/22.pdf",
            "page": 0
        }
    },
    {
        "page_content": "Sammanfattning\nRapporten\nbehandlar\nutvecklingen\nav\nsj\u00e4lvk\u00f6rande\nbilar\noch\nbilar\nmed\nf\u00f6rarassistans\nur\nett\nh\u00e5llbarhetsperspektiv.\nI\narbetet\nhar\ndet\nframkommit\natt\ndet\nfr\u00e4mst\n\u00e4r\nsocial\noch\nekonomisk\nh\u00e5llbarhet\nsom\nkommer\ni\nfokus\nmed\nutveckling\nav\nbilar\nmed\navancerade\nassistanssystem\nf\u00f6r\nf\u00f6rare.\nSystem\nmed\nhelt\nsj\u00e4lvk\u00f6rande\nbilar\nskulle\nkunna\nge\nytterligare\nf\u00f6rdelar\nsom\nexempelvis\nb\u00e4ttre\nresursutnyttjande\nav\nbefintliga\nbilar\nvilket\nkan\nbidra\ntill\nekologisk\nh\u00e5llbarhet.\nMen\nv\u00e4gen\ndit\n\u00e4r\nl\u00e5ng.\nVi\ntar\n\u00e4ven\nupp\nkvarst\u00e5ende\nfr\u00e5gor\nsom\nexempelvis\nansvarsfr\u00e5gor\nvid\nolyckor\nsamt\nen\ndiskussion\nom\ndatas\u00e4kerhet\ni\nf\u00f6rarsystem.\nFr\u00e5gorna\nbelyses\nur\ns\u00e5v\u00e4l\nf\u00f6retags-\nsom\ningenj\u00f6rsperspektiv.\nSlutsatsen\n\u00e4r\natt\nutvecklingen\nmot\nsj\u00e4lvk\u00f6rande\nbilar\nefter\nen\nteknikutv\u00e4rdering\n\u00e4r\netisk\neftersom\nde\npositiva\neffekterna\n\u00f6verv\u00e4ger\nde\nnegativa.\n2\n(\n13\n)",
        "metadata": {
            "source": "pdf/22.pdf",
            "page": 1
        }
    },
    {
        "page_content": "Inneh\u00e5llsf\u00f6rteckning\nSammanfattning\n2\nInneh\u00e5llsf\u00f6rteckning\n3\n1.\nInledning\n4\n2.\nSyfte\n4\n3.\nAvgr\u00e4nsningar\n4\n4.\nMetod\n5\n5.\nTeoretiskt\nunderlag\n5\n5.1\nTeknikv\u00e4rdering\n5\n5.2\nH\u00e5llbarhetsaspekter\n5\n5.2.1\nSocial\nh\u00e5llbarhet\n5\n5.2.2\nEkonomisk\nh\u00e5llbarhet\n6\n5.2.3\nEkologisk\nh\u00e5llbarhet\n6\n5.3\nEtik\n7\n6.\nResultat\n7\n6.1\nTeknikv\u00e4rdering\n7\n6.2\nH\u00e5ller\ntekniken\nvad\nden\nlovar\nur\nett\nh\u00e5llbarhetsperspektiv?\n8\n6.3\nEtiska\nfr\u00e5gest\u00e4llningar\n9\n7.\nDiskussion\noch\nslutsats\n10\nK\u00e4llv\u00e4rdering\n11\nK\u00e4llh\u00e4nvisning\n12\n3\n(\n13\n)",
        "metadata": {
            "source": "pdf/22.pdf",
            "page": 2
        }
    },
    {
        "page_content": "1.\nInledning\nMjukvara\nspelar\nen\nviktig\nroll\nf\u00f6r\navancerade\nf\u00f6rarassistanssystem\noch\ni\nsj\u00e4lvk\u00f6rande\nbilar.\nDet\n\u00e4r\nden\navg\u00f6rande\nfaktorn\nsom\ns\u00e4rskiljer\nolika\nfordon\nn\u00e4r\ndet\ng\u00e4ller\nprestanda\noch\nsj\u00e4lvk\u00f6rningsupplevelse.\nMjukvarans\nkvalitet\noch\ninteraktion\nmed\nf\u00f6raren\n\u00f6kar\nsannolikheten\natt\nman\nn\u00e5r\ndestination\ns\u00e4kert\noch\nsnabbt.\nDet\nG\u00f6teborgsbaserade\nf\u00f6retaget\nZenseact\nligger\ni\nframkant\nav\ninnovation\noch\nutveckling\nav\nmjukvara\nf\u00f6r\nautonom\nk\u00f6rning\noch\navancerade\nf\u00f6rarassistanssystem.\nMan\ngrundades\nav\nVolvo\nCars\nf\u00f6r\natt\nutveckla\ndetta\nsegment\nav\nbilbranschen.\nI\nf\u00f6retaget\narbetar\nman\nmed\natt\nhitta\netiska\noch\nh\u00e5llbara\nl\u00f6sningar\ntill\nvardagliga\nproblem\ng\u00e4llande\nsj\u00e4lvk\u00f6rande\nbilar\noch\navancerade\nf\u00f6rarst\u00f6dsystem.\nI\nden\nh\u00e4r\nrapporten\nkommer\nvi\nforts\u00e4ttningsvis\natt\ns\u00e4rskilja\nhelt\nsj\u00e4lvk\u00f6rande\nbilar\n(AD\n-\nAutonomous\nDriving)\nfr\u00e5n\nbilar\nmed\nassistanssystem\n(ADAS\n-\nAdvanced\nDriver-Assistance\nSystem).\nMed\nhelt\nsj\u00e4lvk\u00f6rande\nbilar\nmenas\nbilar\nd\u00e4r\nmjukvaran\nstyr\nbilen\nhelt\noch\nh\u00e5llet\nutan\nf\u00f6rarens\ninteraktion.\nAssistanssystem\ninneb\u00e4r\natt\nf\u00f6raren\nhar\ndet\nyttersta\nansvaret\nf\u00f6r\nbilen,\nmen\natt\ndet\ni\nbilen\nfinns\nsystem\nsom\nhj\u00e4lper\ntill\ngenom\natt\nidentifiera\nm\u00f6jliga\nhot,\nstyra\nundan\neller\nbromsa\nom\nn\u00e5gon\nspringer\nut\nframf\u00f6r\nbilen\neller\nstyra\nupp\nbilen\nifall\nden\n\u00e4r\np\u00e5\nv\u00e4g\nner\ni\nett\ndike.\n2.\nSyfte\nSyftet\nmed\ndenna\nrapport\n\u00e4r\natt\nbeskriva\nZenseact\noch\ndess\nverksamhet\nur\nett\nh\u00e5llbarhetsperspektiv\noch\nett\ningenj\u00f6rsetiskt\nperspektiv.\nMed\ndetta\nmenas\natt\nvi\nst\u00e4ller\nfr\u00e5gan\nom\ntekniken\nmed\nsj\u00e4lvk\u00f6rande\nbilar\nh\u00e5ller\nvad\nden\nlovar\nur\nett\nh\u00e5llbarhetsperspektiv\noch\nom\nutvecklingen\nkan\nanses\nsom\netisk.\nDessutom\nunders\u00f6ks\nhur\ningenj\u00f6rer\ni\nf\u00f6retaget\nf\u00f6rh\u00e5ller\nsig\ntill\nfr\u00e5gor\nom\nf\u00f6retagets\nverksamhet\nutifr\u00e5n\netik\noch\nh\u00e5llbarhetsperspektiv.\n3.\nAvgr\u00e4nsningar\nVi\nkommer\natt\navgr\u00e4nsa\nrapporten\ntill\natt\ndiskutera\nf\u00f6retagets\nroll,\noch\ningenj\u00f6rens\nansvar\nfr\u00e4mst\nen\netisk\nteknikutv\u00e4rdering\noch\nsedan\nbed\u00f6ma\nhur\nde\nh\u00e5llbarhetsm\u00e4ssiga\nf\u00f6rdelarna\nsom\ntekniken\nmed\nsj\u00e4lvk\u00f6rande\nbilar\nutlovar\ninfrias.\nF\u00f6retaget\nsom\nvi\nanv\u00e4nder\ni\nintervjun\ntillhandah\u00e5ller\nmjukvara\nf\u00f6r\nsj\u00e4lvk\u00f6rande\nbilar\noch\navancerade\nf\u00f6rarsystem,\nvi\nkommer\nd\u00e4rf\u00f6r\ninte\natt\ndiskutera\nhuruvida\nproduktionen\nav\nbilar\ni\nsig\n\u00e4r\nh\u00e5llbara.\nI\nden\nteknikv\u00e4rdering\nvi\ng\u00f6r\nser\nvi\nhuvudsakligen\np\u00e5\nkonsekvenser\nf\u00f6r\nf\u00f6rare\noch\nsamh\u00e4llet\ni\nstort.\n4\n(\n13\n)",
        "metadata": {
            "source": "pdf/22.pdf",
            "page": 3
        }
    },
    {
        "page_content": "4.\nMetod\nGruppen\nstartar\narbetet\nmed\natt\ng\u00f6ra\nlitteraturs\u00f6kningar\nf\u00f6r\natt\nskaffa\nen\nfaktam\u00e4ssig\nbakgrund\ntill\nprojektrapporten\noch\nsom\nunderlag\nf\u00f6r\nde\nfr\u00e5gor\nvi\nst\u00e4ller.\nD\u00e4refter\ngenomf\u00f6rs\nen\nintervju\nmed\nMattias\nHenriksson\nvilken\njobbar\nsom\ningenj\u00f6r\nmed\nansvar\nf\u00f6r\nverifiering\nav\nsystem\np\u00e5\nZenseact.\nSyftet\n\u00e4r\natt\nf\u00e5\nen\nb\u00e4ttre\nf\u00f6rst\u00e5else\nf\u00f6r\nhur\nett\nf\u00f6retag\nagerar\nkring\nde\nfr\u00e5gest\u00e4llningar\nkring\netik\noch\nh\u00e5llbarhet\nvi\nidentifierat\noch\nvad\ndet\nf\u00e5r\nf\u00f6r\nkonsekvenser\nf\u00f6r\nen\ningenj\u00f6r.\n5.\nTeoretiskt\nunderlag\n5.1\nTeknikv\u00e4rdering\nAtt\nutv\u00e4rdera\nny\nteknik\nutifr\u00e5n\neventuella\nsociala\nkonsekvenser\n\u00e4r\nganska\novanligt\nmen\nn\u00f6dv\u00e4ndigt\nf\u00f6r\natt\nkunna\nutv\u00e4rdera\nny\nteknik\nur\nett\netiskt\nperspektiv.\nEtt\nproblem\n\u00e4r\natt\ndet\n\u00e4r\nsv\u00e5rt\natt\nse\nhur\nden\nnya\ntekniken\nkommer\natt\np\u00e5verka\nsamh\u00e4llet\noch\nindivider\np\u00e5\nsikt.\nAv\ndetta\nsk\u00e4l\nb\u00f6r\nutv\u00e4rdering\ng\u00f6ras\netappvis\noch\nmed\nkorta\ntidshorisonter.\nOm\nman\nanl\u00e4gger\nett\nkonsekvensetiskt\nperspektiv\nuppst\u00e5r\nproblemet\natt\ndet\ninte\np\u00e5\nf\u00f6rhand\ng\u00e5r\natt\nf\u00f6rutse\ndet\nexakta\nutfallet.\nF\u00f6r\natt\nhantera\neventuella\nrisker\nb\u00f6r\nman\nundvika\nde\nrisker\nsom\nkan\nge\nst\u00f6rst\nskada\noavsett\nhur\nsannolika\ndessa\n\u00e4r\n[6].\nDe\nmetoder\nsom\nutvecklats\nf\u00f6r\nallm\u00e4n\nteknikutveckling\nkan\nmed\nlite\nf\u00f6r\u00e4ndringar\nanv\u00e4ndas.\nDet\nvill\ns\u00e4ga\nidentifiera\nvilka\ngrupper\nav\nm\u00e4nniskor\nsom\np\u00e5verkas\noch\nvarf\u00f6r\nde\np\u00e5verkas,\nvilka\nf\u00f6rh\u00e5llanden\nsom\nspelar\nroll\nf\u00f6r\nen\netisk\nbed\u00f6mning\noch\nj\u00e4mf\u00f6r\nmed\nliknande\nfaktiska\ntekniker.\nD\u00e4refter\nkan\nman\nunders\u00f6ka\nden\nnya\ntekniken\nmed\nde\nolika\nmoralteorier\nsom\nbeskrivs\nnedan\noch\navslutningsvis\nf\u00f6rs\u00f6ka\nskapa\nalternativa\ns\u00e4tt\natt\nl\u00f6sa\nde\nuppgifter\nsom\nden\nnya\ntekniken\n\u00e4r\navsedd\natt\nhantera\n[6].\n5.2\nH\u00e5llbarhetsaspekter\nSj\u00e4lvk\u00f6rande\nbilar\nhar\npotential\natt\np\u00e5verka\nh\u00e5llbarhet\np\u00e5\nflera\ns\u00e4tt,\nb\u00e5de\npositivt\noch\nnegativt.\nDet\n\u00e4r\nviktigt\natt\nanalysera\ndessa\naspekter\nutifr\u00e5n\nde\ntre\nh\u00e5llbarhetsperspektiven\n-\nsocialt,\nekonomiskt\noch\nekologiskt\n-\nf\u00f6r\natt\nf\u00f6rst\u00e5\nde\nbredare\nkonsekvenserna\nav\natt\ninf\u00f6ra\ndessa\nbilar\ni\nsamh\u00e4llet.\n5.2.1\nSocial\nh\u00e5llbarhet\nSocial\nh\u00e5llbarhet\ninnefattar\naspekter\nsom\nr\u00f6r\nm\u00e4nniskors\nlevnadsstandard,\ns\u00e4kerhet,\nj\u00e4mlikhet\noch\nsocial\nsammanh\u00e5llning\nf\u00f6r\nalla\nmedlemmar\ni\nsamh\u00e4llet\n[1].\nGenom\natt\neliminera\nden\nm\u00e4nskliga\nfaktorn,\nsom\nuppskattas\nvara\norsaken\ntill\n\u00f6ver\n90\nprocent\nav\nalla\ntrafikolyckor,\nkan\nsj\u00e4lvk\u00f6rande\nbilar\nf\u00f6rb\u00e4ttra\ntrafiks\u00e4kerheten\noch\nminska\nantalet\ntrafikolyckor.\nDet\nkan\nleda\ntill\nf\u00e4rre\nd\u00f6dsfall\noch\nskador\ni\ntrafiken\noch\nd\u00e4rmed\ni\nsin\ntur\nbidra\ntill\nen\nh\u00f6gre\nlivskvalitet\noch\nst\u00f6rre\ntrygghet\ni\ntrafiken.\nSj\u00e4lvk\u00f6rande\nbilar\nkan\n\u00e4ven\nbidra\ntill\nen\n5\n(\n13\n)",
        "metadata": {
            "source": "pdf/22.pdf",
            "page": 4
        }
    },
    {
        "page_content": "\u00f6kad\ntillg\u00e4nglighet\nf\u00f6r\n\u00e4ldre\npersoner\neller\nmed\nfunktionsneds\u00e4ttningar\nsom\nannars\nskulle\nha\nbegr\u00e4nsad\ntillg\u00e5ng\ntill\ntransportmedel\n[2],\n[3].\nP\u00e5\ns\u00e5\ns\u00e4tt\nkan\ntekniken\nerbjuda\nen\nmer\ninkluderande\ntransportl\u00f6sning\noch\nm\u00f6jligg\u00f6ra\nf\u00f6r\nfler\nm\u00e4nniskor\natt\ndelta\ni\nsamh\u00e4llet\noch\nd\u00e4rmed\nst\u00e4rka\nj\u00e4mlikheten.\nDessutom\nkan\nsj\u00e4lvk\u00f6rande\nbilar\nha\nen\npositiv\ninverkan\np\u00e5\nden\ngenerella\nlivskvaliteten\nd\u00e5\ndessa\nbilar\nhar\npotentialen\natt\nfrig\u00f6ra\ntid\nvid\nbilk\u00f6rning\nresor\nsom\n\u00e4r\nl\u00e5nga,\neller\nl\u00e5ta\nen\nf\u00f6rare\nslippa\ntrafiksituationer\nsom\nupplevs\nsom\nstressigt\neller\ntr\u00f6ttande\n[2].\nNegativa\nkonsekvenser\nkan\nvara\nrisk\nf\u00f6r\n\u00f6kad\nos\u00e4kerhet\ngenom\natt\nsystemen\n\u00e4r\nk\u00e4nsliga\nf\u00f6r\ncyberattacker.\nI\ndiskussionen\nom\nsocial\nh\u00e5llbarhet\n\u00e4r\ndet\n\u00e4ven\nviktigt\natt\n\u00f6verv\u00e4ga\nde\nvertikala\noch\nhorisontella\nrelationer.\nMed\nvertikala\nrelationer/interaktioner\nhandlar\ndet\nom\ninteraktioner\nmellan\nolika\nniv\u00e5er\ninom\nsamh\u00e4llet,\ntill.exempel\nf\u00f6retag\noch\nindivid.\nHorisontella\nrelationer\nsyftar\np\u00e5\ninteraktioner\np\u00e5\nsamma\nniv\u00e5,\ndet\nvill\ns\u00e4ga\nmellan\nf\u00f6retag\noch\nmellan\nindivider\n[1].\n5.2.2\nEkonomisk\nh\u00e5llbarhet\nEkonomisk\nh\u00e5llbarhet\nhar\nolika\ndefinitioner,\nmen\ndet\nhandlar\ni\nprincip\nom\natt\ns\u00e4kerst\u00e4lla\natt\nett\nsystem\nkan\nfungera\noch\nutvecklas\np\u00e5\nl\u00e5ng\nsikt\nutan\natt\nman\n\u00e4ventyrar\nden\nekonomiska\ntillv\u00e4xten\n[1].\nEkonomisk\nh\u00e5llbarhet\nutg\u00f6r\nen\ncentral\ndel\nav\nsj\u00e4lvk\u00f6rande\nbilar,\neftersom\nsj\u00e4lvk\u00f6rande\nbilar\nhar\npotentialen\natt\nminska\n\u00e4gande-\noch\ndriftskostnader\ngenom\natt\neffektivisera\ntransporter.\nDetta\nsker\ngenom\natt\noptimera\nrutter,\nhastighet\noch\nminskad\nk\u00f6bildning\ngenom\natt\nh\u00e5lla\nkonstant\navst\u00e5nd\nmellan\nbilar.\nResultatet\nblir\nl\u00e4gre\nanv\u00e4ndning\nav\nel\ni\nelbilar,\nminskat\nunderh\u00e5ll\noch\nb\u00e4ttre\ninfrastruktur.\nSj\u00e4lvk\u00f6rande\nbilar\nhar\n\u00e4ven\npotentialen\natt\nfrig\u00f6ra\nmer\ntid\nf\u00f6r\nf\u00f6raren\nmed\nuppgifter\nindividen\nannars\nhade\nvarit\nupptagen\nmed\natt\nk\u00f6ra,\ndet\ninneb\u00e4r\natt\nsamh\u00e4llet\nblir\npotentiellt\nmer\ntidseffektiv\n[2],\n[3],\nvilket\ni\nsin\ntur\nskulle\nkunna\nbidra\ntill\nskapandet\nav\nnya\naff\u00e4rsm\u00f6jligheter,\nvilket\nkan\nbidra\ntill\nen\nekonomisk\ntillv\u00e4xt.\nDessutom\nfinns\ndet\npotential\nf\u00f6r\nnya\nmarknader\noch\naff\u00e4rsmodeller\natt\nuppst\u00e5,\ngenom\ndelade\ntransport\nsom\ntj\u00e4nst,\nmen\n\u00e4ven\nandra\nmarknader\nsom\ninte\ni\nnul\u00e4get\ng\u00e5r\natt\nf\u00f6rutsp\u00e5\n[3],\n[4].\n5.2.3\nEkologisk\nh\u00e5llbarhet\nEkologisk\nh\u00e5llbarhet\ninnefattar\naspekter\nom\natt\ninte\nl\u00e5ta\nm\u00e4nsklig\naktivitet\n\u00f6verutnyttja\neller\nkompromissa\nnaturresurser\noch\nbiologisk\nm\u00e5ngfald,\noch\nbevara\noch\nskydda\nde\nekologiska\nsystem\n[1].\nSj\u00e4lvk\u00f6rande\nbilar\nkan\nbidra\ntill\nen\nminskad\nmilj\u00f6p\u00e5verkan\noch\n\u00f6ka\nden\nekologiska\nh\u00e5llbarheten\np\u00e5\nflera\ns\u00e4tt.\n\u00c4ven\nh\u00e4r\nser\nvi\natt\ntekniken\nkan\noptimera\nrutter\noch\nk\u00f6rm\u00f6nster\nf\u00f6r\natt\np\u00e5\ns\u00e5\ns\u00e4tt\nminska\nbehovet\nav\natt\nladda\nupp\nbilen.\n\u00c4ven\nminskad\ntrafik\nbidrar\npositivt\ntill\nden\nekologiska\nh\u00e5llbarheten\nd\u00e5\nproduktionen\nav\nnya\nbilar\nkan\nminska.\nDessutom\nskulle\nt\u00e4nkta\ntj\u00e4nster\nf\u00f6r\natt\ndela\nbil,\nminska\nbehovet\nav\nprivat\u00e4gda\nbilar\nvilket\ninneb\u00e4r\nen\nmer\neffektiv\nanv\u00e4ndning\nav\nresurser.\nSj\u00e4lvk\u00f6rande\nbilar\nkunna\nbidra\ntill\nen\nmer\neffektiv\nanv\u00e4ndning\nav\nyta\noch\ninfrastruktur\ngenom\natt\nminska\np\u00e5\ntrafik\noch\nd\u00e4rmed\ntr\u00e4ngsel\noch\n\u00e4ven\nminska\np\u00e5\nparkeringsytor\nf\u00f6r\natt\nfrig\u00f6ra\nytor\nf\u00f6r\nannan\nanv\u00e4ndning.\nNegativa\nkonsekvenser\nkan\nvara\n\u00f6kad\ntrafik\nom\nfler\nv\u00e4ljer\nbilen\nframf\u00f6r\nmer\nmilj\u00f6v\u00e4nliga\nalternativ\n[2],\n[3],\n[4].\n6\n(\n13\n)",
        "metadata": {
            "source": "pdf/22.pdf",
            "page": 5
        }
    },
    {
        "page_content": "5.3\nEtik\nInom\netik\nfinns\ndet\nframf\u00f6r\nallt\ntre\nf\u00f6rh\u00e5llningss\u00e4tt,\nvilket\n\u00e4r\ndygdetik,\npliktetik\noch\nkonsekvensetik,\ndessa\nramar\nser\np\u00e5\nproblem\np\u00e5\nlite\nolika\ns\u00e4tt.\nPliktetik\nkan\n\u00e4ven\nkallas\nf\u00f6r\nregeletik\noch\ninneb\u00e4r\natt\nsamh\u00e4llets\nnormer\navg\u00f6r\nvad\nsom\n\u00e4r\nr\u00e4tt\neller\nfel\ni\nolika\nsituationer\n[6].\nDygdetik\nanser\natt\nen\nperson\nsom\nf\u00f6ljer\ndygder\nautomatiskt\ng\u00f6r\nmoraliskt\nr\u00e4tt\nbeslut\n[6].\nOch\nkonsekvensetiken\nanser\natt\nman\nska\nse\np\u00e5\nkonsekvenserna\nman\nskapar\nav\nsin\nhandling\nf\u00f6r\natt\nveta\nom\nen\nhandling\n\u00e4r\nmoraliskt\nsund\n[6].\nN\u00e4r\ndet\nkommer\ntill\netiken\nang\u00e5ende\nsj\u00e4lvk\u00f6rande\nbilar\n\u00e4r\nen\nav\nde\nst\u00f6rre\ndiskussionerna\nom\nhur\nsj\u00e4lvk\u00f6rande\nbilar\nb\u00f6r\nhantera\nen\nolyckssituation.\nEtt\nk\u00e4nt\ndilemma\n\u00e4r\ndet\ns\u00e5\nkallade\n\u201dTrolley\nproblem\u201d\nvilket\nkortfattat\ninneb\u00e4r\natt\nen\n\u00e5sk\u00e5dare\ntill\nen\nvagn\nsom\nrullar\np\u00e5\nen\nr\u00e4ls\nkan\nv\u00e4lja\natt\nagera\ngenom\natt\ndra\ni\nen\nspak\noch\nr\u00e4dda\ntre\npersoner\neller\nf\u00f6rh\u00e5lla\nsig\npassiv\noch\nr\u00e4dda\nen\nperson\n[5].\nLiknande\netiska\nutmaningar\nkan\nformuleras\nf\u00f6r\nsj\u00e4lvk\u00f6rande\nbilar,\ndet\nvill\ns\u00e4ga\nhur\nska\nen\nbil\nv\u00e4lja\nmellan\ntv\u00e5\nlika\nnegativa\nresultat.\nSj\u00e4lvk\u00f6rande\nbilar\nm\u00e5ste\nd\u00e4rf\u00f6r\nvara\nrustade\nf\u00f6r\natt\nhantera\nkomplicerade\nsituationer.\nVilka\nkrav\nsom\nrimligt\nkan\nst\u00e4llas\np\u00e5\nen\nhelt\nsj\u00e4lvk\u00f6rande\nbil\ni\nolika\nsituationer\nkommer\natt\nvara\navg\u00f6rande\nf\u00f6r\nteknikens\nfortsatta\nutveckling.\n6.\nResultat\nDet\nfinns\nsom\nbeskrivits\novan\nm\u00e5nga\npotentiella\nf\u00f6rdelar\nur\nett\nh\u00e5llbarhetsperspektiv\nmed\nhelt\nsj\u00e4lvk\u00f6rande\nbilar.\nDet\ng\u00e4ller\nsamtliga\ntre\ndimensioner,\nsocial,\nekologisk\noch\nekonomisk\nh\u00e5llbarhet.\nH\u00e4r\nj\u00e4mf\u00f6r\nvi\ndet\nsom\nframkommit\ni\nv\u00e5r\nlitteraturs\u00f6kning\nmed\nresultatet\nfr\u00e5n\ngenomf\u00f6rd\nintervju.\n6.1\nTeknikv\u00e4rdering\nSamh\u00e4llet\nkommer\natt\np\u00e5verkas\nom\nsj\u00e4lvk\u00f6rande\nbilar\ninf\u00f6rs\np\u00e5\nbred\nfront.\nDet\nkommer\natt\nrevolutionera\ns\u00e4ttet\nvi\nreser\noch\nflyttar\ngods\np\u00e5.\nMen\nf\u00f6r\natt\ns\u00e4kerst\u00e4lla\natt\ndenna\nteknik\n\u00e4r\ns\u00e4ker,\neffektiv\noch\ntillg\u00e4nglig\nf\u00f6r\nsamh\u00e4llet,\nm\u00e4nniskor\noch\nmilj\u00f6n\nkr\u00e4vs\nnoggranna\nteknikv\u00e4rderingar\n[6].\nS\u00e4kerheten\nf\u00f6r\nsj\u00e4lvk\u00f6rande\nbilar\n\u00e4r\nden\nviktigaste\nsamh\u00e4llsaspekten\nn\u00e4r\ndet\nkommer\ntill\ndenna\nteknik.\nDessa\nbilar\nm\u00e5ste\nkunna\nnavigera\ns\u00e4kert\ngenom\nmilj\u00f6er\noch\nhantera\nomst\u00e4ndigheter\nav\nalla\nslag\n[7].\nTillf\u00f6rlitlighetsbed\u00f6mning\nf\u00f6r\ntekniken\nbakom\nsj\u00e4lvk\u00f6rande\nbilar\nsom\nmjukvara,\nh\u00e5rdvara\noch\nsensorer\nm\u00e5ste\nd\u00e4rf\u00f6r\nutv\u00e4rderas\noch\ntestas\nnoggrant\ninnan\nbilarna\nlanseras\np\u00e5\nmarknaden.\n7\n(\n13\n)",
        "metadata": {
            "source": "pdf/22.pdf",
            "page": 6
        }
    },
    {
        "page_content": "N\u00e4r\ndet\ng\u00e4ller\nhantering\noch\nreaktionstider\nb\u00f6r\nprestandan\nhos\nsj\u00e4lvk\u00f6rande\nbilar\nvara\nb\u00e4ttre\n\u00e4n\neller\n\u00e5tminstone\ni\nniv\u00e5\nmed\nm\u00e4nskliga\nf\u00f6rare.\nKomplexa\nmilj\u00f6er\ninklusive\nkorsningar,\nmotorv\u00e4gar\noch\nparkeringsplatser\nm\u00e5ste\nkunna\nhanteras\nav\nsj\u00e4lvk\u00f6rande\nbilar\n[8].\nF\u00f6retaget\nbeskriver\nsin\nvision\nsom\natt\nde\nvill\nbidra\ntill\natt\nsnabbare\nf\u00e5\nned\nantalet\nolyckor\ni\ntrafiken\ngenom\natt\ng\u00f6ra\ns\u00e5\natt\nbilarna\nk\u00f6r\nb\u00e4ttre\n\u00e4n\nm\u00e4nniskor\n[9],\nmen\nfokus\nligger\njust\nnu\ninte\np\u00e5\nsj\u00e4lvk\u00f6rande\nbilar.\nDessa\n\u00e4r\n\u00e4n\ns\u00e5\nl\u00e4nge\nganska\novanliga.\nMattias\nHenriksson\nbeskriver\ndem\nsom\ndyra,\nfr\u00e4mst\nberoende\np\u00e5\nl\u00e5g\ntillverkningsvolym\nav\nde\nsensorer\noch\nannan\nteknik\nsom\nanv\u00e4nds.\nSystem\nmed\nsj\u00e4lvk\u00f6rande\nbilar\nfinns\nocks\u00e5\nhuvudsakligen\ni\nomr\u00e5den\nd\u00e4r\ndet\ng\u00e5r\natt\nk\u00f6ra\nl\u00e5ngsamt,\nupp\nmot\n30\nkm/h,\noch\nd\u00e4r\ntrafiksystemen\n\u00e4r\nenkla.\nI\nintervjun\nexemplifieras\ndetta\nmed\nsj\u00e4lvk\u00f6rande\ntaxi-\neller\nbudbilar\nsom\nk\u00f6rs\ni\namerikanska\nst\u00e4der\nmed\nr\u00e4tlinjigt\ngatusystem.\nI\nst\u00e4der\nsom\nhar\nen\nst\u00f6kigare\ntrafik\nsom\nexempelvis\nJerusalem\nskulle\ndagens\nteknik\ninte\nfungera\n[10].\nV\u00e4gen\nfram\u00e5t\nbeskrivs\ni\nintervjun\nsom\nen\nfortsatt\nutveckling\nav\nfordon\nmed\nassistanssystem.\nDet\nvill\ns\u00e4ga\nutvecklingen\nkommer\natt\nske\ninkrementellt\ni\nmindre\nsteg.\nSamma\nbudskap\nges\nocks\u00e5\np\u00e5\nf\u00f6retagets\nhemsida\n[9].\nEftersom\nbilar\nmed\nassistanssystem\ns\u00e4ljs\ni\nen\nbetydligt\nst\u00f6rre\nvolym,\nkan\nman\ni\ndessa\nmodeller\ninf\u00f6ra\ndyrare\noch\nb\u00e4ttre\nteknik\nf\u00f6r\natt\np\u00e5\nsikt\nf\u00e5\nned\nkostnaderna.\nEtt\nexempel\n\u00e4r\nVolvo\nsom\ni\nkommande\nmodeller\ninf\u00f6r\ns\u00e5\nkallad\nlidar,\nvilket\ni\nkorthet\n\u00e4r\nen\nlaserbaserad\nradar.\nLidar\nhar\ntidigare\ni\nprincip\nbara\nfunnits\ni\nhelt\nsj\u00e4lvk\u00f6rande\nbilar\n[10].\nAtt\nanv\u00e4nda\nbilar\nmed\nassistanssystem\nf\u00f6r\natt\nintroducera\nmer\navancerade\nsystem\nhar\nenligt\nMattias\nHenriksson\nocks\u00e5\nf\u00f6rdelen\natt\nf\u00f6raren\nalltid\n\u00e4r\nansvarig\nf\u00f6r\nfordonets\nframf\u00f6rande\n[10].\nDet\nvill\ns\u00e4ga\nman\nslipper\nansvarsfr\u00e5gan.\nEventuella\nfel\nsom\nuppst\u00e5r\nblir\nd\u00e4rigenom\nenklare\natt\n\u00e5tg\u00e4rda.\nM\u00e4ngden\nfordon\nmed\nde\nolika\nsystemen\ng\u00f6r\nocks\u00e5\natt\nfel\nsnabbare\nger\nsig\ntill\nk\u00e4nna.\nTekniken\nbeskrivs\nav\nf\u00f6retaget\nsom\nl\u00e4rande,\ndet\nvill\ns\u00e4ga\nvarje\nolycka\neller\nincident\nbidrar\ntill\nen\n\u00f6kad\nkunskapsbas\nsom\nanv\u00e4nds\nf\u00f6r\natt\nf\u00f6rb\u00e4ttra\nsystemen\n[10].\nInf\u00f6randet\nav\nhelt\nsj\u00e4lvk\u00f6rande\nbilar\nkommer\natt\nske\nsuccessivt.\nInte\nbara\nsom\nbeskrivs\novan\ngenom\nutveckling\nav\nalltmer\navancerade\nf\u00f6rarassistanssystem,\nutan\nocks\u00e5\ngenom\natt\nsj\u00e4lvk\u00f6rande\nbilar\nf\u00e5r\ntillst\u00e5nd\natt\nk\u00f6ra\ni\nrelativt\nenkla\nmilj\u00f6er\nsom\nexempelvis\nmotorv\u00e4gar.\nOmr\u00e5den\nmed\ntill\u00e5telse\natt\nframf\u00f6ra\nhelt\nsj\u00e4lvk\u00f6rande\nbilar\nkan\nsedan\nutvidgas.\nDenna\nutveckling\nbygger\ndock\np\u00e5\natt\nbilarna\nb\u00e5de\nkan\nframf\u00f6ras\nsom\nsj\u00e4lvk\u00f6rande\noch\nmed\nen\nf\u00f6rare\nsom\nhar\nansvaret\n[10].\n6.2\nH\u00e5ller\ntekniken\nvad\nden\nlovar\nur\nett\nh\u00e5llbarhetsperspektiv?\nEnligt\nMattias\nHenriksson\n\u00e4r\ndet\nsom\nocks\u00e5\nn\u00e4mnts\novan,\nfr\u00e4mst\n\u00f6kad\ns\u00e4kerhet\nsom\nkan\ntillhandah\u00e5llas.\nDet\ng\u00e4ller\nb\u00e5de\nf\u00f6r\nhelt\nsj\u00e4lvk\u00f6rande\nbilar\n(AD)\noch\nf\u00f6r\nbilar\nmed\nassistanssystem\n(ADAS).\nHan\nlyfter\nfram\nstatistik\nsom\nvisar\natt\nADAS\nkan\nminska\nantalet\ntrafikolyckor\nmed\nupp\ntill\n30\nprocent\n[10].\nDe\nsenare\nbilarna\nfinns\nredan\np\u00e5\nmarknaden\nmed\nsystem\nsom\nexempelvis\nkan\nbromsa\nbilen\nvid\nfara,\nstyra\nupp\nbilen\nom\nden\n\u00e4r\np\u00e5\nv\u00e4g\ni\ndiket\neller\nriskerar\natt\nhamna\np\u00e5\nfel\nsida\nv\u00e4gen.\nAlla\ndessa\nfunktioner\nbidrar\ntill\natt\n\u00f6ka\ntrafiks\u00e4kerheten.\nMinskade\nolyckor\ninneb\u00e4r\nocks\u00e5\nminskade\nkostnader\nf\u00f6r\nsamh\u00e4llet\nvilket\nbidrar\ntill\nekonomisk\nh\u00e5llbarhet.\n8\n(\n13\n)",
        "metadata": {
            "source": "pdf/22.pdf",
            "page": 7
        }
    },
    {
        "page_content": "Helt\nsj\u00e4lvk\u00f6rande\nbilar\nskulle\ni\nett\nbreddinf\u00f6rande\nkunna\nminska\nk\u00f6er,\nfrig\u00f6ra\ntid\nf\u00f6r\nf\u00f6raren\noch\nskapa\nm\u00f6jligheter\nf\u00f6r\nf\u00e4rre\nbilar\ni\noch\nmed\natt\nen\nbil\nkan\nanv\u00e4ndas\ntill\nannat\n\u00e4ven\nn\u00e4r\nf\u00f6raren\nexempelvis\n\u00e4r\np\u00e5\njobbet.\nMen\nser\nman\ntill\nutvecklingen\nsom\nbeskrivs\nteknikv\u00e4rderingen\novan,\ns\u00e5\n\u00e4r\ndet\nl\u00e5ngt\nkvar.\nI\nintervjun\nframkom\natt\nden\nekologiska\naspekten\ni\nf\u00f6retaget\ninte\n\u00e4r\ns\u00e5\nframtr\u00e4dande\neftersom\nsj\u00e4lvk\u00f6rande\nbilar\ni\nprincip\nalltid\n\u00e4r\nelbilar\n[10].\nMen\nf\u00f6retaget\nmenar\nocks\u00e5\natt\nen\nteknik\nsom\ninneb\u00e4r\natt\nman\nkan\nuppdatera\nbilens\nmjukvara,\nungef\u00e4r\nsom\nen\ndator,\nskulle\ninneb\u00e4ra\natt\nbilens\nlivsl\u00e4ngd\n\u00f6kar\n[9]\noch\nd\u00e4rmed\nminska\nbehovet\nav\natt\nk\u00f6pa\nen\nny\nbil.\n6.3\nEtiska\nfr\u00e5gest\u00e4llningar\nSom\ningenj\u00f6r\nmed\nansvar\nf\u00f6r\ntestning\noch\nverifiering\n\u00e4r\nMattias\nHenriksson\nmedveten\nom\netiska\nutmaningar\nsom\nf\u00f6ljer\nmed\nteknologin.\nHan\nbeskriver\nhur\nman\nsom\ntestare\nhar\nansvar\nb\u00e5de\nf\u00f6r\natt\ntesterna\nfaktiskt\nverifierar\natt\nmjukvaran\nm\u00f6ter\nde\ntekniska\nspecifikationerna\n[10].\nMen\nocks\u00e5\natt\nde\ntekniska\nspecifikationerna\n\u00e4r\nrimliga.\nF\u00f6r\nMattias\nroll\nkan\nman\ns\u00e4ga\natt\nhan\njobbar\nf\u00f6r\np\u00e5\nett\npositivt\ns\u00e4tt\nf\u00f6r\netiska\nf\u00f6rh\u00e5llningss\u00e4tt\nd\u00e5\nman\narbetar\nf\u00f6r\natt\nf\u00f6rs\u00e4kra\nsig\nom\natt\ninte\nsl\u00e4ppa\nut\nen\nprodukt\nsom\ninte\n\u00e4r\nfullt\nfunktionell\n[10].\nUnder\nintervjun\nframg\u00e5r\ndet\natt\ni\nden\nroll\nden\nintervjuade\nhar\ns\u00e5\n\u00e4r\nandra\navdelningar\np\u00e5\nf\u00f6retaget\nsom\ntar\nfram\nstrategier\noch\nspecifikationer\n[10].\nM\u00f6jligheten\natt\np\u00e5verka\ndessa\n\u00e4r\nmer\nindirekt.\nAtt\nrapportera\nfel\neller\navvikelser\nuppmuntras\nav\nf\u00f6retaget\nliksom\np\u00e5pekanden\natt\nf\u00f6reslagen\nverifieringsmetod\nav\nolika\nsk\u00e4l\nborde\nf\u00f6r\u00e4ndras\n[10].\nEn\npopul\u00e4r\netisk\ndilemma\ng\u00e4llande\ns\u00e4kerhet\n\u00e4r\ndet\nk\u00e4nda\n\u201ctrolley-problemet\u201d\nsom\nbeskrivits\novan.\nMattias\nHenriksson\n\u00e4r\navf\u00e4rdande\nom\ndilemmat\nmed\nmotiveringen\natt\nom\nen\nbil\n\u00e4r\ni\nen\ns\u00e5dan\nsituation\ns\u00e5\nhar\ndet\nredan\nuppst\u00e5tt\nett\nproblem\nsom\nborde\nha\nundvikits\nfr\u00e5n\nb\u00f6rjan,\nprecis\nsom\nvi\nm\u00e4nniskor\ngenerellt\ninte\nhamnar\ni\ns\u00e5dana\nsituationer\n[10].\nN\u00e4r\ndet\ng\u00e4ller\nADAS,\nfordon\nmed\nassistanssystem\n\u00e4r\ndet\nalltid\nf\u00f6raren\nsom\nhar\ndet\nyttersta\nansvaret.\nMen\nf\u00f6r\nAD,\nsj\u00e4lvk\u00f6rande\nbilar\n\u00e4r\ndet\nlite\nmer\nos\u00e4kert\n[10].\nMattias\nHenriksson\nn\u00e4mner\ndock\natt\nVolvo\nhar\nmeddelat\natt\nde\nskulle\nta\nansvar\nom\nn\u00e5got\nh\u00e4nder\nmed\nderas\nsj\u00e4lvk\u00f6rande\nbilar\n[10].\nDetta\nmarkerar\nen\nviktig\nskiftning\ni\nansvar\nfr\u00e5n\nf\u00f6raren\ntill\ntillverkaren,\noch\ndet\n\u00e4r\nen\nfr\u00e5ga\nsom\nkommer\natt\nkr\u00e4va\nmycket\ndebatt\noch\nlagstiftning\nf\u00f6r\natt\nl\u00f6sa.\nSj\u00e4lvk\u00f6rande\nbilar\n\u00e4r\nmycket\nberoende\nav\nsensorer\noch\navancerade\ndatorer,\nvilket\ng\u00f6r\natt\ndet\nst\u00e4ndigt\nfinns\noro\nf\u00f6r\ncyberattacker\n[11].\nAtt\nutv\u00e4rdera\nde\ncybers\u00e4kerhets\u00e5tg\u00e4rder\nsom\nimplementeras\ni\ndessa\nfordon\n\u00e4r\nviktigt\nf\u00f6r\natt\nse\ntill\natt\nsj\u00e4lvk\u00f6rande\nbilar\nskyddas\nfr\u00e5n\nhackning.\nMattias\nHenriksson\n\u00e4r\nmedveten\nom\ntyngden\nav\nproblemet\nmen\nhar\nsj\u00e4lv\ningen\ninsikt\np\u00e5\nhur\nZenseact\narbetar\nmed\ndessa\nfr\u00e5gor\n[10].\nTillg\u00e4ngligheten\nf\u00f6r\nsj\u00e4lvk\u00f6rande\nbilar\nf\u00f6r\nolika\nbefolkningsgrupper\n\u00e4r\nviktig\ni\nen\netisk\nteknikv\u00e4rdering.\nSj\u00e4lvst\u00e4ndigheten\noch\nr\u00f6rligheten\nf\u00f6r\nm\u00e4nniskor\nsom\ninte\nkan\nk\u00f6ra\nbil,\ns\u00e5som\n\u00e4ldre\neller\npersoner\nmed\nfunktionshinder,\nskulle\nkunna\n\u00f6ka\ntack\nvare\nsj\u00e4lvk\u00f6rande\nteknik\n[11].\nD\u00e5\nuppst\u00e5r\nfr\u00e5gor\nom\nhur\ntillg\u00e4ngligheten\nf\u00f6r\ndessa\ngrupper\nkommer\natt\nvara?\nVem\n\u00e4r\ndet\nsom\nkommer\nha\nr\u00e5d\nmed\ntekniken\noch\nkan\ndet\nleda\ntill\nsociala\noch\nekonomiska\nklyftor?\nMattias\nHenriksson\ntror\natt\np\u00e5\nsamma\ns\u00e4tt\nsom\natt\nalla\nbilar\nnumera\nhar\n9\n(\n13\n)",
        "metadata": {
            "source": "pdf/22.pdf",
            "page": 8
        }
    },
    {
        "page_content": "antisladdsystem,\ntrots\natt\nn\u00e4r\ndessa\nvar\nv\u00e4ldigt\ndyra\nn\u00e4r\nde\ninf\u00f6rdes,\ns\u00e5\nkommer\npriset\ng\u00e5\nner\nf\u00f6r\nsj\u00e4lvk\u00f6rande\nbilar\ns\u00e5\natt\nde\nblir\nekonomiskt\ntillg\u00e4ngligt\nf\u00f6r\nflera\n[10].\n7.\nDiskussion\noch\nslutsats\nRapporten\nhar\nanalyserat\npotentialen\nf\u00f6r\nf\u00f6rarl\u00f6sa\nbilar,\nAD\noch\nADAS\nur\nett\nh\u00e5llbarhetsperspektiv.\nVi\nhar\nsett\natt\nb\u00e5de\nAD\noch\nADAS\n\u00e4r\nlovande\nteknologier\nn\u00e4r\ndet\ng\u00e4ller\n\u00f6kad\ntrafiks\u00e4kerhet.\nDe\nsystem\nredan\nidag\nfinns,\nfr\u00e4mst\ni\nbilar\nmed\nassistanssystem,\nkan\nminska\nantalet\ntrafikolyckor.\nResultatet\n\u00e4r\natt\nman,\nsparar\nliv\noch\n\u00e4ven\nminskar\nkostnader\nf\u00f6r\nsamh\u00e4llet\noch\nindivider\n[13],\n[14].\nDet\n\u00e4r\nen\ntydlig\nkoppling\ntill\nb\u00e5de\nsocial\noch\nekonomisk\nh\u00e5llbarhet.\nUt\u00f6ver\ns\u00e4kerheten\nfinns\ndet\nf\u00f6rv\u00e4ntningar\natt\ntekniken\nska\nminska\ntrafikstockningar,\nfrig\u00f6ra\ntid\nf\u00f6r\nf\u00f6rare\noch\nskapa\nm\u00f6jligheter\nf\u00f6r\natt\nfrig\u00f6ra\nytor\nav\nbilar\n[2],\n[4].\nH\u00e4r\nfinns\nen\nkoppling\nf\u00f6r\nekologisk\nh\u00e5llbarhet.\n\u00c4ven\nom\ndet\nfinns\ngott\nom\ntekniska\nframsteg\n\u00e4r\ndet\ntydligt\natt\nmycket\narbete\n\u00e5terst\u00e5r\nf\u00f6r\natt\ng\u00f6ra\nn\u00e4r\ndet\ng\u00e4ller\natt\ng\u00f6ra\ndessa\nsystem\np\u00e5litliga\noch\ntillg\u00e4ngliga\nf\u00f6r\nallm\u00e4nheten.\nVi\ntror\ntill\nexempel\natt\nvi\nf\u00f6rst\nkommer\natt\nse\nminskade\ntrafikstockningar\nf\u00f6rst\nn\u00e4r\nen\nsignifikant\ndel\nav\ntrafikanterna\nhar\nadopterat\nsj\u00e4lvk\u00f6rande\nbilar.\nDe\nf\u00f6rdelar\nsom\nh\u00e4nf\u00f6r\nsig\ntill\nekologisk\nh\u00e5llbarhet\nligger\nd\u00e4rf\u00f6r\nl\u00e4ngre\nfram\ni\ntiden.\nVi\nnoterar\natt\nf\u00f6retaget\ninte\nfokuserar\ns\u00e5\nmycket\np\u00e5\nde\nekologiska\nh\u00e5llbarhetsaspekterna.\nI\nvidare\nperspektiv\nskulle\nalternativ\ntill\nutveckling\nav\nsj\u00e4lvk\u00f6rande\nbilar\nkunna\nvara\nutveckling\nav\nfler\nh\u00e5llbara\nmobilitetsl\u00f6sningar\nsom\nkollektivtrafik,\ng\u00e5ng\noch\ncykel.\nDock\ntror\nvi\natt\nbilen\nkommer\natt\nvara\ncentral\nf\u00f6r\nde\nflesta\nmobilitetsl\u00f6sningar\nunder\n\u00f6versk\u00e5dlig\nframtid\nF\u00f6r\natt\ntekniken\nmed\nsj\u00e4lvk\u00f6rande\nbilar\nska\nbli\nframg\u00e5ngsrik\nfinns\ndet\nbehov\nav\natt\nforts\u00e4tta\ndiskutera\nde\netiska\nfr\u00e5gest\u00e4llningarna.\nMedan\nteknologin\nutvecklas\nbeh\u00f6ver\nlagen\noch\nvi\nsom\nsamh\u00e4lle\nkomma\ni\nkapp.\nTill\nexempel\nkan\nstatlig\npolitik\noch\nregleringar\np\u00e5verka\nhur\nf\u00f6retag\nutvecklar\nsj\u00e4lvk\u00f6rande\nteknologi.\nSer\nman\ntill\nvertikala\nrelationer\nmellan\nf\u00f6retag\noch\nkonsument\nsamt\nmellan\nstat\noch\nmedborgare\ns\u00e5\nbeh\u00f6vs\nett\nrobust\nregelverk\nsom\nalla\nkan\nlita\np\u00e5.\nHorisontella\nrelationer\nkan\nocks\u00e5\np\u00e5verka\nutvecklingen\nd\u00e5\nsamarbeten\nmellan\nolika\nf\u00f6retag\nkan\nske\nf\u00f6r\natt\nf\u00f6rb\u00e4ttra\nteknologin.\nFr\u00e5gor\nom\nvem\nsom\nb\u00e4r\nansvar\nvid\nolycka\nsamt\ntillg\u00e4nglighet\noch\nr\u00e4ttvisa\nm\u00e5ste\nocks\u00e5\n\u00f6verv\u00e4gas\nf\u00f6r\natt\nse\ntill\natt\ndessa\nsystem\ngynnar\ns\u00e5\nm\u00e5nga\nsom\nm\u00f6jligt\noch\ninte\norsakar\neventuella\nsociala\noch\nekonomiska\nklyftor\ni\nsamh\u00e4llet\n[4].\nKvarst\u00e5ende\nutmaningar\n\u00e4r\natt\ngarantera\ns\u00e4kerhet\noch\nrobusthet\nf\u00f6r\nbilens\nolika\ntekniska\nsystem.\nBilens\nsystem\nm\u00e5ste\nskyddas\nmot\nolovlig\np\u00e5verkan\noch\nom\nsystemen\nfallerar\nm\u00e5ste\nbilen\nkunna\nframf\u00f6ras\nmanuellt.\n\u00c4ven\ndetta\nkan\nh\u00e4nf\u00f6ras\ntill\nden\nvertikala\nrelationen\nmellan\nf\u00f6retag\noch\nkonsumenter.\nN\u00e4r\ndet\ng\u00e4ller\ningenj\u00f6rens\nansvar\noch\nm\u00f6jligheter\natt\np\u00e5verka\ns\u00e5\nberor\ndet\np\u00e5\nvar\ni\nf\u00f6retagsstrukturen\nman\nverkar.\nDet\ndirekta\nansvaret\ninneb\u00e4r\natt\ngenomf\u00f6ra\nsina\nuppgifter\nenligt\nden\nuppdragsbeskrivning\nsom\ngivits.\nSom\ningenj\u00f6r\nfinns\ndet\nd\u00e5\nalltid\nen\nrisk\natt\nman\nblir\nen\nkugge\ni\nhjulet\nmed\nbegr\u00e4nsade\nm\u00f6jligheter\natt\np\u00e5verka\nteknikutveckling.\nResultatet\nav\ndetta\n\u00e4r\natt\ndet\nkan\nvara\nsv\u00e5rt\natt\nse\nvilka\neventuella\nnegativa\nkonsekvenser\nens\narbete\nkan\n10\n(\n13\n)",
        "metadata": {
            "source": "pdf/22.pdf",
            "page": 9
        }
    },
    {
        "page_content": "leda\ntill.\nI\nZensects\nfall\nupplever\nvi\natt\nf\u00f6retaget\nuppmuntrar\nen\npositiv\nsamverkan\nmellan\ndess\nanst\u00e4llda\noch\nett\ngemensamt\nansvarstagande\ni\nutvecklingsprocessen\n[9].\nVi\nkan\nkonstatera\natt\nteknologin\nf\u00f6r\nsj\u00e4lvk\u00f6rande\nbilar,\nb\u00e5de\ni\nform\nav\nAD\noch\nADAS,\nhar\nen\nstor\npotential\natt\nbidra\ntill\nen\nmer\nh\u00e5llbar\noch\ns\u00e4ker\nframtid.\nDe\nbetydande\nf\u00f6rdelar\nsom\nvi\nhar\nidentifierat\ninkluderar\n\u00f6kad\ntrafiks\u00e4kerhet,\npotentialen\nf\u00f6r\nminskade\ntrafikstockningar,\nfrig\u00f6rande\nav\ntid\nf\u00f6r\nf\u00f6rare\noch\nm\u00f6jligheterna\natt\nfrig\u00f6ra\nytor\nf\u00f6r\nandra\n\u00e4ndam\u00e5l.\nDessa\nframsteg\np\u00e5verkar\npositivt\nalla\naspekter\nav\nh\u00e5llbarhet;\nsocial,\nekonomisk\noch\nekologisk.\nEn\netisk\nteknikutv\u00e4rdering\nger\nvid\nhanden\natt\nde\npositiva\nf\u00f6rdelarna\n\u00f6verv\u00e4ger\neventuella\nnegativa\naspekter\noch\natt\ntekniken\nur\nett\nkonsekvensetiskt\nperspektiv\nd\u00e4rf\u00f6r\n\u00e4r\natt\nbetrakta\nsom\netisk.\nDen\nnegativa\nkonsekvens\nsom\nvi\nser\nsom\nviktigast\natt\nundvika\n\u00e4r\nom\ntekniken\n\u00f6kar\nsamh\u00e4llets\ns\u00e5rbarhet\ngenom\nen\nf\u00f6r\nsvag\ndatas\u00e4kerhet\ni\nsystemen\nsamt\nrisk\nf\u00f6r\noj\u00e4mlikt\ninf\u00f6rande\nd\u00e4r\nvissa\nsamh\u00e4llsgrupper\nst\u00e4lls\nutan\nm\u00f6jlighet\natt\nta\ntillvara\nden\nnya\ntekniken.\nTeknologin\nbeh\u00f6ver\nforts\u00e4tta\natt\nutvecklas,\nlagstiftning\noch\nregelverk\nbeh\u00f6ver\nkomma\ni\nkapp\nf\u00f6r\natt\nta\nitu\nmed\nfr\u00e5gor\nom\nansvar\nvid\nolyckor\noch\nkrav\np\u00e5\nsystems\u00e4kerhet.\nSamtidigt\nbeh\u00f6ver\nvi\nsom\nsamh\u00e4llsakt\u00f6rer\nreflektera\n\u00f6ver\nde\npotentiella\nsociala\noch\nekonomiska\noj\u00e4mlikheter\nsom\ndenna\nteknologi\nkan\norsaka.\nK\u00e4llv\u00e4rdering\nZenseact\noch\nandra\nbilf\u00f6retag\nHemsidor\nfr\u00e5n\nZenseact,\nGM\noch\nTesla\nhar\nanv\u00e4nts\nf\u00f6r\natt\nge\nanv\u00e4ndbar\ninformation\nom\nf\u00f6retagens,\nmen\n\u00e4ven\nindustrins,\nuppdrag,\nvision,\nprodukter\noch\nstatistik.\nTrots\natt\ndet\nfinns\nanv\u00e4ndbar\ninformation\natt\nh\u00e4mta\nhos\nf\u00f6retagen\n\u00e4r\ndet\nviktigt\natt\nt\u00e4nka\np\u00e5\natt\nf\u00f6retagen\nsj\u00e4lva\nkan\nvara\npartiska\noch\ninte\nobjektiva\nmed\nhur\nde\nf\u00f6rmedlar\ninformation\noch\nborde\nd\u00e4rf\u00f6r\nanv\u00e4ndas\nf\u00f6rsiktigt,\ns\u00e5\ndet\nfinns\nen\nrisk\natt\npositiva\naspekter\nlyfts\nupp\nmedan\nnegativa\naspekter\nminimeras.\nKurslitteratur\nKurslitteratur,\nsom\nH\u00e5llbar\nutveckling\n-\nnyanser\noch\ntolkningar\noch\nTeknik\noch\netik,\n\u00e4r\nframtagen\nf\u00f6r\nutbildningssyfte\noch\nen\nbra\nk\u00e4lla\nf\u00f6r\natt\nskaffa\nsig\nen\nmer\ngrundl\u00e4ggande\nf\u00f6rst\u00e5else\nav\nbegrepp.\nBegrepp\nfr\u00e5n\ndessa\nk\u00e4llor\nhar\nvi\nanv\u00e4nt\noss\nsom\nutg\u00e5ngspunkter\nf\u00f6r\natt\nstrukturera\nv\u00e5ra\ntankar\noch\nf\u00f6r\natt\nformulera\nv\u00e5ra\nargument.\nMarknadsunders\u00f6knings-\noch\nkonsultfirma,\nAMR\nAllied\nMarked\nResearch\n(AMR)\nanv\u00e4nder\nGlobeNewswire\nsom\nen\nplattform\nf\u00f6r\natt\npublicera\nsina\nrapporter,\nhar\nanv\u00e4nts\nsom\nen\nk\u00e4lla\nf\u00f6r\natt\nf\u00e5\nen\nb\u00e4ttre\ninsikt\np\u00e5\nhur\nmarknaden\noch\nprognoser\nser\nut\nf\u00f6r\nbilindustrin.\n\u00c4ven\nom\nAMR\nbest\u00e5r\nav\nsj\u00e4lvst\u00e4ndiga\nanalytiker\noch\nkonsulter\nsom\ntillhanda\nh\u00e5ller\ninblickar\ni\nen\nindustri,\nverkar\nAMR\nvara\nen\nkommersiell\norganisation\nsom\ns\u00e4ljer\nsina\nrapporter.\nDetta\ng\u00f6r\natt\ndet\nkan\nfinnas\nen\nrisk\nf\u00f6r\npartiskhet\nf\u00f6r\natt\nframst\u00e4lla\nmarknaden\np\u00e5\nett\ns\u00e5dant\ns\u00e4tt\nsom\n\u00e4r\nmer\nattraktivt\nf\u00f6r\npotentiella\nk\u00f6pare.\nAkademiska\nartiklar\nFlera\nakademiska\nartiklar\nhar\nanv\u00e4nts\nf\u00f6r\natt\nge\nen\nmer\nvetenskaplig\noch\nteknisk\ninsikt\ni\n\u00e4mnet.\nK\u00e4llorna\nsom\n\"The\nFuture\nof\nTransportation:\nEthical,\nLegal,\nSocial\nand\nEconomic\nImpacts\nof\nSelf-driving\nVehicles\nin\nthe\nYear\n2025,\"\noch\n\"Help\nor\nhindrance?\nThe\ntravel,\n11\n(\n13\n)",
        "metadata": {
            "source": "pdf/22.pdf",
            "page": 10
        }
    },
    {
        "page_content": "energy\nand\ncarbon\nimpacts\nof\nhighly\nautomated\nvehicles,\"\nger\nmer\ndjupg\u00e5ende\nstudier\nom\ntekniska,\netiska\noch\nsamh\u00e4lleliga\naspekter\nav\nsj\u00e4lvk\u00f6rande\nbilar\nvilket\nvar\nv\u00e4ldigt\npassande\ntill\nv\u00e5r\nanalys.\nDessa\nk\u00e4llor\n\u00e4r\nvanligtvis\np\u00e5litliga\neftersom\nde\noftast\ngenomg\u00e5r\nen\ns\u00e5\nkallad\n\u201cpeer-review\u201d\nprocess,\ndvs\nen\ngranskning\nd\u00e4r\nartiklar\ngranskas\nav\nandra\nexperter\ninom\nsamma\n\u00e4mnesomr\u00e5de\ninnan\nde\npubliceras.\nDock\n\u00e4r\ndet\nviktigt\natt\np\u00e5peka\natt\nresultaten\noch\nslutsatserna\ni\nalla\nstudier\nfortfarande\nkan\nutmanas.\nIntervju\nmed\ningenj\u00f6r,\nMattias\nHenriksson,\nZenseact\nEn\nintervju\nmed\ningenj\u00f6ren\nMattias\nHenriksson\nfr\u00e5n\nZenseact,\nanv\u00e4nds\nf\u00f6r\natt\nf\u00e5\nunika\ninsikter\noch\nperspektiv\np\u00e5\nf\u00f6rarl\u00f6sa\nbilar\nd\u00e5\nhan\njobbar\naktivt\ninom\nindustrin.\nAtt\nf\u00e5\nen\nk\u00e4lla\nmed\nv\u00e4rdefulla\nerfarenheter\noch\nsynvinklar\n\u00e4r\nviktigt,\nmen\ndet\n\u00e4r\nocks\u00e5\nviktigt\natt\nt\u00e4nka\np\u00e5\natt\nhan\nkan\nvara\npartisk\nd\u00e5\nhan\neventuellt\nf\u00f6rs\u00f6ker\nrepresentera\nf\u00f6retagets\nsyn\np\u00e5\n\u00e4mnet.\nK\u00e4llh\u00e4nvisning\n[1]\nF.\nHedenus,\nM.\nPersson\noch\nF.\nSprei,\nH\u00e5llbar\nutveckling\n-\nnyanser\noch\ntolkningar.\n2.\nuppl.,\nLund,\nSverige,\nStudentlitteratur\nAB,\n2022.\n[2]\nM.\nRyan,\n\u201cThe\nFuture\nof\nTransportation:\nEthical,\nLegal,\nSocial\nand\nEconomic\nImpacts\nof\nSelf-driving\nVehicles\nin\nthe\nYear\n2025,\"\nScience\nand\nEngineering\nEthics\n,\nvol.\n26,\nnr.\n4,\nss.\n1185\u20131208,\nAug.\n2020,\ndoi:10.1007/s11948-019-00130-2.\n[3]\nZ.\nWadud,\nD.\nMacKenzie,\noch\nP.\nLeiby,\n\u201cHelp\nor\nhindrance?\nThe\ntravel,\nenergy\nand\ncarbon\nimpacts\nof\nhighly\nautomated\nvehicles,\"\nTransportation\nResearch\nPart\nA:\nPolicy\nand\nPractice\n,\nvol.\n86,\nnr.\n6,\nss.\n1-19,\nApr.\n2016,\ndoi:10.1016/j.tra.2015.12.001.\n[4]\nD.\nBissell,\nT.\nBirtchnell,\nA.\nElliott\noch\nE.\nL.\nHsu,\n\u201cAutonomous\nautomobilities:\nThe\nsocial\nimpacts\nof\ndriverless\nvehicles,\"\nEnvironment\nand\nPlanning\nD:\nSociety\nand\nSpace\n,\nvol.\n38,\nnr.\n2,\nss.\n219-236,\nApr.\n2020,\ndoi:10.1177/001139211881674.\n[5]\nS.\nNyholm,\noch\nJ.\nSmids,\n\u201cThe\nEthics\nof\nAccident-Algorithms\nfor\nSelf-Driving\nCars:\nan\nApplied\nTrolley\nProblem?,\"\nEthic\nTheory\nMoral\nPractice\n,\nvol.\n19,\nss.\n1275\u20131289,\nJul.\n2016,\ndoi:10.1007/s10677-016-9745-2.\n[6]\nS.\nO.\nHansson,\nTeknik\noch\netik.\nStockholm:\nKTH:s\nfilosofienhet,\n2008.\n[Online].\nTillg\u00e4nglig\nvid:\nhttps://search.ebscohost.com/login.aspx?direct=true&db=edslib&AN=edslib.11217130&site=\neds-live&scope=site&authtype=guest&custid=s3911979&groupid=main&profile=eds\n[7]\nM.\nAlfredsson,\noch\nE.\nJohansson\n\"Sj\u00e4lvk\u00f6rande\nbilar\ni\nurbana\nmilj\u00f6er\n-\nen\nstudie\nmed\navseende\np\u00e5\ntrafiks\u00e4kerhet,\"\nexamensuppsats,\nInstitutionen\nf\u00f6r\nTeknik\noch\nSamh\u00e4lle\n/\nV\u00e4g\noch\nTrafik,\nLunds\nuniversitet,\nLund,\nSverige,\n2019.\n[Online].\nTillg\u00e4nglig:\nhttps://lup.lub.lu.se/luur/download?func=downloadFile&recordOId=8982251&fileOId=898225\n7\n12\n(\n13\n)",
        "metadata": {
            "source": "pdf/22.pdf",
            "page": 11
        }
    },
    {
        "page_content": "[8]\nJ.Dobric,\n\"Ny\nAI-forskning\ng\u00f6r\nsj\u00e4lvk\u00f6rande\nbilar\ntill\nb\u00e4ttre\nf\u00f6rare,\"\n\u00d6rebro\nUniversitet,\n[Online],\nDec.\n2,\n20121.\nTillg\u00e4nglig:\nhttps://www.oru.se/nyheter/nyhetsarkiv/nyhetsarkiv-2021/ny-ai-forskning-gor-sjalvkorande-bil\nar-till-battre-forare/\n(h\u00e4mtad:\n2023-05-06).\n[9]\nZenseact,\n\"People\nat\nheart\n-\nZenseact,\"\n[Online].\nTillg\u00e4nglig:\nhttps://zenseact.com/people-at-heart/\n(h\u00e4mtad:\n2023-05-19).\n[10]\nIntervju\nmed\nMattias\nHenriksson,\nZenseact\nAB,\nden\n10\nmaj\n2023.\n[11]\nT.\nBengtsson,\n\"Vem\nansvarar\nf\u00f6r\nsj\u00e4lvk\u00f6rande\nbilar?,\"\nexamensuppsats,\nJuridiska\ninstitutionen,\nUppsala\nUniversitet,\nUppsala,\nSverige,\n2022.\n[Online].\nTillg\u00e4nglig:\nhttp://uu.diva-portal.org/smash/get/diva2:1726525/FULLTEXT01.pdf\n[12]\nGlobeNewswire,\n\"Global\nAutonomous\nVehicle\nMarket\nis\nExpected\nto\nReach\n$556.67\nBillion\nby\n2026,\"\n2019.\n[Online].\nTillg\u00e4nglig:\nhttps://www.globenewswire.com/news-release/2019/07/03/1877861/0/en/Global-Autonomou\ns-Vehicle-Market-is-Expected-to-Reach-556-67-Billion-by-2026.html\n(h\u00e4mtad:\n2023-05-20).\n[13]\nGeneral\nMotor,\n\"UMTRI\nStudy,\"\n[Online].\nTillg\u00e4nglig:\nhttps://www.gm.com/stories/umtri\n(h\u00e4mtad:\n2023-05-20).\n[14]\n\"2022\nTesla\nImpact\nReport\nHighlight,\"\nTesla,\nInc,\nAustin,\nTexas,\nUSA,\n2022.\n[Online].\nTillg\u00e4nglig:\nhttps://www.tesla.com/ns_videos/2022-tesla-impact-report-highlights.pdf\n,\nH\u00e4mtad:\n2023-05-20.\n13\n(\n13\n)",
        "metadata": {
            "source": "pdf/22.pdf",
            "page": 12
        }
    },
    {
        "page_content": "Physics-Informed Neural Networks for\nCharge Dynamics in Air\nMaster\u2019s thesis in Complex Adaptive Systems\n\u00c1rni Konr\u00e1\u00f0sson\nDEPARTMENT OF Electrical Egineering\nCHALMERS UNIVERSITY OF TECHNOLOGY\nGothenburg, Sweden 2022\nwww.chalmers.se",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 0
        }
    },
    {
        "page_content": "",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 1
        }
    },
    {
        "page_content": "Master\u2019s thesis 2022\nPhysics-Informed Neural Networks for\nCharge Dynamics in Air\n\u00c1rni Konr\u00e1\u00f0sson\nDepartment of Electrical Engineering\nDivision of High Voltage Engineering\nChalmers University of Technology\nGothenburg, Sweden 2022",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 2
        }
    },
    {
        "page_content": "Physics-Informed Neural Networks for Charge Dynamics in Air\n\u00c1rni Konr\u00e1\u00f0sson\n\u00a9\u00c1rni Konr\u00e1\u00f0sson, 2022.\nSupervisors: Olof Hjortstam, Hitachi Energy\nExaminer: Yuri Serdyuk, Department of Electrical Engineering\nMaster\u2019s Thesis 2022\nDepartment of Electrical Engineering\nDivision of High Voltage Engineering\nChalmers University of Technology\nSE-412 96 Gothenburg\nTelephone +46 31 772 1000\nTypeset in LATEX, template by Kyriaki Antoniadou-Plytaria\nPrinted by Chalmers Reproservice\nGothenburg, Sweden 2022\niv",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 3
        }
    },
    {
        "page_content": "Physics-Informed Neural Networks for Charge Dynamics in Air\n\u00c1rni Konr\u00e1\u00f0sson\nDepartment of Electrical Engineering\nChalmers University of Technology\nAbstract\nPhysics-Informed Neural Networks (PINNs) are neural networks trained to consider\nthe laws of physics outlined in Partial Di\ufb00erential Equations (PDEs). PINNs are\na new tool aimed at solving di\ufb03cult problems, such as systems arising from PDEs,\ncomplementing or replacing traditional methods that already exist. This thesis\nfocuses on applying PINNs to the drift-di\ufb00usion equation and the Poisson equation,\nwhich are fundamental PDEs that describe the physics of electrical discharges in air\nand other gases under high-voltage stress. The aim of the work is to predict the\nelectric \ufb01eld in an air gap between two electrodes and to study how ionic charges\ndrifting in the gap modify the electrostatic \ufb01eld distribution. In the \ufb01rst case,\nthe equations are considered by PINN independently. In the second case, they\nare coupled together to implement the e\ufb00ect of the space charge on the electric\n\ufb01eld. The results obtained in both cases are compared with respective analytical\nsolutions and/or solutions obtained from the Finite Element Method (FEM) using\nCOMSOL Multiphysics software. Details of the implementation of PINNs as well\nas encountered limitations are discussed. In particular, best practices when setting\nup problems and choosing hyperparameters are highlighted. The obtained results\nindicate that PINNs can, in some aspects, outperform FEM in accuracy although\nusing signi\ufb01cantly longer processing time. Further research is needed to con\ufb01rm\nthe feasibility of PINNs for electrical gas discharges, taking into account additional\ne\ufb00ects of more complex geometries and physics. Additionally, the usability of PINNs\nforsolvingtheinverseproblem(i.e.,extractingphysicalparametersde\ufb01ningthePDE\nfrom its solution) needs to be analyzed further.\nKeywords: Physics-Informed Neural Network, Arti\ufb01cial Neural Network, Partial\nDi\ufb00erential Equation, Discharge physics, Air discharges, DeepXDE\nv",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 4
        }
    },
    {
        "page_content": "",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 5
        }
    },
    {
        "page_content": "Acknowledgements\nThis thesis is written under the supervision of Hitachi ABB Power Grids, a world-\nleading supplier of grid infrastructure. In particualar it was carried out in collaba-\nration with the research center located in V\u00e4ster\u00e5s, Sweden.\nMy gratitude goes out to my family for having the patience to support me through\nthe project and providing me with immense moral support. I would also like to give\nspecial thanks to my supervisors and examiner, Olof Hjortstam, Christian H\u00e4ger\nand Yuriy Serdyuk, for providing helpful insights and discussions that have shaped\nmy work.\n\u00c1rni Konr\u00e1\u00f0sson, Gothenburg, June 2022\nvii",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 6
        }
    },
    {
        "page_content": "",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 7
        }
    },
    {
        "page_content": "Contents\nList of Figures xi\nList of Tables xiii\n1 Introduction 1\n1.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.2 Purpose . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.3 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.4 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2 Background 5\n2.1 Deep Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2 Physics-Informed Neural Network . . . . . . . . . . . . . . . . . . . . 6\n2.2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.2.2 DeepXDE Framework . . . . . . . . . . . . . . . . . . . . . . 7\n2.3 Finite Element Method . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n3 Methods 11\n3.1 Problem Description . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3.1.1 Laplace and Poisson\u2019s Equations . . . . . . . . . . . . . . . . . 11\n3.1.2 Drift-Di\ufb00usion Equation . . . . . . . . . . . . . . . . . . . . . 12\n3.1.3 Mono Polar Charge Transport Coupled with Poisson\u2019s equation 14\n3.2 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n3.2.1 Activation Function tanh and Scaling. . . . . . . . . . . . . . 15\n3.2.2 Collocation Points . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.2.3 Optimizers and Learning Rate . . . . . . . . . . . . . . . . . . 16\n3.2.4 Loss Weights and Hard Boundary Conditions . . . . . . . . . 17\n3.2.5 Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n4 Results 19\n4.1 Coaxial Laplace Equation . . . . . . . . . . . . . . . . . . . . . . . . 19\n4.2 Drift-Di\ufb00usion Equation . . . . . . . . . . . . . . . . . . . . . . . . . 20\n4.3 Mono Polar Charge Transport Coupled with Poisson\u2019s Equation . . . 23\n4.4 Summary of Best Practices . . . . . . . . . . . . . . . . . . . . . . . . 26\n5 Discussion 27\n5.1 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\nix",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 8
        }
    },
    {
        "page_content": "Contents\n5.1.1 Improve Current Work . . . . . . . . . . . . . . . . . . . . . . 28\n5.1.2 Use PINNs for Inverse Problem . . . . . . . . . . . . . . . . . 28\n6 Conclusion 29\nBibliography 31\nx",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 9
        }
    },
    {
        "page_content": "List of Figures\n2.1 Schematic graph of a simple feed forward neural network . . . . . . . 6\n2.2 Schematic representation of a PINN . . . . . . . . . . . . . . . . . . . 8\n3.1 Coaxial electrode setup . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n3.2 Schematic representation of PINN for 1D Laplace equation . . . . . 13\n3.3 Schematic of the PINN network for the drift-di\ufb00usion equation . . . . 14\n3.4 Schematic of PINN network for mono polar charge transport coupled\nwith Poisson equation . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n3.5 Schematic of network modi\ufb01cation for applying hard constraints . . . 18\n4.1 Comparison of model prediction of the potential and the true values\nfor the Laplace coaxial equation . . . . . . . . . . . . . . . . . . . . . 20\n4.2 Comparison of model prediction of the electrical \ufb01eld and the true\nvalues for Laplace coaxial equation . . . . . . . . . . . . . . . . . . . 21\n4.3 Comparison of the drift model with the solution found using Comsol\nwithout adding numerical stabilization. . . . . . . . . . . . . . . . . . 21\n4.4 Comparison of drift model with solution found using Comsol with\nnumerical stabilisation . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n4.5 Good result form a drift-di\ufb00usion PINN model compared with so-\nlution found using Comsol with numerical stabilization on 50 mesh\npoint grid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n4.6 Not so good result form a drift-di\ufb00usion PINN model compared with\nsolution found using Comsol with numerical stabilisation on 50 mesh\npoint grid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n4.7 Best result form a drift-di\ufb00usion PINN model compared with solution\nfound using Comsol with numerical stabilisation on 50 mesh point grid 24\n4.8 Coupled PDE PINN model with low injection compared with solution\nfound using Comsol with numerical stabilization on 50 mesh point grid 25\n4.9 Coupled PDE PINN model with the highest injection solved, 1e9\nions/m3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\nxi",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 10
        }
    },
    {
        "page_content": "List of Figures\nxii",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 11
        }
    },
    {
        "page_content": "List of Tables\n3.1 Parameters and relevant descriptions of the drift-di\ufb00usion equation\nfor ionic drift in air [2]. . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.2 Description of hyperparameters of PINNs . . . . . . . . . . . . . . . . 16\n4.1 Time and iterations di\ufb00erent boundary conditions and optimizer take\nuntil results have a L2 error of less than 1e\u22122forR0= 0.1m. . . . . 20\n4.2 Test loss, training time, and optimizer speci\ufb01cationsfor PINNs shown\nin Figures 4.3-4.7. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n4.3 Test loss, training time, and optimizer speci\ufb01cationsfor PINNs shown\nin Figures 4.8-4.9. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n4.4 Number of layers and neurons used for the best results for each prob-\nlem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\nxiii",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 12
        }
    },
    {
        "page_content": "List of Tables\nxiv",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 13
        }
    },
    {
        "page_content": "1\nIntroduction\n1.1 Background\nDue to the worldwide need for a strong reduction in greenhouse gases to suppress\nongoing climate changes, the electrical energy system is under great changes. An\nimportant part of the development of high voltage equipment needed for this chal-\nlengeistopredicttheinsulationperformanceofairaroundsuchequipment. Physical\nprocesses in air discharges are normally modeled utilizing hydrodynamic approaches\n[1]. One such model is the drift-di\ufb00usion equation for charge carriers coupled with\nPoisson\u2019s equation.\n\u2202ne\n\u2202t+\u2207\u00b7(neWe\u2212De\u2207ne) =Re\n\u2202np\n\u2202t+\u2207\u00b7(npWp\u2212Dp\u2207np) =Rp\n\u2202nn\n\u2202t+\u2207\u00b7(nnWn\u2212Dn\u2207nn) =Rn(1.1)\nHere, the subscripts e,nandpindicate electrons, positive ions, and negative ions.\nDstands for di\ufb00usion coe\ufb03cient. The \ufb01rst term represents the rate in change of\nthe number density, n, over time. The second and third terms are divergence of the\n\ufb02ux due to advection and di\ufb00usion of particles respectively. The source terms of\nEquation 1.1 incorporate rates of processes to be considered in the model:\nRe=\u03b1ne|We|\u2212\u03b7ne|We|\u2212\u03b2epnenp+R0+Rph\nRp=\u03b1ne|We|\u2212\u03b2epnenp\u2212\u03b2pnnpnn+R0+Rph\nRe=\u03b7ne|We|\u2212\u03b2pnnpnn\nRp+Re+Rn= 0(1.2)\nwhere\u03b2epand\u03b2npare electron-ion recombination coe\ufb03cients and ion-ion recombi-\nnation coe\ufb03cients, respectively, R0andRphare the rate of background and photo\nionization, respectively, \u03b1is the \ufb01rst ionization coe\ufb03cient and \u03b7is the attachment\ncoe\ufb03cient.\nEvery one of the source terms have strong non-linear \ufb01eld dependence as most of\nthe parameters in these drift-di\ufb00usion equation are dependant on the electric \ufb01eld\nE, in which case we couple them with the Poisson\u2019s equation\n\u2207\u00b7(\u2212\u03f50\u03f5r\u2207V) =e(np\u2212ne\u2212nn)\nE=\u2212\u2207V.(1.3)\n1",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 14
        }
    },
    {
        "page_content": "1. Introduction\nHere,\u03f50is the permittivity of vacuum and \u03f5ris the relative permittivity, Vis\nthe electric potential, and eis the electron charge. This is a complex transport\nand charge generation model with strong non-linear behaviors. Current solution\nmethods such as the Finite Element method (FME) have their shortcomings, the\ndrift-di\ufb00usion equation is in-stable, high speed charges need high resolution on the\ncomputational grid and small time steps which results in time and memory con-\nsuming calculations. The time scales of the problem are also vastly di\ufb00erent: parts\nof the discharge physics take place at short time scales, while other parts, such as\ndrift of ions, take a much longer time. Georgi Karman simpli\ufb01es Equations 1.1-1.3\nto one dimension in his thesis work [2], we will refer to his work for the choices of\nparameters in Chapter 3.\nIn recent years, researchers in machine learning have started to develop a method\nknown as physics-informed neural networks (PINNs). This approach was \ufb01rst pro-\nposed by Raissi et al.[4][5]\nPINNs are neural networks trained to solve supervised learning tasks while respect-\ningphysicallawsdescribedbygeneralnonlinearpartialdi\ufb00erentialequations(PDE).\nThere are two types of PINNs, the data-driven solution of PDEs and the data-driven\ndiscovery of PDEs. In the data-driven solution, also called the forward problem,\nneural networks are used to solve a given PDE, and in the data-driven discovery of\nPDEs, also called inverse problems, neural networks are used to \ufb01nd suitable pa-\nrameters for PDEs. Although the use of experimental data and PINN to \ufb01nd PDEs\nis a promising area of study, we will focus solely on the forward problem and leave\nthe inverse problem for future studies.\n1.2 Purpose\nThis thesis investigates charge transport in an air gap between two electrodes using\nPINNs. The PDEs used in the study are Poisson\u2019s equation and the drift-di\ufb00usion\nequation, and the long-term goal is to be able to predict electric breakdown in air\ninsulation. The Poisson equation is solved analytically and used for comparison,\nwhereas the drift-di\ufb00usion equation is compared to a solution found using FEM.\nThe research questions that this work aims to answer are as follows.\n1. How useful are PINNs for problems arising in discharge physics?\n2. What are the best practices when using PINNs to solve PDEs?\n3. What are the limitations and advantages of using PINNs compared to tradi-\ntional numerical solvers?\n1.3 Limitations\nUsing PINN to solve Equations 1.1-1.3 would be ideal, but its complexity is too\nhigh for the current thesis. We therefore have to accept some limitations. First, we\nonly look at simple geometry, one-dimensional interval in both coaxial and Cartesian\ncoordinates. Second, we have a simple physical system that does not account for ion\ngeneration, recombination, and more. Third, we make use of parameters to PDEs\n2",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 15
        }
    },
    {
        "page_content": "1. Introduction\nthat are found using approximations, simpli\ufb01cations, and results from experiments.\nThis will result in PDE that will never be completely accurate. Typical parameters\nfor air discharge are taken from Karman [2].\n1.4 Related Work\nThe work done in this study is largely based on the work by Raissi et al. [4][5]. Part\none of their paper focuses on showing the promise of using PINNs to solve PDEs\nfor both continuous and discrete time. The second paper [5] focuses on \ufb01nding the\nbest parameters of a PDE describing the observed data. The equations studied\nare Burger\u2019s equation, Schr\u00f6dinger\u2019s equation, Navier-Stokes equation, Kortewedge\nVries equation, and Allen-Cahn equation. Since the publication of the article by\nRaissietal., therehavebeenanumberofstudiesexploringtheapplicationofPINNs.\nIn particular Poisson\u2019s equation and di\ufb00usion equation in 1D in mechanics [15], the\nwave equation, KdV and KdV-Burger equations in 1D [13], and the swing equation\nfor both forward and inverse problem [14]. Other applications include the heat\ntransfer equation in 1D [16], coupled PDE [18], and many more.\nUsing PINNs to replace traditional solvers is already being studied. Markids [12],\nstudies the Poisson equation and how choosing di\ufb00erent hyperparameters a\ufb00ects\ntraining time and convergence. Many insightful thoughts and lessons can be taken\nfrom his paper.\n3",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 16
        }
    },
    {
        "page_content": "1. Introduction\n4",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 17
        }
    },
    {
        "page_content": "2\nBackground\n2.1 Deep Neural Networks\nA deep neural network is an arti\ufb01cial neural network with multiple layers between\nthe input and output layers. Many di\ufb00erent neural networks have been developed\nin the past decades including convolutional neural networks, recurrent neural net-\nworks (RNN), residual neural networks (ResNet) and feed forward neural networks\n(FFNN). We consider FFNN in this study. Using other networks could be of use in\nfurther research.\nLet us now give a short description of a deep neural network. We have NL(x)a\nL\u2212layered neural network from RdtoRDwheredis the input dimension and D\nthe output dimension. Layer ihasNineurons, so N0=dandNL=D. Now we\ndenote the weight matrix and bias vector in layer \u2113byW\u2113\u2208RN\u2113\u00d7N\u2113\u22121andb\u2113\u2208RN\u2113\nrespectively. Given a nonlinear activation function \u03c3applied elementwise we can\nde\ufb01ne a FFNN recursively with\ninput layer :N0(x) =x\u2208Rd\nhidden layers :N\u2113=\u03c3(W\u2113N\u2113\u22121+b\u2113)\u2208RN\u2113for1\u2264\u2113\u2264L\u22121\noutput layer :NL=WLNL\u22121+bL\u2208RD\nA schematic graph of a typical FFNN can be seen in Figure 2.1, xis the spatial\ninput,tis the time input, and \u02c6uis the output from the net.\nPINNs require computation of derivatives of the network outputs with respect to the\ninputs. This can be done in four di\ufb00erent ways [11], hand-coding analytical deriva-\ntive, numerical approximation (like \ufb01nite element method), symbolic di\ufb00erentiation\n(used in Mathematica, Maple and such) and the fourth is automatic di\ufb00erentiation\n(AD). Deep learning utilizes a specialized technique of AD called backpropagation\nfor evaluating the derivatives.\nDeep neural network represent a compositional function and therefore AD can apply\nthe chain rule repeatedly to compute the derivative. AD can be boiled down to two\nsteps, \ufb01rst a forward pass to calculate the value of all variables and then a backward\npass to compute the derivatives. A simple example of how AD works can be found\nin [10].\n5",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 18
        }
    },
    {
        "page_content": "2. Background\nFigure 2.1: Schematic graph of a simple feed forward neural network\n2.2 Physics-Informed Neural Network\nPINNs are introduced in this section starting with an overview and background.\nThe second part of the section focuses on DeepXDE, a framework for implementing\nPINNs.\n2.2.1 Overview\nDeep neural networks need a large amount of training data to obtain accurate re-\nsults. However when analyzing complex physical systems, the cost of acquiring data\nis prohibitive which forces us to draw conclusions and make decisions with only par-\ntial information. PINNs do not have this need for large amount of data, instead they\nmake use of prior knowledge, such as physical laws, empirically validated results or\nother domain expertise. This prior knowledge acts as a regularization agent that\nconstrains the space of available solutions to a more manageable size, resulting in\nless data needed for training.\nPINNs can be be used for two types of problems, the forward and the inverse prob-\nlem. The forward problem is a so-called data-driven solution of PDEs, focusing on\n\ufb01nding solutions to a predetermined PDE. The inverse problem is a so-called data-\ndriven discovery of PDEs, focusing on \ufb01nding parameters of PDEs that \ufb01t given\ndata. The work done in [4][5] is split into two types of models, continuous-time\nand discrete-time models. This study will only consider continuous-time forward\nproblems and leave inverse problems for future research.\nThe general idea for both data-driven solutions and data-driven discovery of PDEs\nis as follows. Let us consider a parameterized and nonlinear PDE of the general\nform\nut+N[u;\u03bb] = 0, x\u2208\u2126, t\u2208[0,T] (2.1)\nwhereu(t,x)denotes the latent (hidden) solution, N[\u00b7;\u03bb]is a nonlinear operator\n6",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 19
        }
    },
    {
        "page_content": "2. Background\nparameterized by a vector \u03bband\u2126is a subset of Rd. We use the notation ut,ux\nanduxxfor,udi\ufb00erentiated by time, x\u2212coordinate and twice by x\u2212coordinate,\nrespectively.\nWhen dealing with a forward problem, we assume that all \u03bbare known and constant.\nWe de\ufb01nef(t,x)as given by the left-hand side of Equation 2.1,\nf:=ut+N[u]. (2.2)\nAn approximate of u(t,x)is found with the use of a deep neural network. This\napproximation along with 2.2 result in a PINN f(t,x).\nThe shared parameters between the neural networks u(t,x)andf(t,x)can be\nlearned by minimizing the loss function. In particular, the loss given by summing\ntogether the mean squared loss from network uand network f\nMSE =MSEu+MSEf (2.3)\nwhere\nMSEu=1\nNuNu\u2211\ni=1(u(ti\nu,xi\nu)\u2212ui)2and MSE f=1\nNfNf\u2211\ni=1(f(ti\nf,xi\nf))2.(2.4)\nHere{ti\nu,xi\nu,ui}Nu\ni=1denote the initial and boundary training data on u(t,x)and\n{ti\nf,xi\nf}Nf\ni=1are the collocation points for f(t,x). The loss MSE ucorresponds to the\nboundary and initial conditions, while MSE fenforces the PDE structure at a \ufb01nite\nset of so-called collocation points chosen in the domain of interest. Figure 2.2 shows\na general PINN schematic. The neural network inputs are points in space ( x) and\ntime (t) at the initial and boundary conditions. The net outputs an approximation\n(\u02c6u) of the PDE solution uwhich is then used to calculate the gradients appearing\nin the PDE. The results are provided by the function f.\nThe inverse problem uses an idea similar to the forward problem keeping \u03bbfrom\nEquation 2.1 as a variable parameter. \u03bbthen becomes a parameter of the PINN\nf(t,x).\n2.2.2 DeepXDE Framework\nThere are a number of di\ufb00erent frameworks for creating machine learning models,\namong the most popular is the Tensor\ufb02ow framework created by Google. Tensor-\n\ufb02ow has many built-in modules and is useful for deep learning. Tensor\ufb02ow is not\nspeci\ufb01cally built for PINNs, so every function, class, or module would need to be\ncreated from scratch. That is where DeepXDE comes in.\nDeepXDE was designed by Lu et al. [10], and is built on Tensor\ufb02ow. It serves both\nas an educational tool and as a research tool to solve problems in computational\nscience and engineering. It can solve both forward problems given initial and bound-\nary conditions, as well as the inverse problem given speci\ufb01c data. It was designed\nso that the code written is short and comprehensive, resembling the mathematical\nformulations.\nSolving PDEs with DeepXDE is done in nine steps:\n7",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 20
        }
    },
    {
        "page_content": "2. Background\nFigure 2.2: Schematic representation of a PINN\n1. Specify the computational domain\n2. Specify the PDE\n3. Specify the boundary/initial condition\n4. Use a built in function to create training and testing points for the system\nde\ufb01ned by 1,2 and 3.\n5. Construct neural network\n6. De\ufb01ne a model\n7. Set hyperparameters and compile the model\n8. Train the network\n9. Predict the PDE solution\nWhen looking at each step more closely, we \ufb01rst look at the geometry. DeepXDE\nhas many built-in geometries, but we only use an interval and combine it with\na time interval when we have time-dependent PDEs. When specifying the PDE,\nwe use grammar from Tensor\ufb02ow. DeepXDE was developed in Tensor\ufb02ow 1 and\neven though it has the option to use other backends, like Tensor\ufb02ow 2, we found\nthat it worked best using Tensor\ufb02ow 1. DeepXDE has four common boundary\nconditions, Dirichlet, Neumann, Robin, and periodic. In this study we only have\nDirichlet boundary conditions. The fourth step is to combine the geometry, PDE,\nand boundary/initial conditions together, creating either time-dependent or time-\nindependent training data. We do this with a one line command where we specify\nhowmanytrainingpointswewantinthedomainandtheboundaries. Wealsospecify\nwhich distribution should be used to sample training points from the domain. If\nwe have an analytical solution to the PDE, can we also input that function, then\nwe can add a metric to our training that evaluates the PINN. De\ufb01ning a model is\n8",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 21
        }
    },
    {
        "page_content": "2. Background\na simple one-line command combining the data from step 4 and the network from\nstep 5. Then we choose hyperparameters, like optimizer, learning rate and more,\nsee Section 3.2. Then it is time to compile and train the model. After training we\nhave a model capable of predicting the desired PDE solution.\n2.3 Finite Element Method\nNumerical methods are an established way of solving PDEs. The performance of\nsuch methods is crucial when analytical solutions are either hard or impossible to\n\ufb01nd.\nThe \ufb01nite element method is a popular numerical method, especially when deal-\ning with complex geometry. To solve a problem, FEM divides a large system into\nsmaller, simpler parts called \ufb01nite elements. This is done by discretization in the\nspace dimensions which is implemented by construction of a mesh of the object,\nwhich has a \ufb01nite number of points. The method approximates the unknown func-\ntion over the domain. The simple equations that model these \ufb01nite elements are\nthen gathered together to make a larger system that models the entire problem.\nFEM then approximates a solution by minimizing an associated error function.[9]\nFEM will be used for validation of the equations solved by PINN in the cases where\nno analytical solution exists.\n9",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 22
        }
    },
    {
        "page_content": "2. Background\n10",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 23
        }
    },
    {
        "page_content": "3\nMethods\nThischapterintroducesthemethodsused. First, inSection3.1weprovideadetailed\ndescription of the problems we will examine. Then in Section 3.2 we go over what\nhyperparameters to consider.\n3.1 Problem Description\nThe target of the present work is to investigate some speci\ufb01c features of 1D formula-\ntions of the equation presented in 1.1-1.3: Namely, the coupling of Poisson equation\nand the drift-di\ufb00usion for a case with only one charge species and no source terms\nR. First, we look at each of them separately. Then combine them and show how\npredictions of the electric \ufb01eld can be made.\n3.1.1 Laplace and Poisson\u2019s Equations\nThe general Poisson\u2019s equation is given by\n\u22072U=\u2212\u03c1\n\u03f5(3.1)\nwhereUis the electric potential, \u03c1is the space-charge distribution and \u03f5is the\npermittivity. If we have no space charge present in the system, \u03c1will be zero, and\nEquation 3.1 will become Laplace\u2019s equation.\nThesolutionstoboththeLaplaceandPoissonequationsin1DCartesiancoordinates\nare quite simple, given that the charge distribution is not too complex. A more com-\nplex but still simple geometry that is relevant to study of high voltage applications is\nthe coaxial geometry that can be described by coaxial coordinates. Therefore, when\nlooking at Laplace\u2019s equation in one-dimensional coaxial coordinates, we model the\nelectric potential as a function of the radius according to\nd2U\ndr2+1\nrdU\ndr= 0 (3.2)\nLooking at Figure 3.1, we have a coaxial electrode setup with an inner electrode with\nsome radius R0and a \ufb01xed potential. Then we have an outer electrode, a grounded\ncage, with radius R1. Then we say that R0is the inner radius and R1the outer\nradius. For simplicity we want to model Equation 3.2 with boundary conditions\nU(r=R0) = 1V U (r=R1) = 0V (3.3)\n11",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 24
        }
    },
    {
        "page_content": "3. Methods\nFigure 3.1: Coaxial electrode setup\nTo study geometries relevant for air discharge experiments [2], the dimensions of\nR0= 1mmandR1= 0.5mare relevant to study. We have an analytical solution to\nEquation 3.2 with these boundary conditions.\nU(r) =lnr\u2212lnR1\nlnR0\u2212lnR1(3.4)\nThus we can compare with the PINN model. From a physical point of view, the\nelectrical \ufb01eld is a important parameter that can be calculated if the potential\ndistribution is known. We know the relation between the \ufb01eld and the potential\nexpressed in coaxial coordinates is,\nE=\u2212dU\ndr(3.5)\nand the analytical solution for the \ufb01eld is\nE=\u22121\nr1\nlnR0\u2212lnR1(3.6)\nFigure 3.2 shows the schematic representation of the PINN used to solve the one-\ndimensional coaxial Laplace equation.\n3.1.2 Drift-Di\ufb00usion Equation\nThe next problem we look at is mono polar charge transport in an external electric\n\ufb01eld in one dimension. A suitable PDE for this is the drift-di\ufb00usion equation found\nin Karman [2], we only look at one charge species and leave out the source term,\n\u2202n\n\u2202t=\u2212w\u2202n\n\u2202x+Diff\u22022n\n\u2202x2(3.7)\nTable 3.1 shows the description and values used for the parameters of Equation 3.7,\nall parameters are taken from Karman [2].\n12",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 25
        }
    },
    {
        "page_content": "3. Methods\nFigure 3.2: Schematic representation of PINN for 1D Laplace equation\nNow we have two electrodes separated by one meter air gap. We have a constant\nexternal electric \ufb01eld over the gap. We want to model what happens when we inject\nninjions into the left boundary, given a constant density n0before injection. This\ncan be modeled using the boundary conditions\nn(t,x= 0) =ninj, n (t= 0,x) =n0 (3.8)\nTable 3.1: Parameters and relevant descriptions of the drift-di\ufb00usion equation for\nionic drift in air [2].\nnion density (ions/m3)\nwspeed of charges (m/s)w=\u00b5nEext= 200\nDiffDi\ufb00usion coe\ufb03cient (m2/s)Diff=\u00b5nkbT\nq= 5.05e\u22126\n\u00b5nmobility of positive ions (m2/Vs)\u00b5n= 2e\u22124\nEextExternal electric \ufb01eld (V/m)Eext= 1e6\nkbBoltzmann\u2019s constant (J/K)kb= 1.38e\u221223\nTTemperature (K)T= 293\nqelementary electric charge (C)q= 1.602e\u221219\nFigure 3.3 shows the schematic representation of the PINN used to solve the one-\ndimensional time-dependent drift-di\ufb00usion equation.\n13",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 26
        }
    },
    {
        "page_content": "3. Methods\nFigure 3.3: Schematic of the PINN network for the drift-di\ufb00usion equation\n3.1.3 MonoPolarChargeTransportCoupledwithPoisson\u2019s\nequation\nNow we want to see how the electric \ufb01eld distribution changes over time in the air\ngap if ionic charges are drifting in the electric \ufb01eld. For this we couple Equation 3.7\nwith Poisson\u2019s equation in 1 dimensional Cartesian coordinates.\n\u2202n\n\u2202t=\u2212w\u2202n\n\u2202x+Diff\u22022n\n\u2202x2\n\u22022U\n\u2202x2=\u2212q\u00b7n\n\u03f50(3.9)\nHere the charge distribution nfeeds into Poisson\u2019s equation and the electrical \ufb01eld\ncan be calculated by, E=\u2212\u2202U\n\u2202x. At one electrode we have 1MVelectric potential\nand at the other zero. We start with a low injection of charges, with a charge density\nof1ion/m3at the injecting boundary, under so low a charge density, the equations\nare essentially uncoupled. We then increase the charge density, aiming to have a\nsolution for a model with injection of 5e13ions/m3.\nFigure 3.4 shows a schematic of the PINN for Equation 3.9.\n14",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 27
        }
    },
    {
        "page_content": "3. Methods\nFigure 3.4: Schematic of PINN network for mono polar charge transport coupled\nwith Poisson equation\n3.2 Hyperparameters\nWhen creating PINNs there are numerous so-called hyperparameters that need to\nbe considered. Table 3.2 lists them and gives a brief description.\n3.2.1 Activation Function tanh and Scaling.\nChoosing the activation function is an important step for its performance, di\ufb00erent\nactivation functions can provide considerably di\ufb00erent accuracy and convergence.\nIn fact, it has been shown theoretically [6] and con\ufb01rmed experimentally [12] that\nnon-smooth activation functions such as ReLU (Recti\ufb01ed Linear Unit) are not con-\nsistently convergent, so it is recommended to use smooth functions such as the\nhyperbolic tangent, tanh, or the sigmoid function. In this paper, we choose tanh as\nthe activation function. Tanh is de\ufb01ned by\ntanh(x) =2\n1 +e\u22122x\u22121 (3.10)\nand squeezes values into a range between -1 and 1. Because of this, it is important\nto rescale the input to the network and to scale the parameters of the PDE so that\nthe solution is also in this range. If we do not rescale, then training can become\ndi\ufb03cult, as the net output is of order 1 but the output of the PDE could be of order\n10 or even higher.\nWhen selecting which activation function to use, we tried training with every built-\nin function in DeepXDE. It soon became clear that non-smooth functions did not\n15",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 28
        }
    },
    {
        "page_content": "3. Methods\nTable 3.2: Description of hyperparameters of PINNs\nActivation function Addsnon-linearitytoneuralnetworks, tanhandsigmoid\nare most used in PINNs\nScaling Rescaling of PDE parameters and inputs to the network\nNetwork architecture Number of hidden layers and number of nodes in each\nlayer\nCollocation Points Points where the PDE residual function is evaluated at.\nOptimizer Algorithm used to optimize weights of neural network.\nMost commonly L-BFGS or Adam\nLearning rate Determines how large a step to take at each iteration\nwhile moving towards the minimum of the loss function.\nLoss weights How much each loss function should contribute to the\noverall loss\nHard boundary conditions Function applied to the output of the neural network\nthat ensures that one or more boundary conditions are\nmet\nLoss function The loss function in a neural network quali\ufb01es the dif-\nference between the expected outcome and the outcome\nfrom the model\nconverge as fast as the others and when comparing the smooth functions, tanh was\nalways either fastest to converge or the most accurate. We then decided tanh was a\ngood choice.\n3.2.2 Collocation Points\nCollocation points are the input points in the integration domain making up the\ntraining data set. In practice, a large number of collocation points result in larger\nvalues of the loss function as well as longer training time. The distribution of\nthe collocation points can also impact the loss. The three most used distributions\nare uniform, pseudo-random and Sobol. In a uniform distribution, the data set is\nuniformly spaced on the simulation domain. In a pseudo-random distribution, the\npoints are sampled randomly from the simulation domain. In the Sobol distribution,\nthe points are from the Sobol low-discrepancy sequence [20]. Sobol is the default\ndistribution when using DeepXDE. Selecting a good distribution is more relevant\nwhen we have higher dimension and larger domain. When we have a small domain\nand are in 1 dimension, the number of collocation points is more important. After\ntrying out di\ufb00erent distributions we decided to use Sobol for the Laplace equation\nand pseudo-random for the time dependent equations.\n3.2.3 Optimizers and Learning Rate\nMost widely used optimizer for training neural networks is the Adam optimizer [21].\nAdam is a variant of the stochastic gradient descent method that considers the es-\ntimates of the \ufb01rst and second order moments of the gradients i.e., the mean and\n16",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 29
        }
    },
    {
        "page_content": "3. Methods\nthe uncentered variance. The moments are inserted into the update formula for the\nnetwork parameters. The main advantages of Adam are that it works well with\nnoisy or sparse data and has fast convergence.\nAnother optimizer used in present thesis is Limited memory Broyden-Fletcher-\nGoldfarb-Shanno (L-BFGS) [22] optimizer, it is a quasi-Newton method that uses\nlimited memory and approximates the BFGS algorithm. The L-BFGS algorithm\nuses an estimate of the inverse of the Hessian matrix to minimize the second-order\nTaylor expansion of the loss function. Since the Hessian is found with the second\norder partial derivatives while the Adam optimizer uses the \ufb01rst order derivative,\nL-BFGS optimizer converges to a more accurate results.\n3.2.4 Loss Weights and Hard Boundary Conditions\nThe loss we have is a combination of two or more losses\nMSE =MSEu+MSEf (3.11)\nwhere MSE uis the loss from boundary and initial conditions and MSE fis the loss\nfrom the PDEs. Say we have two boundary conditions, boundary A and B, and two\nPDEs, call them C and D. Then the loss becomes\nMSE =MSEA+MSEB+MSEC+MSED (3.12)\nNow, if one of the losses is much larger than the others, we will have a problem train-\ning. S.Wang et al. [7] detail this problem and suggest an algorithm to deal with\nthis problem, but for simplicity this study does not implement it. Instead we assign\nweights to each loss and manually assign values so the losses have similar magnitude.\nAnother approach that mitigates this problem and while also simplifying the train-\ning is to use hard boundary constraints introduced by Lu et al. [8]. Contrary to\nthe normal way to enforce boundary conditions via loss functions, hard boundaries\nstrictly impose the boundary conditions by modifying the network architecture.\nOnly Dirichlet and period BC were considered by Lu and this study will only con-\nsider Dirichlet BC.\nDirichlet boundary condition for the solution uis given by\nu(x) =g(x),x\u2208\u0393D (3.13)\nwhere \u0393D\u2282\u2202\u2126is a subset of the boundary. To make the approximate solution\n\u02c6u(x;\u03b8u)satisfy this BC, they construct the solution.\n\u02c6u(x;\u03b8u) =g(x) +\u2113(x)N(x;\u03b8u) (3.14)\nHere,N(x;\u03b8u)is the network output and \u2113is a function satisfying\n{\n\u2113(x) = 0,x\u2208\u0393D\n\u2113(x)>0,x\u2208\u2126\u2212\u0393D(3.15)\nIf\u0393Dis a simple geometry we can choose \u2113(x)analytically. For example, if \u0393D=a,b\ni.e., the boundary of an interval from atob, we can choose \u2113(x)as(x\u2212a)(b\u2212x)\n17",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 30
        }
    },
    {
        "page_content": "3. Methods\nor(1\u2212ea\u2212x)(1\u2212ex\u2212b). For more complex domains it becomes di\ufb03cult to \ufb01nd \u2113(x)\nanalytically so it is possible to use a spline function to approximate \u2113(x)but this is\noutside of the scope of this study.\nNow if we impose hard constraints on one of the boundaries the corresponding loss\nwould be zero thus reducing the complexity of the overall loss minimization improv-\ning training time and accuracy. Schematic of the network architecture modi\ufb01cation\ncan bee seen in Figure 3.5.\nFigure 3.5: Schematic of network modi\ufb01cation for applying hard constraints\n3.2.5 Loss Function\nThe loss function in a neural network quanti\ufb01es the di\ufb00erence between the expected\noutcome and the outcome produced by the model. From the loss function, we can\nderive the gradients that are used to update the weights. The average over all losses\nconstitutes the cost. In this paper, we used only the mean squared error (MSE).\nOther possible loss functions include mean absolute error, logarithmic MSE, mean\nl2 relative error, softmax cross entropy error, and more.\n18",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 31
        }
    },
    {
        "page_content": "4\nResults\nThis chapter presents the results of various created models.\n4.1 Coaxial Laplace Equation\nThe \ufb01rst model we consider is the one dimensional coaxial Laplace equation. Like\nwe said in Section 3.1.1, a reasonable relevant target for inner and outer radii is\nR0= 0.001mandR1= 0.5m. The Laplace equation with these boundaries is highly\nnon-linear, we can increase the linearity by having a larger inner radius.\nThe theory on using neural network as function approximator states that any func-\ntion can be approximated with a neural network given su\ufb03ciently many nodes.\nHowever, larger neural networks take longer to train, and the same can be said for\nthe training set. Starting with a small network and few collocation points is there-\nfore bene\ufb01cial to keeping training time short.\nIt was found that it was much easier to solve Equation 3.2 for larger values of R0\ncompared to smaller values. To handle this, the following approach was used to be\nable to solve the equation for as small R0as possible.\n1. Set an initial R0\n2. Add hard constraints\n3. Solve for as low R0as possible for the current setup.\n4. Change Hyperparameter: collocation points, network architecture, Optimizer\nand learning rate, activation function.\n5. Solve for current R0, if successful go to Step 3 else go to Step 4\nIt is worth mentioning that all parameters and the inner and outer radii are of order\none so no scaling is required here.\nStarting with an inner radius of 0.1mwe set up a small net with two hidden layers,\neach with 10 nodes, tanh activation function, sample 30 points on the interval and\none on each boundary. Table 4.1 shows how many iterations and how long it takes\nusing either hard boundary conditions or soft boundary conditions. It is clear that\nusing hard boundary conditions speeds up the training time considerably especially\nwhen using the Adam optimizer.\nAfter many iterations of changing hyperparameters we where able to \ufb01nd a reliable\nsolution for 0.005minner radius. One setup that gives good results every time is a\nnet with 3 hidden layers with 40 nodes and tanh activation function. We sampled\n128pointsontheintervalandtwoontheboundarieswithhardboundaryconditions.\nThe model is trained for 5000 iterations using Adam with 0.001 learning rate and\nthen the L-BFGS optimizer. One realization of this setup can be seen in Figure 4.1\n19",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 32
        }
    },
    {
        "page_content": "4. Results\nTable 4.1: Time and iterations di\ufb00erent boundary conditions and optimizer take\nuntil results have a L2 error of less than 1e\u22122forR0= 0.1m.\nBoundary Condition Optimizer Nr iterations timeL2 error\nHard Adam - lr 0.001 1500 6s 8e\u22123\nSoft Adam - lr 0.001 6000-12000 9-30s 5\u22128e\u22123\nHard L-BFGS 150 7s 2.3e\u22125\nSoft L-BFGS 314 10s 1.8e\u22124\nand the corresponding electric \ufb01eld prediction can be seen in Figure 4.2.\nWhen the inner radius is lower than 0.005mwe encounter problems, mainly because\nthe training loss does not converge to a good solution consistently. Chapter 5 details\npossible reasons for this and possible solutions.\nFigure 4.1: Comparison of model prediction of the potential and the true values\nfor the Laplace coaxial equation\n4.2 Drift-Di\ufb00usion Equation\nWe mentioned in Section 3.1.2 that a realistic values for the drift speed wis 200\nm/s and the di\ufb00usion coe\ufb03cient Diffis5.05e\u22126. Like with the Laplace equation,\nit is bene\ufb01cial to look at a simpli\ufb01ed case, so we choose to ignore di\ufb00usion, i.e. set\nDiffto zero, and set the drift speed to be 1 m/s. The omission of di\ufb00usion is not\nan unreasonable simpli\ufb01cation because the di\ufb00usion is small relative to the system.\nWe \ufb01nd a good setup, using a net with 8 layers of 20 neurons, with 500 points in\nthe domain, 200 on the boundaries and 150 at the initial time. To see how well the\nPINN can model charge transport we used FEM with the help Comsol [19] using\nimplementations found in Karman [2]. We compare the solution we get from PINN\n20",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 33
        }
    },
    {
        "page_content": "4. Results\nFigure 4.2: Comparison of model prediction of the electrical \ufb01eld and the true\nvalues for Laplace coaxial equation\nto the solution found using Comsol with 200 mesh points both with and without\nnumerical stabilization.\nFigures 4.3 and 4.4 show a comparison of the PINN model with Comsol simulations\nfor 11 di\ufb00erent times; each drop from 1 to zero is a prediction for a di\ufb00erent time.\nFigure 4.3: Comparison of the drift model with the solution found using Comsol\nwithout adding numerical stabilization.\n21",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 34
        }
    },
    {
        "page_content": "4. Results\nFigure 4.4: Comparison of drift model with solution found using Comsol with\nnumerical stabilisation\nTable 4.2: Test loss, training time, and optimizer speci\ufb01cations for PINNs shown\nin Figures 4.3-4.7.\nFigure AdamlrL-BFGS TimeLoss\n4.3 and 4.4 --356 37s 4.2e\u22128\n4.5 --1210 269s 2.9e\u22126\n4.6 --911 11s 3.2e\u22123\n4.7 20000.01521 30s 7.5e\u22127\nNow we move on to the more realistic case with w= 200m/sandDiff= 5.05e\u22126.\nFirst thing to consider is scaling, since wis not of order one. After dividing Equation\n3.7 byw, we have all parameters of order one:\n1\nw\u2202n\n\u2202t+\u2202n\n\u2202x+Diff\nw\u22022n\n\u2202x2= 0 (4.1)\nIt is also worth noting that applying hard constraints to this case does not improve\ntraining and convergence as consistently as for the Laplace equation. The reason\nfor this could be the discontinuity on the left boundary.\nWhen using the same model setup as in the previous example, we can get good\nresults (Figure 4.5, takes 3.5-5min), however we can also get results that are not\nas good (Figure 4.6, takes 0-3min). This randomness comes from how the values\nin the weight matrix are initialized. We can achieve faster training time by \ufb01rst\ntraining with Adam (as recommended by Markids [12]). The results obtained from\nthis setup are still subject to the randomness of the initialization. It is di\ufb03cult\n\ufb01nding a con\ufb01guration that consistently outputs good results. Figure 4.7 shows\nbest results found for Equation 4.1, it took 30s with the same setup as before except\nit is trained for 2000 iterations with Adam with learning rate 0.01 before training\nwith L-BFGS.\n22",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 35
        }
    },
    {
        "page_content": "4. Results\nFigure 4.5: Good result form a drift-di\ufb00usion PINN model compared with solution\nfound using Comsol with numerical stabilization on 50 mesh point grid\nFigure 4.6: Not so good result form a drift-di\ufb00usion PINN model compared with\nsolution found using Comsol with numerical stabilisation on 50 mesh point grid\n4.3 Mono Polar Charge Transport Coupled with\nPoisson\u2019s Equation\nNext, wecouplethetwopreviousequations, Poissonandthedrift-di\ufb00usionequation,\ntogether. We use the knowledge that we have gained from the previous equations.\nIt works well to use hard boundary conditions for Poisson\u2019s equation but not for\nthe drift di\ufb00usion equation and we should use scaling when parameters are not of\norder one. It is also bene\ufb01cial to use Adam for a few thousand iterations before\nusing L-BFGS. As mentioned in Section 3.1.3, we \ufb01rst look at the case where we\nhave a low injection of charges. Using the setup that gave the best result for the\ndrift-di\ufb00usion equation we get good results, see Figure 4.8, Table 4.3 shows how\nlong training takes and how low training loss we can achieve.\nWhen increasing the concentration of charges injected we run into problems, as\n23",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 36
        }
    },
    {
        "page_content": "4. Results\nFigure 4.7: Best result form a drift-di\ufb00usion PINN model compared with solution\nfound using Comsol with numerical stabilisation on 50 mesh point grid\nTable 4.3: Test loss, training time, and optimizer speci\ufb01cations for PINNs shown\nin Figures 4.8-4.9.\nFigure Adamlr L-BFGS TimeLoss\n4.8 20000.01 850 96s 4.4e\u22126\n4.9 4000 1e\u22122,1e\u22123,5e\u22124,1e\u2212410482 312s 4.6e\u22124\nstated before it is better to have parameters and boundary conditions of order one,\nand when scaling the injection for the drift-di\ufb00usion equation we have to re-scale\nit up for when it enters in the Poisson equation so the e\ufb00ects are measurable in\nthe electric \ufb01eld. When computed using the same network setup as before we get\ndecent results in 2 minutes, we can improve our results by increasing the number of\nlayers and having more iterations with Adam, the improvement in the electric \ufb01eld\nprediction will however not be substantial and the increase in computation time is\nhigh. Figure 4.9 shows one realization when injection is 1e9ions/m3(highest we\nwhere able to solve for). Training time, optimizer speci\ufb01cation, and loss achieved\ncan be seen in Table 4.3.\nFigure 4.9 shows that we can detect changes in the electric \ufb01eld, although they are\nsmall. When increasing the injection further the loss becomes large and L-BFGS\ngets stuck in local minima and never getting close to the global minima.\n24",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 37
        }
    },
    {
        "page_content": "4. Results\nFigure 4.8: Coupled PDE PINN model with low injection compared with solution\nfound using Comsol with numerical stabilization on 50 mesh point grid\nFigure 4.9: Coupled PDE PINN model with the highest injection solved, 1e9\nions/m3.\n25",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 38
        }
    },
    {
        "page_content": "4. Results\n4.4 Summary of Best Practices\nFor all equations:\n\u2022Start with a simple problem.\n\u2022Have a smooth activation function. In our case tanh was used.\n\u2022Scale parameters such that they are of order one.\n\u2022Use hard boundaries or loss weights to ensure losses have similar magnitudes.\n\u2022Start training with adam and then use L-BFGS optimizer.\nIn the Laplace set-up we get better results increasing the number of neurons in each\nlayer than increasing the number of layers. However in the time dependent cases,\ndrift-di\ufb00usion and coupled equation, we get better results using deeper nets with\nfewer nodes in each layer. The network used for each problem can be seen in Table\n4.4\nTable 4.4: Number of layers and neurons used for the best results for each problem.\nProblem Layers Neurons\nLaplace best 3 40\nDrift-di\ufb00usion 8 20\nCoupled equations 8 20\n26",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 39
        }
    },
    {
        "page_content": "5\nDiscussion\nNow we have gone through one approach to solve coupled PDEs with PINN model\nand it is time to re\ufb02ect on what lessons we can learn. The coaxial Laplace equation\nshows that even for a simple example that has an analytical solution, the PINN\ncan run into problems if the system is highly nonlinear. When the inner radius is\nsmaller, the non-linearity of the system increases. This could be the reason for the\ndi\ufb03culties we have when set R0lower than 0.005m. Possible solutions that have\nnot been explored include changing the network architecture using another type of\nnetwork, such as ResNet. ResNet uses the input at every layer and we have the\ninput in the PDE so that could be bene\ufb01cial. Another possible solution could be to\nwrite Equation 3.2 di\ufb00erently. When ris small, 1/ris large and having large values\nin the PDE can prove troublesome in training as we saw in the coupled equation.\nTherefore, it could be bene\ufb01cial to multiply Equation 3.2 by r.\nThe drift-di\ufb00usion equation was more of a success, it shows that it is possible to get\nmore accurate results form a PINN model than from FEM. There are still improve-\nments to be made if PINNs are to be used. The PINN is currently much slower\nthan FEM, simulations that take a few seconds in Comsol take up to half a minute\nwith PINN. Then there is the problem of consistency. When the solution space\ngets more complex like when we introduce di\ufb00usion and increase the drift-speed\nthe initial weight matrix starts to have a bigger e\ufb00ect on weather we converge to a\ngood solution or not. To help mitigate this problem it might be useful use transfer\nlearning, i.e. train a net on a similar problem that is simpler and then use that as\nthe initial net for the more complex problem.\nWhen we moved on to the coupled equation it was bene\ufb01cial to have already looked\nat the equations separately. That allowed us to \ufb01nd out what worked well for\neach equation and implement it for the coupled equation. When all the parameters\nof the coupled PDE were of order 1, the PINN was able to \ufb01nd good solutions.\nHowever when some parameter gets large it makes the training slow and ine\ufb03cient.\nPossible solutions are to re-scale the whole system or possibly applying a logarithmic\ntransform to the PDEs making everything a lot smaller.\n5.1 Future Work\nWe have only just began answering the question, How useful are PINNs for problems\narising in discharge physics. Alot more research is needed to determine that. In this\nsection we will suggest how we think it is possible to build upon our work.\n27",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 40
        }
    },
    {
        "page_content": "5. Discussion\n5.1.1 Improve Current Work\nWe think that the next step would be to \ufb01nd a working solution to the mono polar\ncharge transport equation coupled with Poisson\u2019s equation for up to 5e13charges\ninjected. As mentioned earlier in the report several possible strategies for improved\nconvergence are suggested such as using: ResNets, transfer learning, RNN, more\nadvanced weights for the loss function, transformer network or another loss function\n1. Then adding more complexity to the equations by introducing ion-generation,\nion-ion recombination, electrons and possibly more. Having done that, we think it\nwould be a good idea to take the Poisson equation and solve it in 2D and 3D with\nincreasingly complex geometry. To better understand the advantages of PINN over\nFEM with respect to propagation of steep charge density gradients a systematic\ncomparison between FEM and PINN is needed to be done. In such as comparison\nPINN collocation points and FEM mesh and time steps needs to be well chosen to\nmake a fair comparison.\n5.1.2 Use PINNs for Inverse Problem\nWhen \ufb01nding the parameters of Equations 1.1 and 1.2 many assumptions and sim-\npli\ufb01cations are made, resulting in only approximates of the real parameters, there is\na need for methods that extract these physical parameters from experimental data.\nOne such method is using PINNs data-driven discovery of PDEs. The problem of\ndata-driven discovery of partial di\ufb00erential equations poses the following question,\ngiven a small set of scattered and potentially noisy observations of the hidden state\nof a system, what are the parameters that best describe the observed data? This\nmethod has been shown to give good results.\n1Suggested by thesis opponents Eric Jerg\u00e9us and Leo Karlson Oinonen.\n28",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 41
        }
    },
    {
        "page_content": "6\nConclusion\nThe results show that although PINNs are promising, there is still a need to develop\nhow to use them e\ufb00ectively. They can be useful as shown by good results in the\ndrift-di\ufb00usion equation.\nFrom the three case studies we learn that:\n\u2022Laplace equation in coaxial coordinates can be solved if the non-uniformity of\nthe solution is not too large.\n\u2022The drift-di\ufb00usion equation can be solved with a higher accuracy than for\nFEM. However, the solution time is much longer.\n\u2022Coupled Poisson a and drift-di\ufb00usion equation can be successfully solved by\nPINN if the equations are weakly coupled. Further development is needed to\nsolve these equations if they are strongly coupled.\nIt is a good practice to start with a simple problem and then gradually increase\nthe complexity. One needs to know about every hyperparameter and how changing\nthem a\ufb00ects training time and convergence.\nCurrent limitations of PINNs that we encountered are: Highly non-linear equations\nrusult in slow convergence or non-convergent models. Also when systems are com-\nplex, model accuracy, taining time, and convergence tend to be inconsistent, varying\na lot for every initialization of the weight matrix. We found no clear cut advantages\nusing PINNs rather than traditional numerical solvers. Further research into the\nuses of PINNs for discharge physics is needed.\n29",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 42
        }
    },
    {
        "page_content": "6. Conclusion\n30",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 43
        }
    },
    {
        "page_content": "Bibliography\n[1] S.Singh, \"Computational framework for studying charge transport in high-\nvoltage gas-insulated systems\", Doctor Thesis, 2017\n[2] G. Karman, \"Simulation and Analysis of Corona Currents in Large Scale Coax-\nial Geometry due to Triangular Voltages\", Masters Thesis, 2013\n[3] Hyuk Lee and In Seok Kang, \"Neural algorithm for solving di\ufb00erential equa-\ntions\", In: Journal of Computational Physics 91.1(1990) Pages 110-131\n[4] Maziar Raissi and Paris Perdikaris and George E. Karniadakis, \"Physics In-\nformed Deep Learning (Part I): Data-driven Discovery of Nonlinear Partial\nDi\ufb00erential Equations\"(2017), http://arxiv.org/abs/1711.10561\n[5] Maziar Raissi and Paris Perdikaris and George E. Karniadakis, \"Physics In-\nformed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial\nDi\ufb00erential Equations\"(2017), http://arxiv.org/abs/1711.10566\n[6] S. Mishra and R. Molinaro, \"Estimates on the generalization error of\nPhysics Informed Neural Networks (PINNs) for approximating PDEs\"(2020),\nhttps://arxiv.org/abs/2006.16144\n[7] S. Wang and X. Yu and P. Perdikaris, \"When and why PINNs fail to train: A\nneural tangent kernel perspective\"(2020), https://arxiv.org/abs/2007.14527\n[8] L. Lu and R. Pestourie and W. Yao and Z. Wang and F. Verdugo and S.G.\nJohnson, \"Physics-informed neural networks with hard constraints for inverse\ndesign\"(2021), https://arxiv.org/abs/2102.04626\n[9] Daryl L. Logan (2011). A \ufb01rst course in the \ufb01nite element method. Cengage\nLearning. ISBN 978-0495668251.\n[10] L. Lu and X. Meng and Z. Mao and G.E. Karniadakis, \"Deep-\nXDE: A Deep Learning Library for Solving Di\ufb00erential Equations\"(2021),\nhttps://doi.org/10.1137/19M1274067\n[11] A.G. Baydin and B.A. Pearlmutter and A.A Radul, \"Automatic di\ufb00erentiation\nin machine learning: a survey\"(2015), http://arxiv.org/abs/1502.05767\n[12] S. Markidis, \"The Old and the New: Can Physics-Informed Deep-Learning\nReplace Traditional Linear Solvers?\" (2021), https://arxiv.org/abs/2103.09655\n[13] Y. Guo and X. Cao and B. Liu and M. Gao, \"Solving Partial Di\ufb00er-\nential Equations Using Deep Learning and Physical Constraints\", (2020),\nhttps://www.mdpi.com/2076-3417/10/17/5917\n[14] G.S Misyris and A. Venzke and S. Chatzivasileiadis, \"Physics-Informed Neural\nNetworks for Power Systems\", (2019), https://arxiv.org/abs/1911.03737\n[15] Q. Zhang and Y. Chen and Z. Yang, \"Data driven solutions and\ndiscoveries in mechanics using physics-informed neural network\", (2020)\nhttp://cs229.stanford.edu/proj2020spr/report/Zhang ChenYang.pdf\n31",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 44
        }
    },
    {
        "page_content": "Bibliography\n[16] N. Zobeiry and K.D. Humfeld, \"A physics-informed machine\nlearning approach for solving heat transfer equation in ad-\nvanced manufacturing and engineering applications\", (2021)\nhttps://www.sciencedirect.com/science/article/pii/S0952197621000798\n[17] F. Bragone, \"Physics-informed machine learning in power transformer dynamic\nthermal modelling\", Master thesis, 2021\n[18] M. Barreau and J. Liu and K.H. Johansson, \"Learning-based State Reconstruc-\ntion for a Scalar Hyperbolic PDE under noisy Lagrangian Sensing\", (2021),\nhttps://proceedings.mlr.press/v144/barreau21a.html\n[19] Comsol Multiphysics software, https://comsol.com\n[20] https://en.wikipedia.org/wiki/Sobol sequence\n[21] D.P. Kingma and J. Ba, \"Adam: A Method for Stochastic Optimization\", 2014,\nhttps://arxiv.org/abs/1412.6980\n[22] D.C. Liu and J. Nocedal, \"On the limited memory BFGS method for\nlarge scale optimization.\", Mathematical Programming 45, 503\u2013528 (1989),\nhttps://doi.org/10.1007/BF01589116\n32",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 45
        }
    },
    {
        "page_content": "DEPARTMENT OF ELECTRICAL ENGINEERING\nCHALMERS UNIVERSITY OF TECHNOLOGY\nGothenburg, Sweden\nwww.chalmers.se",
        "metadata": {
            "source": "pdf/33.pdf",
            "page": 46
        }
    },
    {
        "page_content": "Teacherprofessional identities andtheirimpactsontranslanguaging\npedagogies inaSTEMEMIclassroom contextinChina:anexusanalysis\nDownloaded from:https://research.chalmers.se, 2024-02-22 10:35UTC\nCitationfortheoriginalpublished paper(version ofrecord):\nOu,W.,Gu,M.(2024).Teacherprofessional identities andtheirimpactsontranslanguaging\npedagogies inaSTEMEMI\nclassroom contextinChina:anexusanalysis.Language andEducation, 38(1):42-64.\nhttp://dx.doi.org/10.1080/09500782.2023.2244915\nN.B.Whencitingthiswork,citetheoriginalpublished paper.\nresearch.chalmers.se offersthepossibility ofretrieving research publications produced atChalmers University ofTechnology.\nItcoversallkindofresearch output:articles,dissertations, conference papers,reportsetc.since2004.\nresearch.chalmers.se isadministrated andmaintained byChalmers Library\n(articlestartsonnextpage)",
        "metadata": {
            "source": "pdf/64.pdf",
            "page": 0
        }
    },
    {
        "page_content": "Language and education\n2024, VoL. 38, no . 1, 42\u201364\nTeacher professional identities and their impacts on \ntranslanguaging pedagogies in a STEM EMI classroom \ncontext in China: a nexus analysis\nAmy Wanyu Oua and Michelle Mingyue Gub\nadepartment of c ommunication and Learning in Science, chalmers university of technology, g oteborg, \nSweden; bdepartment of english Language e ducation, the e ducation university of Hong Kong, Hong Kong, \nHong Kong\nABSTRACT\nWhile translanguaging has gained increasing recognition as a mul -\ntiliteracy pedagogy in English-medium instruction (EMI) education, \nresearch exploring its implementation in STEM classroom contexts \nremains limited. Furthermore, the interplay of EMI teachers\u2019 professional \nidentities and their instructional strategies has received little attention. \nThis qualitative study explores how STEM academics in an EMI pro -\ngramme in China implemented translanguaging pedagogy, developed \ntheir professional identities, and examined the impact of identity on \ntheir classroom instructional language use. Drawing upon nexus anal -\nysis, the study maps the intersecting discourses influencing two EMI \nlecturers\u2019 divergent language ideologies and translanguaging strate -\ngies. The findings highlight the role of teacher identity and agency in \nnavigating institutional and classroom discourses, facilitating planned \nand effective translanguaging pedagogy. The study reveals identity \nstruggles within the examined institution, where academic staff faced \na challenge in balancing their roles as effective EMI teachers and suc -\ncessful researchers due to a discourse of research meritocracy and were \nconstrained in exploring translanguaging pedagogy due to a discourse \nof internationalism. These challenges undermined their motivation to \ninvest in teaching identity and pedagogical skills. This study under -\nscores the need for a balanced view of research and teaching, more \nrobust teacher evaluation systems, and institutional support to foster \neffective translanguaging pedagogy in EMI by incorporating teacher \nidentity construction into EMI teacher preparedness.\nIntroduction\nEnglish medium instruction (EMI) has gained popularity in higher education world -\nwide, notably in science, technology, engineering, and mathematics (STEM) subjects \n(W\u00e4chter and Maiworm 2014 ; Macaro 2018 ); China is also part of this trend (Hu 2009 ; \nJablonkai and Hou 2021 ). In the context of STEM-EMI in China, students whose \n\u00a9 2023 the a uthor(s). Published by i nforma uK Limited, trading as taylor & Francis group.CONTACT amy Wanyu ou  amywa@chalmers.se   division for Language and communication, d epartment of \ncommunication and Learning in Science, chalmers university of technology H\u00f6rsalsv\u00e4gen 2, Se-412 96 g \u00f6teborg, Swedenhttps://doi.org/10.1080/09500782.2023.2244915\nthis is an o pen a ccess article distributed under the terms of the c reative c ommons a ttribution License ( http://creativecommons.org/\nlicenses/by/4.0/ ), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. \nthe terms on which this article has been published allow the posting of the a ccepted Manuscript in a repository by the author(s) or with their \nconsent.ARTICLE HISTORY\nReceived 7 December 2022\nAccepted 29 July 2023\nKEYWORDS\nEMI; nexus analysis; \nteacher identity; teacher \npreparedness; \ntranslanguaging \npedagogySPECIAL ISSUE PAPER",
        "metadata": {
            "source": "pdf/64.pdf",
            "page": 1
        }
    },
    {
        "page_content": "LAnGUAGE  AnD  EDUCATIOn 43\nfoundational disciplinary knowledge primarily resides in their L1 Chinese often face \nlanguage challenges when studying advanced academic concepts and topics in English \n(e.g. Jiang et\u00a0al. 2019 ). The students\u2019 insufficient (academic) English proficiency has \nbeen identified as a key factor contributing to the discrepancy between EMI goals \noutlined in policy documents and the actual practices in classrooms, where Chinese is \nfrequently utilised as a \u2018compromised\u2019 language practice (Hu and Lei 2014 ). Recent \nstudies have increasingly recognised that EMI across diverse contexts is seldom exclu -\nsively conducted in English but is instead multilingual in nature (Mazak and Carroll \n2016 ). Consequently, scholars advocate for a translanguaging perspective in EMI teach -\ning to better capture the fluidity of language use in EMI classes and to leverage the \nmultilingual resources and identities of teachers and students in the learning and teach -\ning process (Paulsrud et\u00a0al. 2021 ). Research on translanguaging pedagogies in EMI \naligns with the contemporary call in applied linguistics for \u2018researching multilingually\u2019 \n(Holmes et\u00a0al. 2013 ) in terms of an ecological view of linguistic diversity within the \nresearch context and the endeavours to normalise the teaching and learning practices \nbased on students\u2019 full linguistic repertoires.\nDespite the growing attention given to translanguaging in EMI literature, research \nexploring teachers\u2019 implementation and practice of translanguaging pedagogies in STEM \nclassrooms remains limited, particularly in China (with a few exceptions noted by Gu \nand Ou forthcoming; Yuan and Y ang 2023 ). Additionally, there is a dearth of studies \ninto the interplay of EMI teachers\u2019 professional identity and their use of translanguaging \npedagogical approaches. It is crucial to understand the role of teachers\u2019 professional \nidentity \u2013 how teachers perceive themselves professionally and enact their professional \nroles (Beijaard et\u00a0al. 2004 ) \u2013 in moulding their translanguaging practices for effective \nimplementation. EMI teachers often navigate multiple linguistic and cultural factors as \nthey engage with classroom teaching (e.g. Trent 2017 ; Xu and Ou 2022 ). Their profes -\nsional identities, influenced by their language expertise, disciplinary backgrounds, and \ninstitutional contexts (van Lankveld et\u00a0al. 2017 ), can significantly impact their beliefs, \ndecision-making, and teaching practices. Therefore, this study aims to explore how \nSTEM teachers\u2019 professional identity influenced their translanguaging pedagogical prac -\ntices in a Chinese EMI context.\nThe present study contributes to this special issue with a critical examination of the \nutilisation of multilingual and multimodal resources by EMI teachers in classroom teach -\ning. It explores the intricate relationship between various levels and dimensions of dis -\ncourses regarding EMI policies and practices, ranging from institutional decisions and \nvalues to individual teachers\u2019 beliefs and actions. Using nexus analysis (Scollon and \nScollon 2004 ; Hult 2017 ), an ethnographic discourse analytical approach, we collaborated \nwith bi/multilingual STEM teachers from the investigated institute and worked with \ndiverse types of multilingual data to uncover how teachers constructed their professional \nidentities in their situated context of EMI education and how these identities impacted \ntheir use of translanguaging practices in classrooms. This study provides insights into \nthe complexities of language, teacher identity, and pedagogical approaches in STEM-\nEMI contexts. It also has practical implications for EMI policymaking and teacher pro -\nfessional development, designing context-specific strategies to support EMI teachers \nand improve their instructional effectiveness in EMI teaching.",
        "metadata": {
            "source": "pdf/64.pdf",
            "page": 2
        }
    },
    {
        "page_content": "44 A. W. OU AnD M. M. GU\nLiterature review\nTeaching STEM in EMI and translanguaging pedagogy\nUsing English in STEM subjects has been increasingly popular worldwide (W\u00e4chter \nand Maiworm 2014 ). This trend is evident in China\u2019s disciplinary representation in \nEMI research, with STEM subjects accounting for 73% of studies (Jablonkai and Hou \n2021 ). Although STEM teachers generally exhibit positive attitudes towards EMI \n(Kuteeva and Airey 2014 ), research across contexts suggests significant challenges in \nimplementing effective teaching. The studies highlight language-related issues stem -\nming from the non-native English-speaking background of both lecturers and students \n(Doiz et\u00a0al. 2012 ; Macaro 2018 ). A recent study in Turkey shows that STEM students \nencountered specific academic English-related challenges concerning reading and writ -\ning across vocabulary, syntax and discourse levels, although to a lesser extent than \nstudents in social science disciplines (Kama\u015fak et\u00a0al. 2021 ). These challenges can make \nEMI learning particularly difficult for students whose prior disciplinary knowledge has \nbeen established in their L1, like Chinese students. As Jiang et\u00a0al. ( 2019 ) argued, lan -\nguage-related difficulty facing EMI teachers is \u2018particularly salient in mainland China\u2019 \n(p. 2), especially in EMI programmes targeting only domestic Chinese students. Similar \nfindings have emerged from recent research conducted in Europe and China (Hu and \nDuan 2019 ; Lasagabaster and Doiz 2023 ), indicating that EMI classes in STEM fields \nare no different from those in other disciplines, such as social science and humanities, \nin terms of the limited interactivity and simplified cognitive and linguistic teacher-stu -\ndent interaction.\nIn response to the language-related challenges, translanguaging, originally proposed \nto break down the boundary between languages in bilingual education (Garc\u00eda and Li \n2014 ), has been widely recommended as a possible pedagogical method in EMI. \nTranslanguaging pedagogy is broadly known as \u2018the instructional mobilization of stu -\ndents\u2019 full linguistic repertoire and the promotion of productive contact across lan -\nguages\u2019 (Cummins 2019 , p. 21). Research has distinguished spontaneous translanguaging  \npedagogies from planned translanguaging  pedagogies, with the former involving the \nundesigned use of teachers\u2019 and students\u2019 linguistic and semiotic repertoires for learn -\ning and the latter requiring \u2018systematic planning on the part of the teacher (and cur -\nriculum designers) and an intimate knowledge of the students\u2019 multilingual linguistic \nresources\u2019 (Lin 2020 , p. 6). In EMI contexts where most students share their L1, \nresearchers found that spontaneous translanguaging practice was an inherent part of \nthe fabric of EMI teaching (Mazak and Carroll 2016 ). Moreover, in a Chinese EMI \nsetting, Chen et\u00a0al. ( 2020 ) reported that STEM lecturers used planned translanguaging \npedagogy strategies for different teaching goals; partial word-by-word and meaning \ntranslation was used to introduce new concepts and formulae to enhance students\u2019 \ncomprehension of content knowledge, while inter-sentential Chinese-English switching \nwas used to develop affiliative bonds with students. Besides enhancing teaching quality, \ntranslanguaging has pedagogical value in facilitating classroom interactions, empow -\nering students to recognise their local languages and identities, and promoting the \nplurality of knowledge systems (e.g. Gu and Lee 2019 ; Li 2022 ; Ou et\u00a0al. 2022 ; Song \n2023 ). Given the numerous benefits of translanguaging pedagogy, STEM education ",
        "metadata": {
            "source": "pdf/64.pdf",
            "page": 3
        }
    },
    {
        "page_content": "LAnGUAGE  AnD  EDUCATIOn 45\nscholars have suggested EMI teachers actively and strategically utilise students\u2019 L1 in \nteaching and develop their professional knowledge in planned translanguaging peda -\ngogies (Rahman and Singh 2022 ).\nHowever, implementing translanguaging pedagogy in EMI classrooms can be com -\nplicated when teachers must navigate various linguistic and cultural factors. Research \nin Chinese EMI settings has highlighted teachers\u2019 divergent and discordant attitudes \ntowards translanguaging (Wang 2019 ). Furthermore, a discrepancy often exists between \nteachers\u2019 perceptions of translanguaging and their actual classroom language use. This \ncan be attributed to political, institutional, and ideological influences (Fang and Liu \n2020 ), a lack of guidance on translanguaging pedagogy (Yuan and Y ang 2023 ), teachers\u2019 \nlinguistic purism ideology (Chang 2019 ), the assumed risks of students\u2019 excessive use \nof L1 (Liu and Fang 2022 ), or combinations of or interactions between the above (Jia \net\u00a0al. 2023 ). These studies suggest that EMI teachers\u2019 translanguaging practice is situated \nin a complex discourse system and mediated by multiple factors beyond one\u2019s language \nproficiency.\nOur review indicates that an increasing number of studies have investigated the various \nstakeholders\u2019 language ideologies of translanguaging, identifying the beneficial impacts of \ntranslanguaging pedagogy on multilingual students\u2019 learning. Nonetheless, a pressing need \nremains for further research to comprehensively capture the intricacies of translanguaging \npedagogical practices in knowledge construction within EMI higher education classrooms \n(Gu et\u00a0al. 2022 ). Moreover, it is crucial to explore how teacher identities influence translan -\nguaging and how they are constructed and shaped in the process. Such research would shed \nlight on developing professional identities among multilingual STEM teachers as they \nengage in meaning-making and knowledge construction across L1 and L2 of their students. \nIt would also highlight teachers\u2019 agency in employing translanguaging pedagogies strate -\ngically and suggest the professional training/support they require for different phases of \nknowledge building in EMI.\nEMI teacher professional identity and translanguaging\nAccording to Beijaard et\u00a0al. ( 2004 ), a teacher\u2019s professional identity is \u2018an ongoing process \nof integration of the \u2018personal\u2019 and the \u2018professional\u2019 sides of becoming and being a \nteacher\u2019 (p. 113). Teachers\u2019 professional identity \u2013 i.e. how they perceive and enact their \nprofessional roles in their daily work \u2013 can influence their beliefs about teaching and \ndecision-making and their actual teaching practices (Beauchamp and Thomas 2009 ). \nTeachers\u2019 professional identity is discursively constructed and negotiated in local socio -\ncultural discourses, often presenting as a struggle  because teachers must make sense of \ndifferent, sometimes conflicting perspectives and expectations at work (Beijaard et\u00a0al. \n2004 ). The professional identities of university staff rarely stand alone but rather are \nbuilt on other identities (i.e. teacher, researcher, academic and intellectual identities) \nand shaped by multiple contextual factors, such as contact with students and colleagues, \nstaff development activities, and the larger institutional and sociocultural contexts of \nhigher education (van Lankveld et\u00a0al. 2017 ).\nIn EMI higher education, teachers\u2019 professional identities and their translanguaging \npractices in classrooms, albeit not focally examined, were frequently reported and made ",
        "metadata": {
            "source": "pdf/64.pdf",
            "page": 4
        }
    },
    {
        "page_content": "46 A. W. OU AnD M. M. GU\nrelevant to the teachers\u2019 language identities, i.e. being a multilingual, native, or non-native \nspeaker of English (e.g. Inbar-Lourie and Donitsa-Schmidt 2020 ). EMI lecturers from China \nhave been found to accentuate their non-native-English-speaker identities, ruling them -\nselves out of language expert roles; for them, translanguaging is a pragmatic strategy because \nthey prioritise subject-content knowledge construction over language in teaching (Jiang \net\u00a0al. 2019 ). Similar findings were reported by Block and Moncada-Comas ( 2022 ), who \npointed out that STEM lecturers\u2019 disciplinary identities \u2013 i.e. their deep feelings of attach -\nment to their respective academic disciplines \u2013 contributed to their reluctance to take \nresponsibility for addressing students\u2019 language development in EMI learning at a Spanish \nEMI university. Furthermore, Trent\u2019s ( 2017 ) study in Hong Kong indicated that some lec -\nturers\u2019 pragmatic use of translanguaging was shaped by a discourse of rationality  in EMI\u2019s \nwider sociocultural and institutional context. University assessment policies place a pre -\nmium on academic staff engaging in research-related activities that best serve their career \ndevelopment needs, such as promotion and contract renewal, and therefore advocate min -\nimising efforts to address students\u2019 language development needs. Although some studies \nhave portrayed EMI lecturers in China as reluctant to take act as language teachers and \nresorting to spontaneous translanguaging as a pragmatic approach (Dang et\u00a0al. 2023 ), other \nstudies have revealed that teachers activate their agency \u2013 i.e. their capacity to act and make \nteaching decisions based on their beliefs and identities (Biesta et\u00a0al. 2015 ) \u2013 to implement \ntranslanguaging as a planned pedagogical strategy, integrating students\u2019 L1 and L2 in class -\nroom teaching to facilitate students\u2019 content comprehension and language development \n(Xu and Ou 2022 ).\nIn sum, previous research has shown that EMI lecturers\u2019 classroom teaching is affected \nby how they position themselves in relation to different languages, their fields of speciali -\nsation, their other professional roles, and university policies and management. This requires \nclosely examining translanguaging pedagogy in its situated sociocultural contexts and \ninvestigating how individual, interpersonal, and institutional factors shape a teacher\u2019s lan -\nguage use in the classroom. This study focuses on exploring the connection between STEM \nteachers\u2019 professional identity and their translanguaging pedagogical practices in EMI class -\nroom settings in China by addressing the following research questions:\n1. How did the STEM lecturers develop their professional identities in EMI teaching?\n2. How did the lecturers\u2019 professional identities influence their use of translanguaging \npedagogical practices in classrooms?\nThe study\nA nexus analysis\nIn this study, we employed nexus analysis as a \u2018meta-methodology\u2019 (Hult 2017 ) to inte -\ngrate complementary data collection and analysis methods to examine the interaction \nof STEM teachers\u2019 professional identity and their translanguaging pedagogies in EMI \nclassrooms. At its core, nexus analysis is an ethnographic discourse analysis method in \nwhich discourse  is understood as \u2018different ways of thinking, acting, interacting, valuing, \nfeeling, believing, and using symbols, tools, and objects\u2019 (Gee 1999 , p. 13). It examines ",
        "metadata": {
            "source": "pdf/64.pdf",
            "page": 5
        }
    },
    {
        "page_content": "LAnGUAGE  AnD  EDUCATIOn 47\na social action \u2013 an individual\u2019s meaning-making behaviour in its social context (Scollon \nand Scollon 2004 ) \u2013 by addressing the connection between two levels of discourses: \u2018the \nmicro-analysis of unfolding moments of social interaction\u2019 and \u2018a much broader socio-po -\nlitical-cultural analysis of the relationships among social groups and power interests in \nthe society\u2019 (Scollon and Scollon 2004 , p. 8). In this study, EMI teachers\u2019 translanguaging \npedagogical practice is the focal social action that is studied through the following three \ninter-related types of discourses:\n1. Discourse in place  (DiP): The material and ideological phenomena that reflect the \ncirculating values and beliefs that exist at the moment of EMI classroom teaching \nand that influence the teacher\u2019s language and pedagogical practices (Hult 2017 );\n2. Interaction order  (IO): The immediate interpersonal and interactional cycle of dis -\ncourse that involves all \u2018possible social arrangements by which we form relationships\u2019 \n(Scollon and Scollon 2004 , p. 13), such as the shared norms co-constructed/negoti -\nated by EMI teachers and students for interaction and knowledge construction.\n3. Historical body  (HB): The embodiment of individuals\u2019 life history and experience \n(Hult 2017 ), especially language repertoires and beliefs and identities, which shape \nthe way EMI teachers perceive teaching and interact with students.\nInformed by nexus analysis ( Figure 1 ), our study examines the translanguaging practice \nin STEM-EMI classrooms enacted by individual teachers as the \u2018nexus of practice\u2019 (Scollon \nand Scollon 2004 , p. viii), shaped by the relationships and interactions between different \nEMI stakeholders\u2019 actions and beliefs (i.e. teachers, students, administrators, and policy -\nmakers). This method highlights teacher identity as an important component of a teacher\u2019s \nhistorical body (HB), which intertwines with the teacher\u2019s life experiences and other dis -\ncourses from the immediate and structural contexts of EMI teaching to impact the pattern \nand effect of the teacher\u2019s classroom practice.\nResearch site and data collection\nThe study is part of a larger linguistic ethnography project conducted in an interdis -\nciplinary STEM institute at a top-tier university in Southeast China. The institute1 \nprovides four undergraduate-level programmes, three master\u2019s programmes, and one \ndoctoral programme in mathematics, physics, chemistry, and biological sciences; all \nare taught in English. The institute enrols about 60 undergraduate students, 60 post -\ngraduate students, and 60 doctoral students every year. Most of the student body comes \nfrom China, though the postgraduate programmes are open to international students. \nThe faculty members ( n = 40) are mainly of Chinese ethnicity, but all obtained post -\ngraduate degrees overseas, primarily in English-speaking countries. The institute imple -\nments a flexible EMI policy, namely \u2018English-based bilingual teaching\u2019 in the policy \ntext, leaving space for faculty members to reinterpret and translate the policy into \nindividualised translanguaging pedagogical strategies (vide infra). Students and teach -\ners at the institute agree, however, that all textbooks, learning materials, and assess -\nments must be in English, while teachers have autonomy in choosing the language used \nin lectures and classroom interactions.",
        "metadata": {
            "source": "pdf/64.pdf",
            "page": 6
        }
    },
    {
        "page_content": "48 A. W. OU AnD M. M. GU\nData collection was conducted in the 2021 spring semester. Using purposive and snowball \nsampling, we recruited four focal teachers from different disciplines with different educa -\ntional backgrounds and teaching experiences. The study involved class observations, inter -\nviews with participants, and informal interviews with other teachers and researchers from \nthe institute to gain a comprehensive understanding of the institute\u2019s EMI policy, common \nvalues, and teaching practices. For this nexus analysis, we re-organised classroom obser -\nvations, field notes, policy texts, and teacher interviews to provide insights into the HB, IO, \nand DiP cycles of discourses of translanguaging pedagogy in EMI. To better illustrate the \nintersection of multi-dimensional discourses, especially the impacts from the HB and IO \ncycles, we focused on the experiences of two teachers \u2013 Y e and Ming (pseudonyms) \u2013 who \nshared sufficient similarities in their backgrounds ( Table 1 ): both were Chinese-English \nbilinguals who spoke English as a second language and identified as proficient users of \nEnglish in their professional contexts. In addition, both were assistant professors with the \nsame required duties regarding their research engagement and teaching activities (two \nundergraduate-level courses per semester) at the time of the data collection.\nFigure 1.  the analytical framework.",
        "metadata": {
            "source": "pdf/64.pdf",
            "page": 7
        }
    },
    {
        "page_content": "LAnGUAGE  AnD  EDUCATIOn 49\nData analysis\nData analysis was informed by the collected ethnographic observation and interview data \nand the nexus analysis framework. Specifically, to examine the dominated DiPs and how \nthey were made relevant by teachers in EMI classrooms, we drew on incorporated critical \ndiscourse analysis (Johnson 2011 ) of the institute\u2019s language policy texts (in both English \nand Chinese, with two pages in each language) and thematic analysis of interviews, includ -\ning formal semi-structured interviews with the two focal teachers and formal or informal \ninterviews with other teachers (documented in field notes). The analysis identified the \ndominant ideologies and assumptions embedded in the institute\u2019s EMI policy texts. \nMoreover, we examined the recontextualisation process of policy from policy texts to \nlocal classroom activities, which \u2018transforms the meaning of a text by either expanding \nupon or adding to the meaning potential or, perhaps, suppressing and filtering particular \nmeanings\u2019 (Johnson 2011 , p. 270).\nInteractional analysis (Scollon and Scollon 2004 ) of class observations provided insights \ninto the IO cycle. Amy observed two continuous classes (90 min) taught by each focal teacher \nduring the regular instructional period in the middle of the semester. Y e was observed in \nhis material characterisation course (class size = 9), and Ming was observed in her analytical \nchemistry course (class size = 38). All classes were recorded with a video camera; field notes \nwere taken. To understand the IO, we coded the teachers\u2019 verbal and non-verbal behaviours \nand interaction patterns with students. Special attention was paid to instances when shared \nnorms of interpretation occurred between the teacher and students to guide their language \nuse and methods of knowledge construction.\nAfter the observations, both teachers received a semi-structured interview, each lasting \nabout one hour, wherein they elaborated on the specific language and teaching methods \nthey employed in the observed classes, discussed their experiences and perceptions of \nEMI teaching, the teaching effects, and how they positioned themselves to different pro -\nfessional roles, i.e. teaching, research, and administration. Both interviews were conducted \nin Mandarin Chinese \u2013 the teachers\u2019 first language \u2013 and were audio-recorded and then \ntranscribed by an independent researcher. The examination of the HB cycle was mainly \nfacilitated by thematic analysis of the interviews, by which we identified the recurring \npatterns related to the teachers\u2019 language ideologies and professional identities, such as \nbeliefs about language and content teaching, teaching and research experiences, and \ninteractions with students.Table 1.  the focal eMi teacher participants.\nteacher genderSelf-reported language \nability disciplineeducation \nbackgroundteaching \nexperience (year \nand place) courses\nYe Male Mandarin chinese (L1)\nenglish (working \nproficiency)Physics undergraduate \n(china)\nPhd (Hong Kong)6 (china) Material \ncharacterisation; \nSemiconductor \nphysics \n(undergraduate \nlevel)\nMing Female Mandarin chinese (L1)\nenglish (working \nproficiency)chemistry undergraduate \n(china)\nPhd (the uK)3 (china) chemistry \n(undergraduate \nlevel)",
        "metadata": {
            "source": "pdf/64.pdf",
            "page": 8
        }
    },
    {
        "page_content": "50 A. W. OU AnD M. M. GU\nOnce the three discourse cycles were identified and analysed, we mapped the three-\ntier discursive relationships ( Figure 1 ). During this phase, we focused on exploring \nthe formation and utilisation of pertinent discourses across various scales (i.e. insti -\ntutional, interpersonal, and individual) of EMI education. We examined how individual \nteachers employed these discourses in their classroom teaching and analysed how their \ninterplay influenced the construction of teachers\u2019 professional identities and mean -\ning-making processes as they engaged in pedagogical translanguaging practices in \nclassroom teaching.\nFindings\nThe discourses of internationalism and research meritocracy\nFirst, our analysis identified two prevailing DiPs within the institute: \u2018the discourse of \ninternationalism\u2019 and \u2018the discourse of research meritocracy\u2019 . These DiPs were frequently \nevoked by the teachers, albeit to varying degrees, to inform their linguistic choices in class -\nroom interactions and pedagogical approaches towards EMI teaching.\nThe discourse of internationalism addresses how the role of English in EMI was con -\nceptualised in the institute\u2019s language policy and by its teaching staff. The analysis shows a \nconsistent understanding of the value of English manifested in the policy text and among \nthe lecturers. That is, English has been promoted in terms of the rhetoric of internationalism, \na symbol of the university\u2019s internationalisation. For example, the institute\u2019s official web -\npage notes:\nThe institute adopts an English-based bilingual teaching mode and provides a large number \nof international exchange opportunities for students\u2026. 60% of 2019 undergraduates went to \noverseas universities to continue their studies, \u2026. Many students were accepted by Oxford \nUniversity, UC Berkeley, Columbia University, University of Pennsylvania, Brown University, \nJohns Hopkins University, University College London and other world-renowned schools.\nThis statement presents the institute\u2019s medium-of-instruction policy \u2013 \u2018English-based \nbilingual teaching\u2019 \u2013 alongside international student exchange programmes, another com -\nmon strategy for the internationalisation of universities. It establishes a direct connection \nbetween the effect of EMI and the institute\u2019s success in bolstering international student \nmobility, using the acceptance rate of students into world-renowned postgraduate pro -\ngrammes in English-speaking countries as an evaluative measure. This indicates the insti -\ntute\u2019s explicit reliance on the ideology of internationalism as a foundation for promoting \ntheir educational practices, emphasising the \u2018extrinsic value\u2019 (Kaplan 2001 ) of English as a \nsource of international education while downplaying the intrinsic value of English as a tool \nof knowledge construction. The faculty members also acknowledged this DiP of interna -\ntionalism; Y e and Jerry (a microbiology lecturer and researcher) highlighted the role of the \nEMI policy in serving the institute\u2019s internationalisation mission, despite the potential chal -\nlenges faced by students in acquiring content knowledge in English at their current stage \nof proficiency:\nJerry: I was told that the university sought so-called \u2018internationalisation\u2019 . So obviously, \nEnglish teaching is quite important. However, if they realise that the students\u2019 language pro -\nficiency is not good, they should provide more English courses.",
        "metadata": {
            "source": "pdf/64.pdf",
            "page": 9
        }
    },
    {
        "page_content": "LAnGUAGE  AnD  EDUCATIOn 51\nY e: Teaching in Chinese is definitely more effective for students. After all, their ways of \nthinking and knowledge foundations were all developed in Chinese. However, the pur -\npose of using English to teach is, we hope, to benefit them in their future studies and \nresearch abroad.\nLike other EMI contexts (e.g. Lasagabaster and Doiz 2021 ), these teachers raised concerns \nregarding the English proficiency of undergraduate students\u2019 English competence when it \ncame to mastering disciplinary knowledge in English. However, the institute did not seem \nto provide students with academic English language support. As Y e articulated, this situation \ncould potentially influence how EMI teachers navigate different languages within their \nclassrooms: teachers might use English in their instructions to improve students\u2019 English \nlanguage proficiency through EMI lessons and increase their success in pursuing educa -\ntion abroad.\nAnother DiP that greatly impacts how teachers develop pedagogical strategies and reg -\nulate their language use in classrooms is the discourse of research meritocracy , referring to \ninstitutional values prioritising research over teaching. The teachers in this study believed \nthat the university\u2019s staff assessment system placed more value on research outputs than \nteaching. Jerry commented that the university sends mixed messages about staff perfor -\nmance evaluation:\nJerry: Even though they claim they will evaluate a particular person on teaching, research, and \nadministration. In reality, only on research. If they want to get rid of you, they will pay atten -\ntion to your teaching.\nA teacher\u2019s words do not fully reflect reality; in many university contexts, faculty mem -\nbers are evaluated on multiple criteria, and their professional identity construction is subject \nto the tensions between their multiple duties (i.e. teaching, research and management) \n(Billot 2010 ). However, Jerry\u2019s opinion indicates that the university staff evaluation system\u2019s \nperceived hierarchy of faculty members\u2019 various activities calls into question an academic\u2019s \nidentity as a teacher. When the EMI institution\u2019s policy and management favour research \nmeritocracy and undervalue teaching, faculty members with both teaching and research \ncommitments may face challenges in allocating sufficient time and effort to develop their \nteacher identity, improve pedagogical skills and care for their students\u2019 learning needs. As \nelaborated below, this discourse is the main source of tension in the focal teachers\u2019 profes -\nsional identity development. How individual teachers position themselves in this discourse \nbecomes decisive for their translanguaging pedagogical strategies, relationships with stu -\ndents in class, and teaching effectiveness.\nYe: \u2018research has taken up a lot of time\u2019\nY e translanguaged in his classes by using English as the primary language for communicating \nwith students and Chinese for translating the instructed technical terminologies. In his \nembodied teaching, Y e actively engaged with multimodal resources in the classroom, such \nas diagrams, charts, drawing, circling and gestures in teaching. The following excerpt \ndemonstrates a typical moment of Y e\u2019s lecturing:",
        "metadata": {
            "source": "pdf/64.pdf",
            "page": 10
        }
    },
    {
        "page_content": "52 A. W. OU AnD M. M. GU\nExcerpt 1\nYe: So what we can do is (.)2 from the \nexperiment (.) we found out (.) one \ncri-rium. So this is called (.) the Rayleigh \ncri-rium (.) \u745e\u9e97\u5224\u8cac . So this Rayleigh \ncri-rium is to (.) determine the resolution \n(.) of the optical microscope. So this is the \ngenerally accepted (.) criterion.\nWhen we have the diffraction patterns (.) \nSo the first diffraction minimum (.) the \nfirst diffraction minimum (.) if we have \ntwo ends (.)\nthe first diffraction minimum (.) of the \nfirst (.) point.\nWhile coincide with a maximum of the \nsecond point.\nSo this is the exact limit (.) situation of (.) \nhow to separate the disk.\nIn the above extract, Y e elucidated the concept of Rayleigh Criteria to the students. Upon \nintroducing the term, he promptly provided its Chinese translation, then elaborated on the \nknowledge point exclusively in English, employing a deliberate pace and incorporating \nfrequent pauses within each sentence to facilitate students\u2019 comprehension. It is noteworthy \nthat Y e\u2019s spoken English during the instruction was not flawless; he frequently utilised \nsimplified and ungrammatical sentence structures and consistently mispronounced \u2018criteria\u2019 ",
        "metadata": {
            "source": "pdf/64.pdf",
            "page": 11
        }
    },
    {
        "page_content": "LAnGUAGE  AnD  EDUCATIOn 53\nas \u2018cri-rium\u2019 until he redirected his attention towards the whiteboard, where the word was \ndisplayed, and encircled it. Nevertheless, his seamless utilisation of body movement in \ninstruction, such as gestures and interaction with the projected slides on the whiteboard, \nhelped students visualise his thoughts. Through translanguaging practice, Y e made his \nteaching an embodied activity of largely meaningful instruction.\nHowever, as demonstrated above, Y e\u2019s translanguaging emerged unintentionally, spon -\ntaneously, and seamlessly during his lectures; his use of the students\u2019 L1 Chinese was min -\nimal and solely employed for offering translations of certain thematic concepts. Remarkably, \nY e remained oblivious to his employment of translanguaging as a valid teaching strategy \nin EMI contexts. When asked about his language use for classroom teaching, he responded \nparadoxically:\nY e: English only. But if there are English words they don\u2019t understand, I just explain the mean -\ning in Chinese. \u201d\nY e\u2019s perspective highlights an interesting dynamic in his language ideology: while \nacknowledging the educational significance of Chinese, his remained rooted in an English \nmonolingualism ideology regarding EMI teaching, reinforced by his perception of the insti -\ntute\u2019s EMI policy:\nY e: When our institute was initially set up, it expected us to deliver instruction solely in \nEnglish. However, considering that not all students feel at ease with English-only instruction, \nwe now allow English-Chinese bilingual teaching. In practice, it\u2019s up to individual teachers to \ndecide whether to use English only or use Chinese.\nTo Y e, the institute\u2019s vision for EMI education aligned with an English-only approach, \nand translanguaging was considered as a comprising pedagogical practice taken due to \nstudents\u2019 insufficient English proficiency. By implementing his classroom teaching in an \nEnglish-dominant manner, Y e embodied his teacher identity (HB) by adhering to an English \nmonolingualism ideology in EMI teaching, aligning himself with the discourse of interna -\ntionalism within the institute. This instructional strategy appears to prioritise the English \nlanguage over the students\u2019 learning of subject content knowledge. This is reflected in Y e\u2019s \nclassroom\u2019s interaction order (IO), as exemplified in Excerpt 1, where a teacher-centred \nand monologic teaching style predominated, allowing limited opportunities for teacher-stu -\ndent and peer interaction.\nIn the interview, Y e acknowledged and reflected on his monologic lecturing and yet-to-\nimprove teaching effect:\nResearcher: How do you evaluate your own EMI teaching ability?\nY e: Well, after all, English is not my mother tongue, so I need to practice more. But most of \nthe time it\u2019s fine, maybe because the science discipline does not require that much English \nproficiency from teachers.\u2026 This group of students are not very interactive in class, they \nusually didn\u2019t initiate questions when I was teaching.\u2026 The curriculum is heavy as well. So \nnow my class leaves no time for group discussion or student presentation. It is basically only \nlecturing.\nResearcher: Then how about the students\u2019 learning effect?",
        "metadata": {
            "source": "pdf/64.pdf",
            "page": 12
        }
    },
    {
        "page_content": "54 A. W. OU AnD M. M. GU\nY e: It\u2019s so-so, not very good. The students\u2019 learning abilities and motivations vary a lot\u2026. Also, \nthe learning materials are only in English. So some students are doing well, and some find it \ndifficult to cope with learning.\nWith an untested belief that EMI teaching in physics demands less English proficiency, \nY e considered himself linguistically competent for EMI instruction, despite English not \nbeing his first language. However, he acknowledged several shortcomings in his teaching \napproach, including a teacher-centred dynamic, limited interaction, suboptimal learning \noutcomes, and students encountering difficulties following his teaching. In the subsequent \ninterview, Y e acknowledged receiving limited recognition for his pedagogical methods, as \nreflected in his average score (40 \u2212 60% among all the teaching staff) in student evaluations \nof his teaching. When questioned about his efforts to enhance his teaching, Y e responded:\nY e: Of course, I hope I can receive a better evaluation from students and teach better, yet I have \nnot found any good method. Also, research has taken up a lot of time.\nResearcher: How do you allocate your time for research and teaching?\nY e: Taking this semester as an example, I use yesterday afternoon, this afternoon, and nights \nfor teaching, preparing for lectures, delivering lectures, and examining homework. Usually, \nteaching takes two days a week, and the rest five days of the week are all for research.\nThe above extract reveals a conflict between the multiple professional roles and identities \u2013 as \nan EMI lecturer and a researcher \u2013 that comprise Y e\u2019s historical body (HB). Undoubtedly, Ye \nrecognised that improving his pedagogical methods and teaching effects could benefit his \nprofessional identity. However, this possibility was undermined by the university\u2019s prevailing \nDiP of research meritocracy, which emphasises research performance over teaching. This \nalso evoked a \u2018discourse of rationality\u2019 (Trent 2017 ) \u2013 a rewards-driven and results-oriented \nvalue to university work and a pragmatic approach to the implementation of EMI teaching \n\u2013 that shaped Y e\u2019s professional identity, driving him to devote most of his time and effort \nto pursuing research outputs at the cost of abandoning his desire to enhance his EMI teacher \nidentity.\nThe DiP of research meritocracy and Y e\u2019s strong researcher identity also contributed to \nhis English-dominated language use in the classroom:\nY e: My course contents are all related to my research, so the process of updating teaching \nmaterials is also a self-study process for me. Furthermore, EMI teaching allows me to practice \nmy spoken English. This benefits my academic English writing, and my ability to make \nEnglish presentations at academic conferences.\nThe discourse of research meritocracy filled Y e\u2019s professional identity with a realistic and \npragmatic view of EMI teaching, in which he found EMI was a space for his own academic \nEnglish language development. This resulted in a more monolingual English approach to \nimplementing EMI teaching, ultimately serving his research interests.\nIn summary, Y e employed a spontaneous translanguaging pedagogy characterised by \nan organic blend of English, Chinese, and multimodal resources, with English playing a \ndominant role. As depicted in Figure 2 , Y e\u2019s pedagogical approach was shaped by the \nintersection of various discourses operating at multiple levels within the examined EMI ",
        "metadata": {
            "source": "pdf/64.pdf",
            "page": 13
        }
    },
    {
        "page_content": "LAnGUAGE  AnD  EDUCATIOn 55\ninstitute, particularly in terms of how Y e constructed his professional identities in relation \nto the institution\u2019s teaching and research values. Specifically, the influence of the English-\nmonolingualism ideology, driven by the discourse of internationalism, and the prominence \nof the researcher identity, prompted by the discourse of research meritocracy that empha -\nsizes research performance in faculty evaluation, played significant roles in shaping Y e\u2019s \ndecision to prioritise English language in the classroom. This decision was pragmatic and \nunadvised, leading to a teacher-centred, monologic, and non-interactive instructional \nenvironment that impeded students\u2019 learning effectiveness.\nMing: \u2018communicating with students is my major source of the sense of \nachievement as a teacher\u2019\nMing developed a different classroom language strategy despite having a language and \neducation background similar to Y e\u2019s. In Ming\u2019s class, Chinese was a verbal resource she \nand her students frequently used. Ming proactively initiated teacher-student interactions, \nand students tended to adopt the language in which she asked questions when responding. \nThe following dialogue exemplifies how knowledge was co-constructed between the teacher \nand students through dialogic translanguaging (Nikula et\u00a0al. 2013 ):\nFigure 2.  discourse nexus of Ye\u2019s translanguaging pedagogy.",
        "metadata": {
            "source": "pdf/64.pdf",
            "page": 14
        }
    },
    {
        "page_content": "56 A. W. OU AnD M. M. GU\nExcerpt 2\n1 Ming: Precision \u9084\u662f accuracy?\n2 Students: Precision.\n3 Ming: Precision. \u5c0d\u5427\uff1f \u662f\u5224\u65b7\u9019\u500b\u6578\u64da\u7684\u7cbe\u5bc6\u5ea6 .\n4 But right now, we want to know if there is difference between the two modification \nmethods. Right? \u525b\u525b\u6211\u5011\u662f\u5206\u5225\u628a\u5169\u7a2e\u65b9\u6cd5\u9032\u884c\u6bd4\u8f03\u3002 \u7136\u5f8c\u6211\u5011\u77e5\u9053\u7b2c\u4e00\u7a2e\u65b9\u6cd5\u6c92\n\u6709\u986f\u8457\u6027\u7684\u6539\u5584 , \u7b2c\u4e8c\u7a2e\u65b9\u6cd5\u6709\u986f\u8457\u6027\u7684\u6539\u5584\u3002 \u90a3\u9019\u5169\u7a2e\u6539\u5584\u65b9\u6cd5\u4e4b\u9593\u6709\u6c92\u6709\u5dee\u5225 , \n\u6211\u5011\u4e5f\u611f\u8208\u8da3\u3002\n5 \u6240\u4ee5\u6211\u5011\u8981\u505a\u4e00\u500b comparison, right? We also calculated that F, and F is also the ratio of \nthese two balances, which gives 1.56. a nd in this case, the critical value is 2. 69.\n6 How can we get the 2.69? d egree of freedom, both are?\n7 a student: Both are twelve.\n8 Ming: twelve, right? Both are twelve. So, we can directly get from this table. We can conclude that \nthe two methods, give equivalent precision. \u5c31\u662f\u8aac\u4ed6\u5011\u5006\u7d66\u7684 precision \u662f\u76f8\u7576\u7684 , \u6c92\u6709\n\u5f88\u660e\u986f\u7684\u5340\u5225\u3002\n9 \u4f46\u662f\u6839\u64da\u6211\u5011\u525b\u525b\u6bd4\u8f03\u7684 , \u7b2c\u4e00\u7a2e\u65b9\u6cd5\u5c31\u662f\u4e0d\u8db3\u4ee5\u6709\u5f88\u597d\u7684\u6539\u5584\u3002 \u5c31\u662f\u8aac\u4f60\u8ddf\u4e0d\u540c\n\u7684\u6771\u897f\u6bd4\u8f03 , \u4ed6\u7684\u7d50\u8ad6\u662f\u4e0d\u4e00\u6a23\u7684\u3002\nIn this dialogue, Ming first asked a question in English (mainly) (turn 1), and the students \nresponded in the same language. We can understand the use of \u2018 \u9084\u662f \u2019(or) as a marker to \nraise the students\u2019 awareness of the concepts of \u2018precision\u2019 and \u2018accuracy. \u2019 In turns 3\u20134, Ming \nprovided a bilingual explanation for a critical part and then used English for additional \nexplanation (turn 5). She followed the flow and raised a question in English to check students\u2019 \nunderstanding in turn 6. In the subsequent explanation (turns 8\u20139), instead of solely repeating \nher English utterances in Chinese, Ming provided further elaboration in Chinese (turn 9).\nAs evident in this dialogue, Ming\u2019s classroom exhibited a high level of interactivity, with \nthe teacher\u2019s and students\u2019 active participation through translanguaging practices. Ming \nstrategically employed a \u2018fluid, flexible, and distributed\u2019 (Lin and Lo 2017 ) utilisation of \nvarious registers of English and Chinese during the knowledge construction process. \nSpecifically, L1 everyday language (e.g. \u9084\u662f , \u5c0d\u5427 ), L1 academic language (e.g. \u7cbe\u5bc6\u5ea6 , \n\u986f\u8457\u6027 ), and English academic language (e.g. modification methods, equivalent precision) \nwere used to facilitate knowledge co-construction. In addition, Ming skillfully responded \nto students\u2019 contributions by elaborating or transitioning to new topics, extending or linking \nthem to students\u2019 personal experiences (Lin and Lo 2017 ).\nMing conventionally ended her classes by playing a short English lecture video ( Figure\u00a03 ) \ncovering the academic content taught in the lecture, but voiced by a native-English-speaking \nacademic. She explained:\n3Ming: I think the speaker\u2019s English in the video is more standard than mine. The video is used \nto help [students] consolidate their newly-learnt knowledge and improve their English.\nMing adeptly incorporated this native-English and multimodal teaching resource as an \nintegral component of her translanguaging pedagogy. In this approach, the video served as \na language expert in EMI teaching, enhancing students\u2019 English proficiency through \u2018stan -\ndard\u2019 English input. This strategy was underpinned by the interplay between the institute\u2019s \nDiP of internationalism, which advocates for EMI implementation regardless of students\u2019 \nEnglish proficiency, and Ming\u2019s strong teacher identity, which strove to support students in \ncontent knowledge acquisition and English language development (HB). She perceived lan -\nguage teaching as essential to her teaching responsibilities while recognising her non-native ",
        "metadata": {
            "source": "pdf/64.pdf",
            "page": 15
        }
    },
    {
        "page_content": "LAnGUAGE  AnD  EDUCATIOn 57\nEnglish speaker language identity and lack of professional training in academic English \nliteracy (suggested in the subsequent interview).\nMing\u2019s interview revealed that her translanguaging in classrooms was a planned pedagogy \n(Lin 2020 ), shaped by her continuous self-reflection on the past three years of teaching \nexperiences and influenced by ongoing negotiations surrounding the institute\u2019s lan -\nguage policy:\nResearcher: How did you decide on your language strategy?\nMing: This is the third year I have taught this course. In the first year, I used 50% English and \n50% Chinese in teaching. In the second year, I used 100% English, no Chinese at all. Then I \nfound the second year\u2019s students demonstrated worse mastery of the knowledge. The diffi -\nculty of the exams was almost the same. So, I think the English-only teaching method could \nmake it difficult for students to learn.\nResearcher: Why half-English-half-Chinese teaching in the first year? Is this the institute\u2019s \nrequirement?\nMing: Our institute requires \u201cEnglish-based bilingual teaching, \" so I only taught my first \nlecture in English. But the students responded that they could not understand. So I switched \nto half-and-half upon the students\u2019 request.\nResearcher: Then why did you switch back to 100% English the next year?\nMing: That was decided by the department head. The institute used to let us decide our class -\nroom language, depending on our students\u2019 situations. To my knowledge, most teachers pre -\nferred speaking more Chinese, but they also found that the students had problems with \nEnglish proficiency. So, our department head suggested we should not make the concessions \nthe students requested. We should \u201cpush them, train them (to learn in English), but not com -\npromise, \u201d she said. She believes learning in English is beneficial for them to study abroad in \nthe future.\nThe above excerpt foregrounds the role of individual lecturers\u2019 agency in a dynamic \nlanguage policy process by which the meaning of \u2018English-based bilingual teaching\u2019 in the \nFigure 3.  Ming played a video about T-test (in english).",
        "metadata": {
            "source": "pdf/64.pdf",
            "page": 16
        }
    },
    {
        "page_content": "58 A. W. OU AnD M. M. GU\npolicy text has undergone reinterpretation across different levels of EMI implementation. \nThe department head\u2019s decision to enforce an English-only policy aligns with the DiP of \ninternationalism, which emphasises the instrumental value of English in the institute\u2019s \ninternationalization efforts but rests upon the untested assumption that EMI teaching exclu -\nsively in English will enhance students\u2019 English language proficiency and increase their \nsuccess in pursuing education overseas. Consequently, this institute-level policy regarded \nmultilingualism as a problem, considering translanguaging pedagogy as a last resort when \nEnglish-only instruction did not work and instructing frontline teachers to enhance stu -\ndents\u2019 English language development through monolingual EMI teaching. However, this \ncontradicts the beliefs held by some EMI lecturers, such as Ming, who perceived students\u2019 \nL1 as an asset in EMI teaching (HB) and experimented with translanguaging pedagogical \nstrategies to draw on students\u2019 multilingual repertoire to teach. As suggested by Ming\u2019s \nthird-year teaching practice, she reacted as an agentive language policy arbiter (Hult 2018 ), \nresisted the institution-level English monolingualism ideology when she found it deterio -\nrated students\u2019 content knowledge learning, and formulated her translanguaging policy in \nthe classroom to facilitate a positive learning environment.\nMing\u2019s agency in language policy negotiation is closely associated with her professional \nidentity (HB), which highlights a pursuit of balance between teaching and research. Unlike \nY e, who showed reluctance to improve teaching, Ming was keen on improving her translan -\nguaging pedagogy to better cater to students\u2019 learning needs. When asked what motivated \nher to explore different pedagogical methods, Ming responded:\nMing: To me, teaching and research are equally important. Because I enjoy teaching very \nmuch. I think communicating with students is the primary source of my sense of achievement \nas a teacher, even though I must acknowledge that the university\u2019s staff assessment is more \ngeared towards research.\nAlike other colleagues, Ming was aware of the prevailing DiP of research meritocracy \nand its constraining effects on academics\u2019 teacher identity development. Nevertheless, \ninstead of fully surrendering to this dominant value and holding research merits as the \nfirst-and-only concern, Ming associated her professional identity with teaching and stu -\ndents\u2019 learning performance, embodied in a sense of achievement when her students showed \nunderstanding and positive responses. Identifying herself as an EMI teacher and researcher \nrather than an intellectual who just happened to teach her subject in English (Block and \nMoncada-Comas 2022 ), Ming thus detached the DiP of research meritocracy from her \nclassroom teaching and performed proactively in reshaping the monolingualism-oriented \nEMI policy into dialogic translanguaging to benefit students\u2019 learning.\nAs illustrated in Figure 4 , with a robust teacher identity and a flexible language ide -\nology when implementing EMI teaching, Ming explored a planned translanguaging \npedagogy in response to students\u2019 learning needs. In contrast to Y e, Ming prioritised her \nprofessional identity as an EMI teacher \u2013 a combination of disciplinary teacher and \nlanguage teacher \u2013 and demonstrated considerable agency in navigating the discourses \nimposed by the institution and transforming her classroom into a dialogic translanguag -\ning environment. In her class, the students\u2019 L1 was valued and strategically employed to \nsupport knowledge construction and online English media was recontextualised to facil -\nitate English language development.",
        "metadata": {
            "source": "pdf/64.pdf",
            "page": 17
        }
    },
    {
        "page_content": "LAnGUAGE  AnD  EDUCATIOn 59\nDiscussion and concluding remarks\nThis study has explored the implementation of translanguaging pedagogy by STEM teachers \nin EMI classrooms and addressed how the teachers\u2019 professional identities can influence \ntheir language strategies in classrooms and the teaching effects. The findings, in line with \nprior research (e.g. Wang 2019 ), indicate the presence of divergent language ideologies \n(monoglot or flexibly multilingual) among the teachers regarding EMI teaching, which \ncorrespond to their respective language approaches (unplanned or planned translanguaging) \nin classroom instruction. Using nexus analysis, our investigation has focused on the intricate \nweb of discourses surrounding how the teachers developed different ideologies and beliefs \nand utilised their linguistic repertoires in teaching. The findings reveal that teachers\u2019 \ntranslanguaging pedagogy is intricately embedded and shaped by a dynamic social-discur -\nsive system, in which the discourses of internationalism and research meritocracy prevailing \nin their institution and the teachers\u2019 individual perceptions of and commitment to EMI \nteaching interact with each other to influence their teaching practices. This discursive \nintersection highlights the impact of individual academic staff \u2019s teacher identity and agency \non their engagement with institutional and classroom discourses. When the university \nprioritised scholarly achievements over teaching quality, a planned and interactive translan -\nguaging pedagogy could only thrive when teachers possess a strong teacher identity that \nFigure 4.  discourse nexus of Ming\u2019s translanguaging pedagogy.",
        "metadata": {
            "source": "pdf/64.pdf",
            "page": 18
        }
    },
    {
        "page_content": "60 A. W. OU AnD M. M. GU\ndrives them to address students\u2019 learning needs, like Ming. In such cases, the teachers are \nempowered to act as local EMI policymakers who can reshape EMI classroom communi -\ncation norms to create a supportive learning environment for students\u2019 knowledge con -\nstruction and language development. Conversely, absent a strong teacher identity, academics \nmay resort to a spontaneous and pragmatic approach to EMI teaching, embracing unplanned \ntranslanguaging practices that alleviate their teaching responsibilities and allow more time \nfor research outputs.\nThe professional identity of the investigated EMI teachers reifies a site of struggle \n(Beijaard et\u00a0al. 2004 ). However, unlike the anticipated conflicting identities associated \nwith their disciplinary expertise and English language competence (e.g. Jiang et\u00a0al. 2019 ; \nBlock and Moncada-Comas 2022 ), participants in this study encountered a more sys -\ntematic identity challenge \u2013 the competition between their professional roles as good \nEMI teachers and successful researchers \u2013 that was deeply embedded in the academic \nstaff management system of the examined institution. This identity dilemma problema -\ntises an unspoken discourse of research meritocracy bred in the contemporary higher \neducation system, which seeks to foster world-class universities but exacerbates academic \npressures on scholars and devalues teaching-related activities (Tian and Lu 2017 ). This \ndiscourse demotivates faculty members from investing in their teacher identity and \nimproving pedagogical skills. For example, Y e\u2019s desire to develop a good EMI teacher \nidentity was greatly hampered by the lack of time for teaching preparation and institu -\ntional support for pedagogical knowledge. This reminds us that the call for more planned \ntranslanguaging pedagogy in EMI (e.g. Lin 2020 ) ought to go hand-in-hand with con -\nstructing robust university staff performance evaluation systems that treat research and \nteaching equally. Establishing a more balanced view of academic staff \u2019s research merits \nand teaching achievements will also facilitate the mutual informing process between \nresearch and teaching.\nOur analysis reveals that the institution-level discourse of internationalism could hinder \nteachers\u2019 identity formation and exercise of teacher agency in EMI teaching. Driven by \nthis discourse, which mainly associates English with its extrinsic and symbolic value in \ninternationalising the institution profile, university administrators (and policymakers) \nperceived translanguaging as problematic and promoted a monoglot approach to EMI \nteaching. This finding aligns with similar observations in other EMI contexts (e.g. \nMortensen 2014 ), where monolingual-English EMI policies often contradict multilingual \nteachers\u2019 and students\u2019 \u2018translanguaging instinct\u2019 (Li 2018 ) and their flexible language use \nfor teaching and learning. Due to the inherent power imbalance between EMI teachers \nand administrators, teachers are often constrained (as observed in Ming\u2019s case) in their \nexploration and development of translanguaging pedagogy based on their own teaching \nbeliefs. This constraint risks undermining the cultivation and preservation of EMI teacher \nidentity, which is already precarious due to its competing relationship with the institu -\ntion-imposed researcher identity.\nIt is important to note that the findings in no way present Ming as a model translan -\nguaging pedagogy executor. While commendable, her exploration of planned translanguag -\ning pedagogy proceeded as a self-motivated process without professional guidance. \nMoreover, Ming acknowledged her challenges in effectively fostering students\u2019 English \nlanguage development due to a lack of academic English literacy teaching training. As \nLasagabaster ( 2022 ) pointed out, when universities strive to satisfy increased demand for ",
        "metadata": {
            "source": "pdf/64.pdf",
            "page": 19
        }
    },
    {
        "page_content": "LAnGUAGE  AnD  EDUCATIOn 61\ninternational education through EMI, teachers must be provided sufficient and handy sup -\nport to effectively deliver EMI education. The results of this study shed light on the impor -\ntance of teacher identity construction as part of teacher training for EMI. Firstly, it is crucial \nthat institutions acknowledge and appreciate faculty members\u2019 teaching accomplishments \non par with their research performance. To that end, the staff assessment mechanism should \nequally recognise teaching merits.\nConcrete guidance and toolkits are also essential for STEM-EMI teachers to develop and \nstrengthen their teacher identity. Specifically, universities should provide pre- and in-service \nteachers with opportunities to engage in reflective teaching practices, enabling them to \ndeepen their understanding of their teaching beliefs, values, and language ideologies. They \nshould move away from the monoglot and English speakerism view of EMI teaching, rec -\nognising the pedagogical value of students\u2019 multilingual repertoires.\nAdditionally, drawing from Ming\u2019s experience, EMI teachers need to adopt a self-reflec -\ntive teaching approach, forester active interaction with students, and draw insights from \ncutting-edge pedagogical translanguaging theories and practices to develop effective \ntranslanguaging instructional strategies. In particular, if STEM teachers are expected to be \nacademic English language educators, they should be equipped with the necessary disci -\nplinary knowledge of academic literacy education and skills to integrate English education \nwith content teaching. One possible method is to build a teacher-researcher partnership in \ncontent and language-integrated learning.\nLastly, developing teacher identity requires a collaborative effort to establish a more \ninclusive language policymaking system within the institution. Teachers should be encour -\naged to exercise their agency in negotiating language policy and creating supportive instruc -\ntional environments, while faculty management members and policymakers should actively \nacknowledge and incorporate teachers\u2019 perspectives, facilitating opportunities for knowl -\nedge-sharing and co-designing language policies.\nThe study has limitations. STEM-EMI teachers\u2019 professional identities were examined \nmainly through the experience of two bilingual Chinese teachers with similar linguistic, \nsociocultural, and professional backgrounds. Despite the contrastive beliefs and practices \nrevealed by the analysis, the study left no space for a more diverse EMI teaching faculty \n(such as Jerry, a native English-speaking teacher at the same institute in a non-research \nposition). Future studies may explore how teachers with divergent backgrounds position \nthemselves differently to EMI policy discourses and compare teachers\u2019 positionings and \nteaching practices from different universities with different EMI policy discourses.\nNotes\n 1. The examined EMI department is anonymised and addressed as [the institute] in this article.\n 2. (.): brief pause.\n 3. This piece of data was published in another work (Gu and Ou forthcoming), but the analyti -\ncal approach and arguments are completely different.\nDisclosure statement\nNo potential conflict of interest was reported by the author(s).",
        "metadata": {
            "source": "pdf/64.pdf",
            "page": 20
        }
    },
    {
        "page_content": "62 A. W. OU AnD M. M. GU\nReferences\nBeauchamp C, Thomas L. 2009 . Understanding teacher identity: an overview of issues in the litera -\nture and implications for teacher education. Camb J Educ. 39(2):175\u2013189. https://doi.org/  \n10.1080/03057640902902252\nBeijaard D, Meijer PC, Verloop N. 2004 . Reconsidering research on teachers\u2019 professional identity. \nTeach Teach Educ. 20(2):107\u2013128. https://doi.org/10.1016/j.tate.2003.07.001\nBiesta G, Priestley M, Robinson S. 2015 . The role of beliefs in teacher agency. Teach Teach. \n21(6):624\u2013640. https://doi.org/10.1080/13540602.2015.1044325\nBillot J. 2010 . The imagined and the real: Identifying the tensions for academic identity. Higher \nEduc Res Dev. 29(6):709\u2013721. https://doi.org/10.1080/07294360.2010.487201\nBlock D, Moncada-Comas B. 2022 . English-medium instruction in higher education and the ELT \ngaze: STEM lecturers\u2019 self-positioning as NOT English language teachers. Int J Biling Educ Biling. \n25(2):401\u2013417. https://doi.org/10.1080/13670050.2019.1689917\nChang S-Y . 2019 . Beyond the English box: constructing and communicating knowledge through \ntranslingual practices in the higher education classroom. Engl Teach Learn. 43(1):23\u201340. https://\ndoi.org/10.1007/s42321-018-0014-4\nChen H, Han J, Wright D. 2020 . An investigation of lecturers\u2019 teaching through English medium of \ninstruction\u2014a case of higher education in China. Sustainability. 12(10):4046. https://doi.org/  \n10.3390/su12104046\nCummins J. 2019 . The emergence of translanguaging pedagogy: a dialogue between theory and \npractice. J Multilingual Educ Res. 9(13):19\u201336.\nDang TKA, Bonar G, Y ao J. 2023 . Professional learning for educators teaching in English-medium-\ninstruction in higher education: a systematic review. Teach Higher Educ. 28(4):840\u2013858. https://\ndoi.org/10.1080/13562517.2020.1863350\nDoiz A, Lasagabaster D, Sierra JM. 2012 . English-medium instruction at universities: global chal -\nlenges. Bristol (UK): Multilingual Matters.\nFang F, Liu Y . 2020 . \u2018Using all English is not always meaningful\u2019: stakeholders\u2019 perspectives on the \nuse of and attitudes towards translanguaging at a Chinese university. Lingua. 247:102959. https://\ndoi.org/10.1016/j.lingua.2020.102959\nGarc\u00eda O, Li W . 2014 . Translanguaging: language, bilingualism, and education. New Y ork (NY): \nPalgrave.\nGee JP . 1999 . An introduction to discourse analysis: theory and method. London: Routledge.\nGu MM, Lee JC-K. 2019 . \u2018They lost internationalization in pursuit of internationalization\u2019: students\u2019 \nlanguage practices and identity construction in a cross-disciplinary EMI program in a university \nin China. High Educ. 78(3):389\u2013405. https://doi.org/10.1007/s10734-018-0342-2\nGu MM, Lee JC-K, Jin T. 2022 . A translanguaging and trans-semiotizing perspective on subject \nteachers\u2019 linguistic and pedagogical practices in EMI programme. Appl Ling Rev. 0(0):1\u201327. \nhttps://doi.org/10.1515/applirev-2022-0036\nGu MM, Ou AW . forthcoming. Trans-languaging and trans-knowledging practices among STEM \nteachers in EMI programmes in higher education. Appl Ling Rev.\nHolmes P , Fay R, Andrews J, Attia M. 2013 . Researching multilingually: new theoretical and meth -\nodological directions. Int J Appl Ling. 23(3):285\u2013299. https://doi.org/10.1111/ijal.12038\nHu G. 2009 . The craze for English-medium education in China: driving forces and looming conse -\nquences. Engl Today. 25(4):47\u201354. https://doi.org/10.1017/S0266078409990472\nHu G, Duan Y . 2019 . Questioning and responding in the classroom: a cross-disciplinary study of the \neffects of instructional mediums in academic subjects at a Chinese university. Int J Biling Educ \nBiling. 22(3):303\u2013321. https://doi.org/10.1080/13670050.2018.1493084\nHu G, Lei J. 2014 . English-medium instruction in Chinese higher education: a case study. High \nEduc. 67(5):551\u2013567. https://doi.org/10.1007/s10734-013-9661-5\nHult FM. 2017 . Nexus analysis as scalar ethnography for educational linguistics. In: M. Martin-\nJones and D. Martin, editors. Researching multilingualism: critical and ethnographic perspec -\ntives. New Y ork: Routledge; p. 89\u2013105.",
        "metadata": {
            "source": "pdf/64.pdf",
            "page": 21
        }
    },
    {
        "page_content": "LAnGUAGE  AnD  EDUCATIOn 63\nHult FM. 2018 . Engaging pre-service English teachers with language policy. ELT J. 72(3):249\u2013259. \nhttps://doi.org/10.1093/elt/ccx072\nInbar-Lourie O, Donitsa-Schmidt S. 2020 . EMI Lecturers in international universities: is a native/\nnon-native English-speaking background relevant? Int J Biling Educ Biling. 23(3):301\u2013313. \nhttps://doi.org/10.1080/13670050.2019.1652558\nJablonkai RR, Hou J. 2021 . English medium of instruction in Chinese higher education: a systematic \nmapping review of empirical research. Appl Ling Rev. 0(0): 1\u201330. https://doi.org/10.1515/applirev-  \n2021-0179\nJia W , Fu X, Pun J. 2023 . How do EMI lecturers\u2019 translanguaging perceptions translate into their \npractice? A multi-case study of three Chinese tertiary EMI classes. Sustainability. 15(6):4895. \nhttps://doi.org/10.3390/su15064895\nJiang L, Zhang LJ, May S. 2019 . Implementing English-medium instruction (EMI) in China: teach -\ners\u2019 practices and perceptions, and students\u2019 learning motivation and needs. Int J Biling Educ \nBiling. 22(2):107\u2013119. https://doi.org/10.1080/13670050.2016.1231166\nJohnson DC. 2011 . Critical discourse analysis and the ethnography of language policy. Crit \nDiscourse Stud. 8(4):267\u2013279. https://doi.org/10.1080/17405904.2011.601636\nKama\u015fak R, Sahan K, Rose H. 2021 . Academic language-related challenges at an English-medium \nuniversity. J Engl Acad Purposes. 49:100945. https://doi.org/10.1016/j.jeap.2020.100945\nKaplan R. 2001 . Language teaching and language policy. Appl Lang Learn. 12(1):81\u201386.\nKuteeva M, Airey J. 2014 . Disciplinary differences in the use of English in higher education: reflec -\ntions on recent language policy developments. High Educ. 67(5):533\u2013549. https://doi.org/10.1007/\ns10734-013-9660-6\nLasagabaster D. 2022 . Teacher preparedness for English-medium instruction. JEMI. 1(1):48\u201364. \nhttps://doi.org/10.1075/jemi.21011.las\nLasagabaster D, Doiz A. 2021 . Language use in English-medium instruction at university: interna -\ntional perspectives on teacher practice. New Y ork: Routledge.\nLasagabaster D, Doiz A. 2023 . Classroom interaction in English-medium instruction: are there \ndifferences between disciplines? Lang Cult Curriculum. 36(3):310\u2013326. https://doi.org/10.1080/\n07908318.2022.2151615\nLi W . 2018 . Translanguaging as a practical theory of language. Appl Ling. 39(1):9\u201330.\nLi W . 2022 . Translanguaging as a political stance: implications for English language education. ELT \nJ. 76(2):172\u2013182.\nLin AM. 2020 . Introduction: translanguaging and translanguaging pedagogies. In: Vaish V , editor. \nTranslanguaging in multilingual English classrooms: an Asian perspective and contexts. Singapore: \nSpringer, p. 1\u20139.\nLin AM, Lo YY . 2017 . Trans/languaging and the triadic dialogue in content and language integrated \nlearning (CLIL) classrooms. Lang Educ. 31(1):26\u201345. https://doi.org/10.1080/09500782.2016.12\n30125\nLiu Y , Fang F. 2022 . Translanguaging theory and practice: how stakeholders perceive translanguaging \nas a practical theory of language. RELC J. 53(2):391\u2013399. https://doi.org/10.1177/0033688220939222\nMacaro E. 2018 . English medium instruction: content and language in policy and practice. Oxford: \nOxford University Press.\nMazak CM, Carroll KS. 2016 . Translanguaging in higher education: beyond monolingual ideolo -\ngies. Tonawanda (NY): Multilingual Matters.\nMortensen J. 2014 . Language policy from below: Language choice in student project groups in a \nmultilingual university setting. J Multiling Multicultural. Dev. 35(4):425\u2013442.\nNikula T, Dalton-Puffer C, Garc\u00eda AL. 2013 . CLIL classroom discourse: research from Europe. JICB. \n1(1):70\u2013100. https://doi.org/10.1075/jicb.1.1.04nik\nOu AW , Gu MM, Lee JC-K. 2022 . Learning and communication in online international higher ed -\nucation in Hong Kong: ICT-mediated translanguaging competence and virtually translocal iden -\ntity. J Multiling Multicultural Dev. 0(0): 1\u201314. https://doi.org/10.1080/01434632.2021.2021210\nPaulsrud B, Tian Z, Toth J. 2021 . English-medium instruction and translanguaging. Bristol: \nMultilingual Matters.",
        "metadata": {
            "source": "pdf/64.pdf",
            "page": 22
        }
    },
    {
        "page_content": "64 A. W. OU AnD M. M. GU\nRahman MM, Singh MKM. 2022 . English medium university STEM teachers\u2019 and students\u2019 ideolo -\ngies in constructing content knowledge through translanguaging. Int J Biling Educ Biling. \n25(7):2435\u20132453. https://doi.org/10.1080/13670050.2021.1915950\nScollon R, Scollon SW . 2004 . Nexus analysis: discourse and the emerging internet. New Y ork (NY): \nRoutledge.\nSong Y . 2023 . \u2018Does Chinese philosophy count as philosophy?\u2019: Decolonial awareness and practices \nin international English medium instruction programs. High Educ (Dordr). 85(2):437\u2013453. \nhttps://doi.org/10.1007/s10734-022-00842-8\nTian M, Lu G. 2017 . What price the building of world-class universities? Academic pressure faced \nby young lecturers at a research-centered University in China. Teach Higher Educ. 22(8):957\u2013\n974. https://doi.org/10.1080/13562517.2017.1319814\nTrent J. 2017 . \u2018Being a professor and doing EMI properly isn\u2019t easy\u2019 . An identity-theoretic investiga -\ntion of content teachers\u2019 attitudes towards EMI at a university in Hong Kong. In: Fenton-Smith B, \nHumphreys P , Walkinshaw I, editors. English medium instruction in higher education in Asia-\nPacific. Cham: Springer. p. 219\u2013239.\nvan Lankveld T, Schoonenboom J, Volman M, Croiset G, Beishuizen J. 2017 . Developing a teacher \nidentity in the university context: A systematic review of the literature. Higher Educ Res Dev. \n36(2):325\u2013342. https://doi.org/10.1080/07294360.2016.1208154\nW\u00e4chter B, Maiworm F. 2014 . English-taught programmes in European higher education. Bonn: \nLemmens.\nWang D. 2019 . Translanguaging in Chinese foreign language classrooms: students and teachers\u2019 at -\ntitudes and practices. Int J Biling Educ Biling. 22(2):138\u2013149. https://doi.org/10.1080/13670050.\n2016.1231773\nXu J, Ou AW . 2022 . Facing rootlessness: language and identity construction in teaching and research \npractices among bilingual returnee scholars in China. J Lang Identity Educ. 0(0):1\u201316. https://\ndoi.org/10.1080/15348458.2022.2060228\nYuan R, Y ang M. 2023 . Towards an understanding of translanguaging in EMI teacher education \nclassrooms. Lang Teach Res. 27(4):884\u2013906. https://doi.org/10.1177/1362168820964123",
        "metadata": {
            "source": "pdf/64.pdf",
            "page": 23
        }
    },
    {
        "page_content": "THESIS FOR THE DEGREE OF DOCTOR OF PHILOSOPHY\nFrom Industry to Practice: Can Users Tackle\nDomain Tasks with Augmented Reality?\nYUCHONG ZHANG\nDepartment of Computer Science and Engineering\nDivision of Interaction Design and Software Engineering\nCHALMERS UNIVERSITY OF TECHNOLOGY\nG\u00f6teborg, Sweden 2023",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 0
        }
    },
    {
        "page_content": "YUCHONG ZHANG\nG\u00f6teborg, Sweden 2023\nISBN 978-91-7905-784-8\nCOPYRIGHT \u00a9YUCHONG ZHANG, 2023\nDoktorsavhandlingar vid Chalmers tekniska h\u00f6gskola\nNy serie Nr 5250\nISSN 0346-718X\nISSN 1652-0769 Technical Report 230D\nDivision of Interaction Design and Software Engineering\nDepartment of Computer Science and Engineering\nChalmers University of Technology\nSE-412 96 G\u00f6teborg, Sweden\nTelephone: +46 (0)31-772 1000\nPrinted by Chalmers Reproservice\nG\u00f6teborg, Sweden 2023FromIndustrytoPractice: CanUsersTackleDomainTaskswithAugmented Reality?",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 1
        }
    },
    {
        "page_content": "From Industry to Practice: Can Users Tackle Domain Tasks with Augmented\nReality?\nYUCHONG ZHANG\nDivision of Interaction Design and Software Engineering\nDepartment of Computer Science and Engineering\nChalmers University of Technology\nAbstract\nAugmented Reality (AR) is a cutting-edge interactive technology. While Vir-\ntual Reality (VR) is based on completely virtual and immersive environments,\nAR superimposes virtual objects onto the real world. The value of AR has been\ndemonstrated and applied within numerous industrial application areas due to\nits capability of providing interactive interfaces of visualized digital content. AR\ncan provide functional tools that support users in undertaking domain-related\ntasks, especially facilitating them in data visualization and interaction by jointly\naugmenting physical space and user perception. Making effective use of the ad-\nvantages of AR, especially the ability which augment human vision to help users\nperform different domain-related tasks is the central part of my PhD research.\nIndustrial process tomography (IPT), as a non-intrusive and commonly-used\nimaging technique, has been effectively harnessed in many manufacturing com-\nponents for inspections, monitoring, product quality control, and safety issues.\nIPT underpins and facilitates the extraction of qualitative and quantitative data\nregarding the related industrial processes, which is usually visualized in various\nways for users to understand its nature, measure the critical process characteris-\ntics, andimplementprocesscontrolinacompletefeedbacknetwork. Theadoption\nof AR in benefiting IPT and its related fields is currently still scarce, resulting in\na gap between AR technique and industrial applications. This thesis establishes\na bridge between AR practitioners and IPT users by accomplishing four stages.\nFirst of these is a need-finding study of how IPT users can harness AR tech-\nnique was developed. Second, a conceptualized AR framework, together with the\nimplemented mobile AR application developed in an optical see-through (OST)\nhead-mounted display (HMD) was proposed. Third, the complete approach for\nIPT users interacting with tomographic visualizations as well as the user study\nwas investigated.\nBased on the shared technologies from industry, we propose and examine an\nAR approach for visual search tasks providing visual hints, audio hints, and gaze-\nassisted instant post-task feedback as the fourth stage. The target case was a\nbook-searching task, in which we aimed to explore the effect of the hints and\niii",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 2
        }
    },
    {
        "page_content": "the feedback with two hypotheses: that both visual and audio hints can posi-\ntively affect AR search tasks whilst the combination outperforms the individuals;\nthat instant post-task feedback can positively affect AR search tasks. The proof-\nof-concept was demonstrated by an AR app in an HMD with a two-stage user\nevaluation. The first one was a pilot study (n=8) where the impact of the vi-\nsual hint in benefiting search task performance was identified. The second was a\ncomprehensive user study (n=96) consisting of two sub-studies, Study I (n=48)\nand Study II (n=48). Following quantitative and qualitative analysis, our results\npartially verified the first hypothesis and completely verified the second, enabling\nus to conclude that the synthesis of visual and audio hints conditionally improves\nAR search task efficiency when coupled with task feedback.\nKeywords: IndustrialProcessTomography,AugmentedReality,Human-centered\nDesign, User Study, Qualitative and Quantitative Analysis\niv",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 3
        }
    },
    {
        "page_content": "Acknowledgements\nI consider my entire PhD journey as an evolution from \u2019nearly knowing nothing\u2019\nto \u2019knowing something\u2019. I would not have accomplished it without the support of\nmany people. First and foremost, I would like to thank my parents who firmly\nstand behind me me all the time. I\u2019ve been studying and working abroad alone\nfor eight years, it is unimaginable for me to become who I am if I don\u2019t have\ntheir constant support. I also want to express my gratitude to all of my family\nmembers caring about me, they are my strongest backing.\nI would like to express my sincere appreciation to my supervisor Prof. Morten\nFjeld for choosing me as one of his PhD students at Chalmers. 5 years ago, I got\nthe chance to become a PhD researcher and write the new chapter of my life. He\nis not only a supervisor but more a good friend of mine, giving me full support\nand recognition unconditionally at all time. Then, I would like also to thank my\nco-supervisors Dr Alan Said. He is really a kind person with strong academic\nskills, which helped my learning and growth a lot in my PhD study. Also, I would\nlike to thank my previous co-supervisor Prof. Marco Fratarcangeli, he is a perfect\nrole model for academic researchers.\nMy sincere thanks to t2i lab and my former and current colleagues. Special\nthanks to Tomasz Kosi\u0144ski, Mads R\u00f8nnow for both of their colleagueship but\nmore importantly the long-term friendship. Thanks to Ziming Wang, Yemao\nMan for being good lab mates/division mates, it is lucky for me to have some\npeople to communicate with in my mother tongue. Many thanks to everyone\nin Interaction Design, IDSE, CSE, Chalmers, a cohesive and friendly unit which\nmakes my PhD journey more wonderful.\nI would like to thank more people who helped me in some forms during my\nPhD. They are: Adam Nowak, Andrzej Romanowski, Pawe\u0142 Wo\u017aniak, Adel Om-\nrani, Rahul Yadav, Guruprasad Rao, Barbara Stuckey, Shengdong Zhao, Philippa\nBeckman.\nFinally, I would like to give my gratitude to the project I was involved in which\nconstituted most of my PhD \u2013 TOMOCON (https://www.tomocon.eu/). This\nproject has received funding from the European Union\u2019s Horizon 2020 research\nand innovation programme under the Marie Sklodowska-Curie grant agreement\nNo. 764902.\nv",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 4
        }
    },
    {
        "page_content": "",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 5
        }
    },
    {
        "page_content": "List of main publications\nThis thesis presents an introduction, summary and extension to the following\nappended main publications: papers I to V\n[I] Y. Zhang, A. Nowak, G. Rao, A. Romanowski, and M. Fjeld, \u201cIs Industrial\nTomography Ready for Augmented Reality? A Need-finding Study of How\nAugmented Reality Can Be Adopted by Industrial Tomography Experts\u201d,\nin International Conference on Human-Computer Interaction (Springer,\n2023, in press).\n[II] Y.Zhang,R.Yadav,A.Omrani,andM.Fjeld,\u201cANovelAugmentedReality\nSystem To Support Volumetric Visualization in Industrial Process Tomog-\nraphy\u201d, in Proceedings of the 2021 Conference on Interfaces and Human\nComputer Interaction (2021), pp. 3\u20139.\n[III] Y. Zhang, A. Omrani, R. Yadav, and M. Fjeld, \u201cSupporting Visualization\nAnalysis in Industrial Process Tomography by Using Augmented Reality\u2013A\nCase Study of an Industrial Microwave Drying System\u201d, Sensors 21, 6515\n(2021).\n[IV] Y. Zhang, Y. Xuan, A. Omrani, R. Yadav, and M. Fjeld, \u201cPlaying with\nData: An Augmented Reality Approach to Immersively Interact with Vi-\nsualizations of Industrial Process Tomography\u201d, in IFIP Conference on\nHuman-Computer Interaction (Springer, 2023, Under review ).\n[V] Y. Zhang, A. Nowak, X. Yueming, R. Andrzej, and F. Morten, \u201cSee or\nHear? Exploring the Effect of Visual and Audio Hints and Gaze-assisted\nTaskFeedbackforVisualSearchTasksinAugmentedReality\u201d,Proceedings\nof the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies\n2023 (Under review ).\nWe always refer to these publications in the thesis as papers I, II, III, IV, and V,\naccording to the labeling in the list above.\nvii",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 6
        }
    },
    {
        "page_content": "Specification of my contributions to\nthe appended main publications \u2013\npapers I to V included in the thesis\nI Led and directed the work included in this paper. Finished the survey study\nwith data collection and analysis. Have written most of the paper with\nfeedback from my co-authors.\nII Led and directed the work included in this paper. Finished the conceptual-\nization, mobile app development, and data analysis. Have written most of\nthe paper with feedback from my co-authors.\nIII Led and directed the work included in this paper. Finished the conceptual-\nization, mobile app development, and data analysis. Have written at least\n70% of the paper together with my co-authors.\nIV Led and directed the work included in this paper. Finished the conceptu-\nalization, reasoning, experimental design, user study implementation, and\ndata analysis. Have written at least 90% of the paper together with my\nco-authors.\nV Led and directed the work included in this paper. Finished the conceptu-\nalization, reasoning, experimental design, user study implementation, and\ndata analysis. Have written at least 90% of the paper together with my\nco-authors.\nviii",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 7
        }
    },
    {
        "page_content": "List of other publications\n[1] Y. Zhang, M. Fjeld, M. Fratarcangeli, A. Said, and S. Zhao, \u201cAffective\nColormap Design for Accurate Visual Comprehension in Industrial Tomog-\nraphy\u201d, Sensors 21, 4766 (2021).\n[2] A. Nowak, Y. Zhang, A. Romanowski, and M. Fjeld, \u201cAugmented Reality\nwith Industrial Process Tomography: To Support Complex Data Analysis\nin 3D Space\u201d, in Adjunct Proceedings of the 2021 ACM International Joint\nConference on Pervasive and Ubiquitous Computing and Proceedings of\nthe 2021 ACM International Symposium on Wearable Computers (2021).\n[3] Y. Zhang, A. Nowak, A. Romanowski, and M. Fjeld, \u201cAn Initial Explo-\nration of Visual Cues in Head-mounted Display Augmented Reality for\nBook Searching\u201d, in Proceedings of the 21st International Conference on\nMobile and Ubiquitous Multimedia (2022).\n[4] Y. Zhang, A. Nowak, A. Romanowski, and M. Fjeld, \u201cOn-site or Remote\nWorking?: An Initial Solution on How COVID-19 Pandemic May Impact\nAugmented Reality Users\u201d, in Proceedings of the 2022 International Con-\nference on Advanced Visual Interfaces (2022), pp. 1\u20133.\nix",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 8
        }
    },
    {
        "page_content": "",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 9
        }
    },
    {
        "page_content": "List of figures\n1.1 Overview of this thesis. . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2.1 The example scene of an IPT laboratory located at Lodz, Poland. . . 5\n2.2 ThethreeARmodalitiespresentedtotheparticipants. A:HeadMounted\nDisplay: These devices can be further categorized into optical see-\nthrough and video see-through [20], displaying information inside\nthe device while allowing users to see the surrounding real world;\nB: Hand Held Display [30]: These are mobile screens, smartphones,\nor tablets with embedded camera and screen. The device fits within\nthe user\u2019s hands and can display virtual objects; C: Spatial Aug-\nmented Reality [31]: These systems utilize digital projectors to dis-\nplay virtual information onto the physical objects. A clear descrip-\ntion of each modality was provided for our participants. . . . . . . 7\n2.3 Conceptualization of our proposed AR approach for IPT visualization\nanalysis. a): The specialized IPT controlled industrial process is\nimplemented in a confined environment. b): The IPT data visual-\nizations originate from different processes. c): Users engage with\nrelevant data analysis. d): User equipped with proposed AR en-\nvironment by an OST HMD interacting with the visualizations for\nfurther analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n4.1 The answers of how domain experts responded to the current AR sta-\ntus. A: Q: Comfort: Which AR scenario shown above looks most\ncomfortable for you? B: Q: Usability: Which AR scenario shown\nabove looks most usable for you (might help with your work)? C:\nQ: Are you aware of any ongoing AR applications in industrial to-\nmography? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n4.2 The conceptualization framework of the proposed AR system is: the\nMWT-controlled industrial microwave drying process block, which-\ncovers the whole running procedure of the process and provides data\nand elements used for analysis; users\u2019 block, representing the peo-\nple involved in this context, such as operators who control and run\nthe process and researchers who observe and analyze the process;\nthe AR app block, which entails the AR implementation of a mobile\nAR app developed on the iOS/Android platform at the initial stage.\n[83, 111]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 10
        }
    },
    {
        "page_content": "4.3 The realization of the proposed AR system. Users hold the handheld\nsmartphones/tablets to run the AR app alongside the industrial\nsetting. After the app activation, users can then observe the vi-\nsualizations displayed on the AR interfaces. At present, the app\nis supported via iOS/Android mobile devices. Users are empow-\nered to switch to different visualizations regarding different phases\nof the process. The visualizations shown on the interfaces are mul-\ntimodal, including the infrared image showing the condition of the\nprocess and the 3D/2D reconstruction of the MWT images (the\nupper left small figure on the floating interface in the upper right\nsubfigure); the reconstructed MWT images with their segmented re-\nsults to show the high moisture areas (the middle right subfigure);\nand the volumetric visualization (the bottom right subfigure) [111]. 20\n4.4 Flowchart of our user study with counterbalancing design. . . . . . . 23\n4.5 Statistical results between the AR approach and the conventional set-\nting. a: The distribution of the understandability levels (1 to 7)\nrated by participants between the AR approach and the conven-\ntional setting. b: The distribution of task completion time (s). c:\nThe distribution of the error rate (0.00 to 1.00). d: The distribu-\ntion of the SUS scores representing usability (0 to 100). e: The\ndistribution of the recommendation levels (1 to 7). . . . . . . . . . 24\n4.6 Study I ( a) and Study II ( b): The mean TCT ( s) used per task by the\nfour groups. Error bars show mean \u00b1standard error (SE). . . . . 25\n4.7 Study I: NASA TLX for the first task by the four groups. Error bars\nshow mean \u00b1standard error (SE). . . . . . . . . . . . . . . . . . . 26\n4.8 Study I: NASA TLX for the second task by the four groups. Error bars\nshow mean \u00b1standard error (SE). . . . . . . . . . . . . . . . . . . 26\n4.9 Study II: NASA TLX for the first task by the four groups. Error bars\nshow mean \u00b1standard error (SE). . . . . . . . . . . . . . . . . . . 27\n4.10 Study II: NASA TLX for the second task by the four groups. Error\nbars show mean \u00b1standard error (SE). . . . . . . . . . . . . . . . 27",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 11
        }
    },
    {
        "page_content": "Contents\nAbstract iii\nAcknowledgements v\nList of main publications vii\nList of other publications ix\nList of figures xi\nContents xiii\n1 Introduction 1\n2 Background 5\n2.1 Need-finding: Is AR Ready for IPT? . . . . . . . . . . . . . . . . . 5\n2.2 AR Framework and Mobile Realization for IPT . . . . . . . . . . . 7\n2.3 The Complete AR Approach with Evaluation for IPT . . . . . . . 8\n2.4 Supporting AR Visual Search Tasks . . . . . . . . . . . . . . . . . 9\n3 Related Work 11\n3.1 AR Visualization with IPT . . . . . . . . . . . . . . . . . . . . . . 11\n3.1.1 Industrial AR . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3.1.2 Visualization for IPT . . . . . . . . . . . . . . . . . . . . . 11\n3.2 AR for Visual Search Tasks . . . . . . . . . . . . . . . . . . . . . . 12\n3.2.1 AR with gaze assistance . . . . . . . . . . . . . . . . . . . . 13\n3.2.2 AR with hints and feedback . . . . . . . . . . . . . . . . . . 13\n4 Problem Solving 15\n4.1 Need-finding: Is AR Ready for IPT? . . . . . . . . . . . . . . . . . 15\n4.1.1 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n4.1.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n4.2 AR Framework and Mobile AR Development . . . . . . . . . . . . 17\n4.2.1 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n4.2.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n4.3 The Complete AR Approach . . . . . . . . . . . . . . . . . . . . . 21\n4.3.1 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n4.3.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nxiii",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 12
        }
    },
    {
        "page_content": "4.4 Practical Visual Search Tasks . . . . . . . . . . . . . . . . . . . . . 24\n4.4.1 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n4.4.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n5 Discussions 29\n5.1 Need-finding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n5.2 AR Framework and Mobile Implementation . . . . . . . . . . . . . 29\n5.3 The Complete AR Approach with Evaluation . . . . . . . . . . . . 30\n5.4 Practical Visual Search Tasks . . . . . . . . . . . . . . . . . . . . . 31\n6 Conclusion 33\nReferences 35\nAppended Papers: Main Publications 47\nPaper I 49\nPaper II 63\nPaper III 73\nPaper IV 91\nPaper V 101\nAppended Papers: Other Publications 121\nPaper 1 123\nPaper 2 145\nPaper 3 151\nPaper 4 157\nxiv",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 13
        }
    },
    {
        "page_content": "1 Introduction\nThe main work presented in this thesis first focuses on investigating how aug-\nmented reality (AR) can help the domain users in specific industrial processes\nmonitored by industrial process tomography (IPT). Second, by sharing the same\ntechnique, it also identifies the capability of AR in improving practical visual\nsearch task performance with gaze assistance. This thesis illustrates the research\ndone in the appended papers I to V, regarding the initial adoption of AR tech-\nniques for IPT and practical task performing with respect to four stages: need-\nfinding of how AR can be used by IPT experts; AR framework conceptualization\nand mobile realization; the complete AR approach with evaluation; and the prac-\ntical search task performing exploration.\nIn each paper, the research questions and the contributions are elicited and\ngenerated. Tables 1.1 and 1.2 give an overview of the specification of the papers\nI to V which constitute the main content of this thesis.\nThis thesis is summarized as follows and shown in Figure 1.1. First, Chapter 1\ngives a preliminary overview of the research as well as the structure presented in\nthe thesis. Chapter 2 introduces the general background of each of the four stages.\nThe related work and contemporary research regarding each specific field is elab-\norated in Chapter 3. Chapter 4 illustrates the methodologies used and systems\ndeveloped for each stage, as well as the results obtained correspondingly. Finally,\ndiscussions and conclusions with future work recommendations are presented in\nChapters 5 and 6.\n1",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 14
        }
    },
    {
        "page_content": "2 Chapter 1. Introduction\nTable 1.1: The research questions and research contributions listed in the papers I to\nV.\nPaper Research Questions Research Contribu-\ntions\nI Could AR be used by\nIPT experts to effec-\ntively support process\nmonitoring ?1. Give an overview of\nthe present status of us-\ning AR by IPT experts.\n2. Identify the need of\nusing AR by IPT ex-\nperts to effectively sup-\nport process monitoring.\n3. Present the poten-\ntial challenges in deploy-\ning AR in IPT\nII How can mobile AR sup-\nport IPT users with vol-\numetric visualization?1. Propose an AR sys-\ntem in supporting col-\nlaborative analysis with\ninteractivity and mobil-\nity for IPT.\n2. Involve volumetric vi-\nsualization regarding the\nIPTprocessestosupport\nmutual collaboration of\nusers.\nIII How can mobile AR sup-\nport IPT users for onsite\nanalysis?1. Propose an entire\ndata processing and vi-\nsualizing workflow of the\nIPTcontrolledindustrial\nprocess.\n2. Integrate AR tech-\nnique to support IPT\ndata visualization and\non-siteanalysisforusers.\nIV Can OST HMD AR fa-\ncilitate IPT users for in-\nteraction and visualiza-\ntion analysis?1. Pioneer a complete\nAR approach to help\nIPT users for contextual\ndata visualization analy-\nsis by OST HMDs.",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 15
        }
    },
    {
        "page_content": "3\nTable 1.2: Cont.\nPaper Research Questions Research Contribu-\ntions\n2. Provideimmersiveex-\nperience and interactive\nvisualizations for users\nto interact and commu-\nnicate with the IPT vi-\nsualizations for better\nunderstandability, task\nperformance and user\nexperience.\n3. Design and im-\nplement a comparative\nstudy to prove the effec-\ntiveness of the proposed\nAR approach compared\nto conventional methods\nand acquire early-stage\nfeedback.\nV How can hints and task\nfeedback affect AR vi-\nsual search task perfor-\nmance?1. Propose an AR ap-\nproach supporting visual\nand audio hints, as well\nas gaze-assisted instant\npost-task feedback for\nvisual search tasks.\n2. Explore the effect\nof visual hints, audio\nhints, combined hints,\nand instant post-task\nfeedback through gaze\nassistance on contextual\nAR searching.\n3. Explore the effect of\ncombininghintsandtask\nfeedback in the same AR\ncontext.",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 16
        }
    },
    {
        "page_content": "4 Chapter 1. Introduction\nFigure 1.1: Overview of this thesis.",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 17
        }
    },
    {
        "page_content": "2 Background\nThis chapter gives a detailed overview of the relevant background within the con-\ntext of this thesis, including the four stages mentioned above in the introduction.\n2.1 Need-finding: Is AR Ready for IPT?\nFigure 2.1: The example scene of an IPT laboratory located at Lodz, Poland.\nIPT is a dedicated and non-invasive imaging technique which is pervasively\nused in manufacturing scenarios for process monitoring or integrated control[1\u20136].\nIt serves as an effective mechanism to extract complex data which is visualized\nin various representations and interprets it for domain users to comprehend the\nessence of the industrial processes [7\u201312]. An example view of an IPT working\nenvironment is shown in Figure 2.1. Due to the speciality and complexity of IPT,\nsome rising technologies are therefore harnessed and made more accessible for all\nusers - expert and lay - to perform complex tomographic data analysis to im-\nprove their productivity and efficiency. Some typical representatives of IPT, such\nas microwave tomography (MWT) [11, 13, 14], electrical resistance tomography\n(ERT) [15], and electrical capacitance tomography (ECT) [16] are widely used\n5",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 18
        }
    },
    {
        "page_content": "6 2 Background\nfor industrial purposes such as moisture detection [17, 18], crack detection and\npowder flow in pipes, and flow pattern detection of granules. This thesis specifi-\ncally concentrates on a unique industrial microwave drying process [10] which uses\nprecise drying and heating equipment for polymer foams with the aid of MWT.\nThe targets of this drying process are firstly heating the object foam placed on\nthe conveyor belt and then detecting the post-drying moisture distribution, by\nadopting the MWT technique.\nWhile having some common points with Virtual Reality (VR), augmented real-\nity/mixed reality (AR/MR) is an interactive technique where computer-generated\nperceptualinformationisvisuallysuperimposedontherealworld[19\u201326]. Whereas\nin VR, a user interacts with virtual objects in a fully artificial three-dimensional\nworld, in AR, a user\u2019s view of the real environment is enhanced with virtual ob-\njects shown at the right time and position from the user\u2019s perspective [27]. AR\ntechnology has been effectively deployed in various applications within industry\ndue to its high mobility and interactivity. For example, AR can provide func-\ntional tools that support users when undertaking domain-related tasks such as\ndata visualization and interaction because of its ability to jointly augment the\nphysical space and the user\u2019s perception [28, 29]. Taking devices and hardware as\nkey parameters [19, 28], AR can be classified into three categories:\n\u2022Head-Mounted Displays (HMD) and wearable hardware: These\ndevices can be further categorized into optical see-through and video see-\nthrough [20], displaying information inside the device while allowing users to\nsee the surrounding real world.\n\u2022Hand-Held Displays (HHD): These are mobile screens, smartphones, or\ntablets with embedded camera and screen. The device fits within the user\u2019s\nhands and can display virtual objects.\n\u2022Spatial Augmented Reality (SAR): These systems utilize digital projec-\ntors to display virtual information onto physical objects.\nAR has successfully provided accessible and innovative solutions for various\nindustrial application domains due to its capacity for interactive visualization\nthrough intelligent user interfaces[32]. Given its proven success within the auto-\nmotive industry [33], shipbuilding [28], and robotics [29], our driving question is\nwhether AR could also be useful in the domain of industrial tomography. The re-\nsearch question is thus elicited: Could AR effectively support process monitoring\nin industrial tomography? Both to capture the current status and to examine the\npotential of AR in this domain, we conducted a need-finding study in the form\nof a systematic survey to model the involved 14 participants. All were industrial\ntomography experts with at least three years of hands-on experience, whom we\nconsidered eligible to offer sufficiently in-depth insights and opinions.",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 19
        }
    },
    {
        "page_content": "7\nFigure 2.2: The three AR modalities presented to the participants. A: Head Mounted\nDisplay: These devices can be further categorized into optical see-through and video\nsee-through [20], displaying information inside the device while allowing users to see\nthe surrounding real world; B: Hand Held Display [30]: These are mobile screens,\nsmartphones, or tablets with embedded camera and screen. The device fits within\nthe user\u2019s hands and can display virtual objects; C: Spatial Augmented Reality [31]:\nThese systems utilize digital projectors to display virtual information onto the physical\nobjects. A clear description of each modality was provided for our participants.\n2.2 AR Framework and Mobile Realization for IPT\nAfter discovering the high potential of deploying AR/MR into the IPT domain\nfrom the need-finding process, we aimed to develop a realistic AR method for\nbenefiting the relevant users. We identified that even though AR has been proved\nand applied in numerous fields such as medicine and education [34], due to defi-\nciencies such as hardware limitation, poor computation power and low knowledge\nconvertibility, it is still not a prevalent tool in most industrial application domains\n[19, 35]. Given that AR is capable of providing an interactive interface displaying\ndigital content and offering ways to interact with information in a multimodal\nand collaborative manner [36], it has great potential to fill the gap between de-\nvelopers and IPT domain users by inciting novel mechanisms for onsite analysis.\nHence, how to develop effective AR tools or systems which provide users with\nstraightforward and comprehensible visualizations pertaining to the IPT process\ndata becomes the central topic.\nVisualization plays a dominant role in IPT-controlled industrial processes in\nthat it can display the relevant data in an observation-friendly manner, which is\nthenprovidedtodomainusers. Theinformationrelatedtothedesignatedprocess,\nfor example, a 2D graph of the tomographic spectrum or a 3D reconstruction of\nthe voxel data, if collected and visualized instantly in a reasonable way, can help\nin better understanding of the process among the users [37]. Sophisticated data\nanalysis always demands efficient reproduction of the measurement regarding the\nimaging process by IPT to uncover the material distribution inside the closed\ncontainers where the industrial process is executed [38]. As mentioned before,\nAR can provide functional tools that support users undertaking domain-related\ntasks, especially facilitating them in data visualization and interaction because",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 20
        }
    },
    {
        "page_content": "8 2 Background\nof its ability to jointly augment the physical space and the user\u2019s perception [28,\n37]. Based on the interactivity and accessibility which AR can supply, we aim to\nexplore the deeper usage of this technique in the IPT domain to support process\ndata visualization and analysis.\nIn this stage, we introduce a novel and preliminary AR framework as well\nas the mobile realization to support interactive and collaborative onsite analysis\nregarding volumetric visualization in IPT. In our AR system, we are capable of\nprovidinguserswithaninteractivewaywithhighmobilitytoconductvisualization\nanalysis alongside the ongoing industrial process. The necessary information is\nvisualized in our mobile AR App which was built on iOS devices as an early-stage\nsetup. Moreover, the proposed AR system incorporates multimodal visualizations\nused for characterizing the MWT processes, which opens the horizon of in-situ\nmutual collaboration of IPT related users.\n2.3 The Complete AR Approach with Evaluation for\nIPT\nAs a representative human computer interaction technology, AR consolidates the\ninterplay between digital objects and human subjects with much immersion based\non a powerful and interactive AR visualization modality [39\u201341]. Such usage has\nstill not become mainstream in most industries but a moderate number of re-\nsearch projects have successfully demonstrated its value in some cases [19]. With\nthe rapid development of hardware technology, head-mounted display (HMD) AR\nhas become widespread in many contexts since it enables interaction between dig-\nital information and human hands [42]. Under this circumstance, manipulation\ntasks that require interaction with the virtual objects can be achieved more ac-\ncurately and quickly than when using conventional tangible interfaces [43, 44].\nOptical see-through (OST) HMDs occupy the majority of contemporary AR con-\ntexts due to their ability to use optical elements to superimpose the real world\nwith virtual augmentation [45]. This differs from video see-though AR where OST\nAR does not bear a component to process captured camera imagery [46]. With\nthe ability of displaying manifold data in a more perceivable and interactive way\n[47, 48], applying AR to generate interactive visualizations for user-oriented tasks\nisoftenthepreferredchoice, andnotonlyforindustrialapplications[49\u201353]. More\nspecifically, it has been proven that OST AR can insert the virtual information\ninto the physical environment with visual enhancement for human observers to\ncontrol, thus improving the performance of information-dense domain tasks and\nfurther adding to its appeal to industrial practitioners [54, 55].\nAfter the framework conceptualization and initial mobile AR realization, we\npropose the complete AR approach with practical user evaluation which (Figure\n2.3.band 2.3. c) allows users to easily observe and manipulate the IPT visualiza-",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 21
        }
    },
    {
        "page_content": "9\nFigure 2.3: Conceptualization of our proposed AR approach for IPT visualization\nanalysis. a): The specialized IPT controlled industrial process is implemented in\na confined environment. b): The IPT data visualizations originate from different\nprocesses. c): Users engage with relevant data analysis. d): User equipped with\nproposed AR environment by an OST HMD interacting with the visualizations for\nfurther analysis.\ntions with high immersion [56]. We propose deploying AR/MR applications to\ntackle the practical problems stemming from the specific area (interactive IPT\nvisualization analysis). The main advantage of the proposed methodology is that\nit initiates, to our knowledge, the first mechanism for furnishing IPT users with\nimmersive OST HMD AR with comparative evaluation to communicate with in-\nformative visualizations, using their hands for better comprehension of the des-\nignated industrial processes. The AR system employs Microsoft HoloLens 2, one\nof the representative OST HMDs (Figure 2.3. d), as the fundamental supplying\nequipment to create the AR/MR environments. The source data derived from\nthe IPT supported industrial processes is always formidable to understand, so\nit needs straightforward and precise patterns to be visualized and interpreted.\nOur proposed approach provides a systematic framework which adopts accurate\n2D/3D IPT data visualizations that are further used for interactive analysis to\nprovide better understandability for domain users. We carried out a comparative\nstudy to demonstrate the superiority of our AR approach on bringing interactive\nvisualization surroundings as well as eliciting better contextual awareness. We en-\nvision our AR approach to benefit areas where users deal with IPT visualization\nanalysis.\n2.4 Supporting AR Visual Search Tasks\nApart from the industrial domain, especially IPT, AR is endorsed and applied in\nfacilitating task performance due to its capability of providing real-time interac-\ntion, as well as generating interactive interfaces of visualized digital content [57,",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 22
        }
    },
    {
        "page_content": "10 2 Background\n58]. Gaze assistance, in particular, has a promising role in AR applications ow-\ning to its easy, natural, and fast way of involving people in virtual environments\n[59]. Gaze-assisted techniques and implementations have been demonstrated to\nefficiently support AR applications by providing better user experiences (UX) [60]\nand more accurate target selection techniques [61]. People\u2019s eye movements are\neasy to track, and gaze implicitly conveys what people are interested in. Thus,\neye-tracking approaches are becoming more pervasive in interactive AR devices.\nThe search task in AR is a widespread research area which has received consid-\nerable attention, while its representations can be diverse, including visual element\n- [62], real world object - [63], or general searching [64, 65]. As a means to make\nthe search process more accessible, it is even possible that HMD devices with the\nmost advanced AR technology could soon become as ubiquitous as smartphones\n[66].\nWhen performing particular tasks in the context of AR, hints can improve the\nultimate task performance [67]. The type of hint, acting as a central and core\ntool, can vary. A widely-used tool is the visual hint, which is a mixture of graph-\nical representations in AR with user interface (UI) actions associated with the\nphysical world [68]. This effectively provides spatial and temporal guidance in\nAR. Another well-adopted hint is the audio hint, which presented as voice com-\nmands or directives can swiftly and clearly aid the user. A number of researchers\nhave exploited audio hints for the purpose of informing, guiding, and directing\nwithin their designs [69, 70]. However, even though there is a substantial body\nof research into how visual and audio hints work in AR tasks, little has focused\non search tasks other than exploring the combination of visual and audio hints in\none AR framework.\nTask feedback also matters. At present, there is almost no substantial research\ninto this area. Task feedback is crucial for domain users to obtain better task\nperformance in many situations related to AR [71, 72]. Here, we explore the effect\noftaskfeedbackwhichisembodiedastheplaybackofthehumaneyegazerecorded\nduring the searching process. A vision-friendly colored dot smoothly following the\npath is used to display the eye trajectory (Figure 2.3. b). According to Vieira et\nal. [73], feedback has the potential to provide more specific information needed\nby the users in AR, which encouraged us to explore its potential influence in the\nsame context.\nIn this stage, a complete AR approach for search tasks is proposed, examining\nthe independent and the combined impact of visual and audio hints, and studying\ninstant post-task feedback through gaze assistance [74]. We specifically concen-\ntrated on a book-searching task case, which includes all the essential components\nof a visual search procedure: visual environment, a particular object (the target\nbook), and distractors (irrelevant books) [75].",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 23
        }
    },
    {
        "page_content": "3 Related Work\nThischapterdescribestherelevantresearchrelatedtothemainworkofthisthesis.\nSpecifically, it emphasizes the research status of harnessing AR/MR into IPT and\nhow AR can affect practical search task performance.\n3.1 AR Visualization with IPT\n3.1.1 Industrial AR\nThere is a body of research to reflect the growing interest of the deployment of\nAR in various industrial applications. Cardoso et al. [19] wrote a systematic\nsurvey paper revealing the present status of industrial AR, where they concluded\nthat most AR applications in the industry domain are incremental as they of-\nfer a different and more efficient way to convey information needed. Bruno et\nal. presented an AR application where industrial engineering data was visually\nsuperimposed on the real object that represents a spatial reference, where the\nexploration is more natural compared to traditional visualization software [76]. A\nrecently published paper by Noghabaei et al. [77] indicated that the utilization of\nAR in architecture, engineering, and the construction industry had a significant\nincrease from 2017 to 2018. More specifically, according to the powerful visualiza-\ntion functionality provided by AR, Lamas et al. [28] pointed out that AR could\nbe used to visualize the 2D location of products and tools as well as hidden instal-\nlation areas in the shipyard industry. Additionally, DePace et al. [29] found that\nAR can enhance a user\u2019s ability to understand the movement of mobile robots.\n3.1.2 Visualization for IPT\nOveradecadeago,Brunoetal. [76]developedanARapplicationnamedVTK4AR,\nfeaturing that functionality which uses AR to scientifically visualize industrial en-\ngineering data for potential manipulation of the dataset. The many requirements\nof applying AR within industrial applications have been summarized by Lorenz et\nal. [78] as they enumerated the user, technical, and environmental requirements.\nMourtzis et al. [52] proposed a methodology to visualize and display industrial\nproduction scheduling and data monitoring by using AR, empowering people to\nsupervise and interact with the incorporated components. In industrial manufac-\nturing, a comparative study conducted by Alves et al. [53], demonstrated that\n11",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 24
        }
    },
    {
        "page_content": "12 3 Related Work\nusers who replied on AR based visualizations obtained better results in physi-\ncal and mental demand in production assembly procedures. Even more specif-\nically, B\u00fcttner et al. [79] identified that using in-situ projection based spatial\nAR resulted in a significantly shorter task completion time in industrial manual\nassembly. They designed two assisting systems and proved that the projection\nbased AR outperformed HMD based AR in ease of use and helpfulness in their\ncontext. Avalle et al. [80] proposed an adaptive AR system to visualize the in-\ndustrial robotic faults through the HMD devices. They developed an adaptive\nmodality where virtual metaphors were used for evoking robot faults, obtaining\nhigh effectiveness in empowering users to recognize contextual faults in less time.\nSatkowskietal. [81]investigatedtheinfluenceofthephysicalenvironmentsonthe\n2D visualizations in AR, suggesting that the real world does not have a noticeable\nimpact on the perception of AR visualizations.\nEven though the volume of intersection research between AR and IPT is not\nsubstantial, other researchers have been investigating diverse pipelines of applying\nAR to generate interactive and effective visualizations for complex tomographic\nprocessdata. Datingbackto2001,Mannetal. [82]exploitedanARapplicationto\nvisualize the solid-fluid mixture in a 5D way in stirred chemical reactors operated\nby one breed of IPT\u2013electrical resistivity tomography (ERT). Recently, a new\nAR solution to visualize IPT data and further lead to collaborative analysis in\nremote locations was proposed by Nowak et al. [16]. More interestingly, their\nteam also explored a more in-depth AR system with a more advanced prototype\nwhich created an entire 3D environment for users to interact with the information\nvisualizations characterizing the workflow of IPT with high immersion [58]. The\nOST HMD AR was satisfactorily adopted in every experiment they conducted.\nFurthermore, Zhang et al. formulated a new system to generalize IPT within the\ncontext of AR, and directed a study where a novel AR framework was developed\nto support volumetric visualization of IPT, especially to yield high mobility for\nusers [83].\n3.2 AR for Visual Search Tasks\nSearch tasks are becoming more common in using AR because of the capability\nof projecting additional virtual information onto the physical environment. Con-\ntreras et al. [63] presented a mobile application with AR encapsulated to enable\nusers to search for desired places, people, or events on a university campus. The\nsuperiority of AR lay in the fact that it offered certain visual elements that helped\nusers to better locate the required result. Rafiq and his colleagues [84] proposed a\ndynamic AR framework to support an online book-searching task by using mobile\naugmented data. This framework also introduced a security layer which ensured\nthe protection of sensitive cloud data. Gebhardt et al. [85] utilized gaze move-\nment data to observe the MR object\u2019s label in a visual searching process through",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 25
        }
    },
    {
        "page_content": "13\na reinforcement learning method. Trepkowski et al. [86] presented a series of\nsimulating experiments to investigate how visual search performance is affected\nby the field of view and information density in AR, indicating that the significant\neffect was aroused by these two factors. Van Dam et al. [87] studied the cues in a\ndrone-based AR signal detection task but found no significant differences across\nAR cue types. Nevertheless, the effect of hints and task feedback continues to be\nmore valued in AR visual searching.\n3.2.1 AR with gaze assistance\nSome research has already pioneered gaze-assisted UI to access more contextual\ninformation [88\u201391]. It has been shown that the optical see-through HMD which\nharnesses human gaze with eye-tracking as the interaction metaphor can con-\ntribute to efficient results [92]. Interestingly, there are some studies exploring eye-\ntracking to present menus to aircraft pilots, and to adjust their contents based\non what the pilot is looking at [93]. In 2005, Curatu et al. [94] proposed a\nnovel conceptualized system adding eye-tracking capabilities to a Head-Mounted\nProjection Display (HMPD), which was satisfyingly performed from a low-level\noptical configuration. Three years later, Park et al. [59] pioneered a system which\nincludes a Wearable Augmented Reality System (WARS) to examine their propo-\nsition on an experiment in which they selected desirable items in an AR gallery\nwith content mobility. Rivu et al. [66] successfully demonstrated the superiority\nof eye-tracking, showing that users are more positive in concentrating on their\nongoing conversations when there is more gaze interactivity within AR environ-\nments. All the relevant research proved that the gaze assistance in AR activates\nimproved perception.\n3.2.2 AR with hints and feedback\nNumerous researchers have endeavored to bring different modalities of hints into\nAR/XR systems as auxiliary tools for reaching desired results. Of these, visual\nhints [68, 95\u201398] and audio hints [69, 97, 99\u2013101] are the two most common repre-\nsentationsused. Arboledaetal. [102]presentedaugmentedvisualhintsinarobot-\ninvolvedARsystemwhichaimedtoenhancethevisualspaceoftherobotoperator\nabout the position of the robot gripper in the workspace, where the visual hints\nwere used to improve distance perception and then the manipulation and grasping\ntask performance. For tangible AR, White et al. [68] examined visual hints to\nenable discovery, learning, and completion of gestural interaction in a tangible\nenvironment. Seven visual hint types were generated: text, diagram, ghost, ani-\nmation, ghost+animation, ghost+text, and ghost+text+animation. Two decades\nago, Sawhney et al. [70] harnessed audio information to keep users of wearable\ndevices updated with incoming messages and events. Lyons et al. [69] developed",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 26
        }
    },
    {
        "page_content": "14 3 Related Work\nan AR game system named \"Guided by Voices\", which equipped the user with\na narrative sound clip that indicated the scenarios encountered and the correct\nsteps to be taken for proceeding. Interestingly and more recently, Mulloni et al.\n[103] found that AR can be integrated with a mobile navigation system, while\naudio hints can be advisable for eye-free usage. Cidota et al. [104] compared the\neffects of automatic visual and audio notifications regarding workspace awareness\nin AR remote collaboration.\nAlthough task feedback evaluation in AR has received little attention [105, 106]\nin the past years, a recent study carried out by Liu et al. [107] evaluated the\ninfluence of real-time task feedback in mobile handheld AR, which suggested that\nsignificant benefits will emerge if task feedback is engaged during the AR task.\nZahiri et al. [108] studied real-time feedback in AR for supporting a surgical train-\ning, finding that most of the users preferred receiving the feedback while the task\nwas being performed. Murakami et al. [109] found that haptic feedback as task\nevaluation can help users perform more effectively in a wearable AR system for\nvirtual assembly tasks. Clemente et al. [72] demonstrated that continuous visual\nAR feedback can deliver effective information for the users in their sensorimotor\ncontrol with a robotic hand, which has implications for amputee assistance in a\nclinical scenario. Even though some researchers have identified the effects of in-\nstant post-task feedback in gaze-interaction embodied virtual reality (VR) [110],\nits combination with AR is still scarce.",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 27
        }
    },
    {
        "page_content": "4 Problem Solving\nThis chapter gives a detailed description of the work done originated from the\nannexed papers, including the methodologies, experiment designs, and results\nobtained. As mentioned before, the main contribution of this thesis is first to\nbring the AR technique to the specialized IPT-controlled processes from zero to\none, and then investigate the impact of AR in practical search task performing.\nFor need-finding, we designed a survey study and conducted a qualitative analysis\nof the results. During the latter AR framework development and mobile AR\nimplementation, the advantages of the AR method were identified and followed\nbythecompleteARapproachbyOSTHMDimplementationwithawithin-subject\nuser evaluation. For the practical search task, two modalities of AR hints and\ngaze-assisted task feedback were deployed within the OST HMD.\n4.1 Need-finding: Is AR Ready for IPT?\nPaper I aimed to identify the gap between AR and its successful deployment.\nMore specifically, we sought to obtain the link between interactive visualization\nof AR and the effective working performance of the experts.\n4.1.1 Method\nTo implement this proposition, we designed a survey study which included a\nnumber of objective questions and several subjective narration parts (25 questions\nin total), divided into three sections: a general demographic section which\nincluded five questions (age, gender, country of residence, place of work -\nindustryoracademia, andsizeoforganization); avisualizationintomography\nsectionwithtwelvequestions , (topicsincludingyearsofexperience, modalities\nof IPT and visualization techniques used in work along with their relevance and\nimportance, and problems regarding visualization during daily work); and a final\npart regarding AR with eight questions , (topics including knowledge and\nexperience with AR, recently involved projects, and possible usage with potential\nadvantages and drawbacks of AR in the industrial tomography domain). In the\nfinal section, the three AR settings: HMD, HHD, and SAR were presented to the\nparticipants. All collected responses were anonymous.\nWeinvited14participantsfromourEurope-wideproject( https://www.tomocon.\neu/) network to complete the survey. The working statuses of the participants\n15",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 28
        }
    },
    {
        "page_content": "16 4 Problem Solving\nFigure 4.1: The answers of how domain experts responded to the current AR status.\nA: Q: Comfort: Which AR scenario shown above looks most comfortable for you? B:\nQ: Usability: Which AR scenario shown above looks most usable for you (might help\nwith your work)? C: Q: Are you aware of any ongoing AR applications in industrial\ntomography?\nwere categorized into working in academia with tight collaboration of industry,\nworking in industry, and working in both academia and industry. All were ei-\nther experienced researchers or industrial practitioners distributed across Europe.\nFurthermore, all had distinguished skills in industrial tomography for at least four\nyears while some were outstanding researchers in this domain. To mitigate un-\ndesired biases as well as to make our study more robust, we carried out a small\npilot study followed by the formal study.\n4.1.2 Results\nRegarding the potential use of AR, even though most domain experts engaged in\nour study did not have much hands-on experience of working with AR, they still\nheld a positive attitude regarding its potential usage in the context of industrial\ntomography after being exposed to the three AR modalities. They described the\nenvisioned applications of AR from their own perspectives. We summarized their\nanswers according to three aspects.\nHigh accessibility. The majority of participants acknowledged the superior\naccessibility of AR for aiding domain experts. According to one expert involved\nin the study, AR, as a mobile tool, can make people concentrate on the targets\nmore directly: \"It is very useful to have relevant information directly at the object\nof interest.\" . The accessible AR supports experts in different manners.\nSee-through capability. Simultaneously, these experts spotted the huge po-\ntential of utilizing AR within industrial tomographic processes. AR might become\none effective tool to support task resolution. One expert came up with a comment\nregarding specific application cases: \"Especially if you deal with the case where\nthere are opaque fluids which you cannot see, AR could be used as a powerful tool\nfor interactive visualization.\" .\n3D visualization ability. Another significant potential of adopting AR is\nits 3D visualization ability based on a few participants. The interactive 3D visu-",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 29
        }
    },
    {
        "page_content": "17\nalization was seen to help with more precise task analytics.\nFrom the responses obtained from these experienced domain experts, we looked\ninto deploying the capability of AR in industrial tomography. The participants\nwho were in favor of AR-tomography interplay claimed specific applications in\nterms of their hands-on projects. Most of them believed that AR could be unex-\npectedly beneficial for domain tasks pertaining to interactive visualization.\nFor the challenges to apply AR in industrial tomography, although most partic-\nipants gave positive responses regarding its potential, we have to realize that AR\nis still not prevalent in most industries [19]. To uncover how to apply AR appro-\npriately in industrial tomography as well as to understand its usability, we probed\nthe latent challenges according to the participants\u2019 descriptions. These findings\nshould be fully explored to situate AR properly within industrial tomography.\nEase of use. The most common challenge they posed was ease of use.\nThrough the survey, participants gained a fundamental conception of AR but\nmostly questioned its ease of use. For example, some experts were reluctant to\nwear HMD AR for long-time inspection of a specialized process which would cre-\nate unacceptable strain on human bodies: \"Yes, I think the headset is too difficult\nto wear for the industrial processes which have longer duration.\" ;In many large\nreactors, the projection is possible. But for AR using head mounted display or\nother devices, it is difficult for hands-on operations and long time use.\" .\nSpeed and accuracy. Participants suggested that the speed and accuracy of\nAR may become a core problem in many application cases. One expert posed a\nmore concrete appraisal: \"Industrial tomographic processes are often faster than\nhuman interaction and need automated control.\" . It is challenging for AR to make\nhuman interaction achieve the pace of industrial processes or to properly visualize\nvital information in the form of virtual elements.\nConvertibility. AR is still rarely used in industrial tomography due to its\nunestablished reputation for suitably converting domain-related data into desir-\nable visualizations. One researcher dealing with massive hands-on tomographic\ntasks remarked: \"From my point of view, converting the output from the tomo-\ngraphic sensor to properly represent output (i.e., moisture distribution) by AR can\nbe challenging.\" . We have to realize that most domain experts lack the special-\nized knowledge of using AR devices, which is indispensable when adopting AR\nfor visualization.\n4.2 AR Framework and Mobile AR Development\nPaper II and paper III concentrated on the theoretical AR framework conceptu-\nalization and actual mobile AR implementation based on the insights obtained\nfrom the need-finding study. Given that the IPT-controlled process in these two\npapers was an MWT for microwave drying which includes three phases\u2013time-\nreversal imaging, post-imaging segmentation, and volumetric visualization, our",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 30
        }
    },
    {
        "page_content": "18 4 Problem Solving\nFigure 4.2: The conceptualization framework of the proposed AR system is: the\nMWT-controlled industrial microwave drying process block, whichcovers the whole\nrunning procedure of the process and provides data and elements used for analysis;\nusers\u2019 block, representing the people involved in this context, such as operators who\ncontrol and run the process and researchers who observe and analyze the process; the\nAR app block, which entails the AR implementation of a mobile AR app developed on\nthe iOS/Android platform at the initial stage. [83, 111].\nmobile AR app has incorporated the data from all of the three phases.\n4.2.1 Method\nTo better support the onsite analysis related to the informative data processing\nand visualizing workflow, we present an AR framework which enhances the in-\nteractivity and immediacy of IPT data analysis. The proposed system has three\nmain components, as displayed in Figure 4.2.\n\u2022MWT-controlled industrial microwave drying process: A unique heating and\ndrying process operated in a confined chamber with sophisticated industrial\nsettings. In our study, the target is a microwave drying process for polymer\nfoams undergone by the precise HEPHAISTOS [17] microwave oven system.",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 31
        }
    },
    {
        "page_content": "19\n\u2022Users: Operators who control and run the drying equipment or researchers\nwho take onsite observations and collect data for further analysis.\n\u2022AR app: The core part of our proposed system. As a preliminary stage,\nthe application is manifested as a mobile app run in iOS/Android mobile\ndevices and used for interactive and collaborative volumetric visualization\nand analysis. A multiple floating interface provided and overlaid in a real\nenvironment and containing information from our proposed data workflow is\nthe main component of the application.\nThe principal part of the initiated AR system is a mobile app developed to visu-\nalize the necessary information revealed from the aforementioned data processing\nandvisualizingpipeline, whileitsupportsinteractiveandcollaborativeonsitedata\nanalysis. Thisappservesasanintermediarytoolbetweentheusersandtheprecise\nexperimental equipment used for displacing the visualized information regarding\nthe ongoing process. The users, for example, operators, can interact with the\nvisualizations projected on the AR interface, with the mobility of merely holding\na mobile device alongside the bulky equipment. As Figure 4.3 illustrates, people\nfrom different communities, such as practical operators or academic researchers,\nhave the capacity to interactively and instantly observe the information related to\nthe process by simply activating the app without initiating other facilities. The\napp was developed via Unity and Vuforia engines, aiming at the present stage\nfor iOS/Android portable devices such as smartphones and tablets. The app it-\nself provides on-site multiple interfaces containing various visualizations obtained\nfrom the data pipeline. Users are able to switch to the visualizations they require\nfor the desired analyses. As shown in Figure 4.3, some different floating interfaces\nincorporating different modalities of visualizations are presented and supported\nby switching after activating the app. The mobility of the AR interface provides\nusers with ongoing visualizations as they move around the equipment that runs\nthe designated process.\n4.2.2 Results\nThe first concern was how to present a complete data processing pipeline with\ninformative visualizations where these modalities are incorporated by the second\nfocus \u2013 a preliminary AR framework to facilitate interactive and collaborative\nonsite data analysis regarding volumetric visualization in IPT systems. As a\npioneerstudyinthisrelatedcontext, ourproposedARsystemopensanewhorizon\nof deploying this interactive technique within IPT, boosting the complicated data\nprocessing and visualizing with an MWT-controlled microwave drying process.\nBased on our study, we present the following findings:\nInteractivity. The most prominent advantage of this system is that the in-\nformation visualization by the AR interface is completely interactive. By using",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 32
        }
    },
    {
        "page_content": "20 4 Problem Solving\nFigure 4.3: The realization of the proposed AR system. Users hold the handheld\nsmartphones/tablets to run the AR app alongside the industrial setting. After the app\nactivation, users can then observe the visualizations displayed on the AR interfaces.\nAt present, the app is supported via iOS/Android mobile devices. Users are empowered\nto switch to different visualizations regarding different phases of the process. The\nvisualizations shown on the interfaces are multimodal, including the infrared image\nshowing the condition of the process and the 3D/2D reconstruction of the MWT images\n(the upper left small figure on the floating interface in the upper right subfigure); the\nreconstructed MWT images with their segmented results to show the high moisture\nareas (the middle right subfigure); and the volumetric visualization (the bottom right\nsubfigure) [111].\nthe AR app after activation, a user can easily get instant access to the visualized\ndata by observing the interface which incorporates the information needed for\ndecision-making. Our AR system provides a new perspective of interactive data\nanalysis to benefit IPT related users.\nMobility. Another strength of this system is its mobility. Generally, control-\nling and supervising an IPT related process requires implementation in specific\nand precise equipment and scientific laboratories. Researchers who observe the\ndata and analyze the ongoing process invariably conduct this work in dedicated\ncomputers located at different places. However, with this system they are empow-\nered to hold a portable mobile device (a smartphone or tablet) with a running",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 33
        }
    },
    {
        "page_content": "21\napp to closely implement the data analysis while standing next to the precise\nequipment.\nInformation richness. The AR interface is used for displaying the most sig-\nnificant information needed for boosting the decision-making of the industrial\nprocess. We report the first AR application for bringing a complete data pro-\ncessing and visualizing workflow related to complex IPT process which contains\nthe key information used for further analysis. For instance, a generated MWT\nimage using a time-reversal technique is able to show the precise moisture distri-\nbution, while the up-to-date volumetric visualization has the capability to reveal\nthe parameters of the drying material.\nMutual collaboration. Our system enables in-situ collaboration by provid-\ning a number of virtual AR interfaces comprising different information, includ-\ning all the sophisticated visualizations emerging from the workflow. Under this\nsetting, users from different communities such as hands-on operators, academic\nresearchers, and engineers, can independently conduct visualization analysis by\nobserving the corresponding visualizations, and interact with each other on com-\nmon problems as well.\n4.3 The Complete AR Approach\nIt is critical for users who intend to deal with IPT related analysis, regardless\nof their expertise, to fully understand the contextual information for in-depth\nvisualization analysis. Paper IV advanced the whole AR pipeline and proposed\na complete AR approach by the OST HMD for interacting with IPT data and\ntackling visualization analysis. A comparative user study was also implemented\nfor evaluation.\n4.3.1 Method\nThe contextual data used in this study was acquired from another microwave\ndrying process for polymer materials monitored by MWT. This imaging modal-\nity was applied to detect the moisture levels and distribution of the polymer\nmaterial through specific tomography imaging algorithms [112]. Three different\nvisualizations were employed in this study from dissimilar MWT drying processes,\nincluding two in 3D and one in 2D; as 3D figures incorporate more information\nregarding the polymer materials used in the process. Different moisture levels\nwere rendered with distinct colors and marked with different letters, denoting low\nmoisture area, moderate moisture, the dry part, and the high moisture area re-\nspectively. The annotations were created based on the physical understanding by\na domain expert.\nA user study was conducted to find out whether our proposed AR approach,\noffering interactive and immersive experience for communicating with IPT visual-",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 34
        }
    },
    {
        "page_content": "22 4 Problem Solving\nizations, was superior to customary in-situ data analysis pertaining to supporting\nusers\u2019 understandability and user experience. The main realization of this study\nwas two different scenarios where the engaged participants encountered two dif-\nferent environments; our proposed AR approach to interact with the data rep-\nresentations with the aid of OST HMD; and the conventional setting using an\nordinary computer with 2D screen and Matlab (only the visualization window\nwas utilized for the experiment). Three tasks were designed towards IPT con-\ntextual visualization analysis for the participants to implement. We selected the\nwithin-subjects principle with the counterbalancing design principle [113]. The\nfollowing hypotheses were made:\n\u2022H1:Our proposed AR approach can obtain better understandability for\nusers compared to using the conventional setting.\n\u2022H2:Our proposed AR approach can contribute to a lower task completion\ntime and fewer task errors.\n\u2022H3:Our proposed AR approach has better usability for users to interact\nwith IPT data visualizations.\n\u2022H4:Our proposed AR approach has a greater recommendation level than\nthe conventional setting.\nTwenty participants (12 male, 8 female) aged between 24 and 41 ( M= 30.1,\nSD= 4.78) were recruited by e-mail and personal invitations at a local university.\nAll participants signed an informed consent at the beginning of each session, being\nawareofthattherewouldbenopersonalinformationcollectedandthattheycould\nquit the study at any time. They were also told that the sessions would be audio-\nrecorded, but all recordings would be treated confidentially and references would\nonly be made in a purely anonymized form for scientific analysis. The participants\nhad sufficient time to read the consent form and ask questions before signing it.\nDue to the complexity of IPT, the data generated is usually difficult to interpret\nby outsiders, even some of the experts. Hence, we started our study with a concise\nbut detailed introduction about the study background, including the fundamental\nschematic of IPT, the source of the data visualizations, and the basic information\nabout the visualizations used in this study. Before we began with the actual\nstudy sessions, we calibrated the Microsoft HoloLens glasses for each participant\nby helping them follow the instructions from the built-in calibration functional\nmodule. A short pre-training session was implemented to get the participants\nfamiliarized with and adapted to the HMD and the AR application used. The\nentire procedure of the user study is displayed in Figure 4.4.",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 35
        }
    },
    {
        "page_content": "23\nFigure 4.4: Flowchart of our user study with counterbalancing design.\n4.3.2 Results\nQualitative Results : We compiled the results collected from the post inter-\nview sessions and real-time feedback of the participants during the study. The\nqualitative measurements generalized by a thematic analysis are presented in this\nsection. The real-time feedback was conveyed spontaneously by the participants\nand recorded by the authors. For the post study interview, we asked the partic-\nipants several subjective questions regarding what they liked and disliked about\nthe tasks regarding our AR approach as well as the comparison between the two\ntested environmental settings. Concerning privacy and anonymity, we denoted\nthe participants as P1 to P16 when recording the real-time feedback and encod-\ning the interview answers. Based on the codes, we derived three themes: in-task\nunderstandability, interaction-based usability, and user experience.\nQuantitative Results : In the following we present our quantitative measure-\nments in relation to our hypotheses. The independent variables were identified\nas the two different environmental settings. From the study process and post-\nstudy questionnaire, five metrics \u2013 TCT, understandablity, error rate, usability,\nand recommendation level \u2013 were employed as dependent variables. The TCT\nwas merely measured from Understand-n-Select since this task is representative\nfor practical IPT visualization analysis. To test the understandability, a 7-point\nLikert scale was adopted for participants to rate the level of the two environmental\nsettings helping with understanding complex IPT data. The error was exclusively\ncollected from the third task when users were supposed to select the designated\nareas. The largest number of errors was four in each environmental setting. Every\nerror, if one occurred, was noted during the study and the error rate was thereby\ncalculated by the authors. The system usability scale (SUS) [114] was harnessed\nto quantify the usability. We uniquely investigated the recommendation level by\nasking the participants the extent (the 7-point Likert scale) of recommending the\ntwoenvironmentalsettings. Westatisticallyanalyzedthecollectedresultstoiden-\ntify any significance among the four metrics evaluated. Normality was checked as\na prerequisite.",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 36
        }
    },
    {
        "page_content": "24 4 Problem Solving\nFigure 4.5: Statistical results between the AR approach and the conventional set-\nting. a: The distribution of the understandability levels (1 to 7) rated by participants\nbetween the AR approach and the conventional setting. b: The distribution of task\ncompletion time (s). c: The distribution of the error rate (0.00 to 1.00). d: The\ndistribution of the SUS scores representing usability (0 to 100). e: The distribution\nof the recommendation levels (1 to 7).\nWe conducted corresponding statistical analyses for those five metrics and sig-\nnificant differences were found in each of them as shown in Figure 4.5. Thus, we\nare confident to conclude that the four hypotheses have been well verified by the\nresults.\n4.4 Practical Visual Search Tasks\nIn paper V, to investigate how visaul/audio hints and task feedback of AR can\nmake a difference in facilitating the task performing for visual search tasks, we\nhypothesized:\n\u2022H5: Both visual and audio hints have a positive effect in facilitating AR\nsearching performance and decreasing cognitive workload in AR, while the\ncombination of these two hints has a greater effect than either does individ-\nually.\n\u2022H6: Instant post-task feedback has a positive effect for task performance\nand cognitive workload reduction in AR search tasks.\n4.4.1 Method\nOur proposed system, featuring an AR app used in OST HMD, is an interactive\ntool for users carrying out a book-searching task. The equipment kit consists of an\nOST HMD with built-in eye tracker which is responsible for recording the user\u2019s\ngaze during the task being conducted. There are two inter-correlated modules in\nourproposedsystem: thehintsmoduleandtheinstantpost-taskfeedbackmodule.\nThe first module was designed to address H5,while the second targeted H6. The\ncomponents \u2013 \"User\", \"The AR app\", and \"Task completion\" are commonly owned",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 37
        }
    },
    {
        "page_content": "25\nFigure 4.6: Study I ( a) and Study II ( b): The mean TCT ( s) used per task by the\nfour groups. Error bars show mean \u00b1standard error (SE).\nby the two modules. The AR app is the core of the system which bears the\nnecessary information needed including hints and task feedback.\nThe book-searching case was chosen and implemented with the aid of the pro-\nposed AR approach. The AR app encoded a number of book titles which were\nused as the stimulus in the study. The user with an OST HMD with a real-time\nbuilt in eye-tracker stood in front of the bookcase, where the task was to find a\nspecific but randomly determined book (Figure 2.3. a) after activating the app.\nWe chose arrows as the indicators, but showed them in an easily-recognizable\ncolor and in a static state. Since all virtual artifacts were displayed in the world\nspace, users\u2019 movements did not affect an object\u2019s position. As shown in Figure\n2.3.c, the bright-purple arrow visually helped the user to locate the target book.\nFor audio hints, users received an audio instruction about the approximate lo-\ncation of the targeted book. The auditory information was presented as a clear\nand simple instructive voice message, allowing the users to finish the task with\nhigher efficiency. The instant post-task feedback was provided as a visible play-\nback of gaze recording. As shown in Figure 2.3. b, the user\u2019s gaze was denoted as\na brightly colored dot following the trajectory. During the book-searching phase,\ngaze trajectories were marked and recorded. After one search task, users were\nable to watch their gaze playback of the trajectory, after which they proceeded to\nthe next task.\nA comprehensive comparative user study based on a between-subject with cou-\npling within-subject factors was then designed. The independent variables were\nthe tasks (within-subject) and the groups (between-subject). The study was com-\nprised of two sub-studies (Study I and II) where the participants from Study II\nreceived the task feedback while those from Study I did not. Before starting the\nexperiment, we conducted a pre-testing session where several people were invited\nto test the desired functionalities of the AR app. Some minor adjustments were\nthen made to improve the visual and locational clarity, including the visual hint\nmade into a more vision-friendly arrow and our testing environment moved to a\nmore spacious and bright function room. The participants (n=96) were randomly\nsorted into four different groups in both Study I and II (n=48 in each): control,",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 38
        }
    },
    {
        "page_content": "26 4 Problem Solving\nFigure 4.7: Study I: NASA TLX for the first task by the four groups. Error bars\nshow mean \u00b1standard error (SE).\nFigure 4.8: Study I: NASA TLX for the second task by the four groups. Error bars\nshow mean \u00b1standard error (SE).\naudio, visual, and the combined groups (n=12 in each).\n4.4.2 Results\nThe two metrics (dependent variables) employed for assessing the tasks were task\ncompletion time (TCT) and NASA Task Load Index (TLX) in both studies.\nTCT in Study I : To identify the statistically significant differences of the\nresults, a 2(task)*4(group) mixed analysis of variance (ANOVA)( p= 0.05) was\nperformed. This analysis was performed using IBM SPSS Statistics, as were\nthe following statistical measurements. Bonferroni-corrected post hoc tests were\nemployed to determine if the pairwise groups were significantly different. We\nfound that all participants did not have statistically significant shorter TCT in\nthe second task compared to the first, but there were significant main effects\nof the four groups on the TCT measured. No significant interaction was found\nbetweenthetasksandthegroups. TheBonferroniposthoctestsshowedstatistical\nsignificance between every pairwise comparison of the control group with all the\nother groups in both tasks.\nNASA TLX in Study I : The study showed that all participants did not\nhave a significantly different cognitive workload in the second task compared to",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 39
        }
    },
    {
        "page_content": "27\nFigure 4.9: Study II: NASA TLX for the first task by the four groups. Error bars\nshow mean \u00b1standard error (SE).\nFigure 4.10: Study II: NASA TLX for the second task by the four groups. Error\nbars show mean \u00b1standard error (SE).\nthe first. The post hoc test showed all three groups were significantly different\nwhen compared to the control group in the first task. In the second task, the\ncontrol group had statistical significance between the other three groups. Again,\nno significant differences between every pairwise comparison of audio, visual, and\ncombined groups were found in both tasks in Study I.\nTCT in Study II : All participants had statistically significantly shorter TCTs\nin the second task compared to the first. The results also showed significant main\neffectsofthefourgroupsonthemeasuredTCT.Inaddition, therewasasignificant\ninteraction between the tasks and the groups. The Bonferroni post hoc tests\nshowed statistically significant differences between every pairwise comparison of\nthe control group with all the other groups in the first task. Likewise, the second\ntask also showed significant differences between the control group and the other\nthree groups. However, we found that the pairs of the audio-combined and the\nvisual-combined had statistical significance as well. The TCT of the combined\ngroup was statistically different from the other three groups in the second task.\nNASA TLX in Study II : All participants had a statistically significantly\nlesser cognitive workload in the second task compared to the first. There were\nsignificant main effects of the NASA TLX results among the four groups, but\nno significant difference was found on the interaction between the tasks and the",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 40
        }
    },
    {
        "page_content": "28 4 Problem Solving\ngroups. For pairwise comparison, the post hoc test showed no significant differ-\nences between every pairwise comparison of audio, visual, and combined groups\nin the first task. However, all the other three groups were significantly different\nwhen compared to the control group. In the second task, the participants from\nthe control group still possessed significantly more cognitive workload compared\nto members of the other three. Nonetheless, the pairs of the audio-combined and\nthe visual-combined also revealed significant differences. The result means the\nsynthesis of the two modalities of the hints we tested worked better than when\nthere was only one modal hint in the second task, where the gaze playback was\nprovided as the task feedback in the same context.",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 41
        }
    },
    {
        "page_content": "5 Discussions\nIn this chapter, the insights obtained with the limitation identified regarding the\nfour stages are described.\n5.1 Need-finding\nAR has become a renowned concept and technique with the rapid development\nof mobile smart devices especially the successful usage in industrial applications.\nDistinct from [77] (specialized in understanding VR/AR in design and construc-\ntion) and [19] (specialized in evaluating AR in industries), we specifically focused\non exploring AR in the domain of industrial tomography to resolve research ques-\ntion I.\nWe found that for IPT experts, visualization is a critical inclusion of their daily\nwork, especially 3D visualization which triggered the most difficulties. We sought\nto dig out more about the prospects, especially in terms of data-driven and ma-\nchine learning-driven applications [115]. According to our surveyed participants,\nHMD and HHD AR were considered to be favorable modalities which were able to\nbring comfort and effectiveness to benefit industrial tomography. Notably, almost\nall of our participants expressed a positive attitude towards the potential usage\nof AR due to its promising functionality such as interactivity and providing an\nextra platform for 3D visualization.\nAt this stage, there still exist limitations of our study. One limitation is that\nour study was done exclusively from the perspectives of a specific group of experts\nin IPT. Furthermore, we surveyed 14 Europe-wide participants, which is only a\nsmall sample of the domain practitioners. A larger number of samples would\ncontribute more rigorous and persuasive conclusions. Last but not least, we did\nnot control all the influential factors of the survey, such as the time length of the\nsurvey, which might affect the answers obtained.\n5.2 AR Framework and Mobile Implementation\nTo tackle research questions II and III, we entered the second stage of this thesis.\nThe first concern of this stage is to present a complete data processing pipeline\nwith informative visualizations, while these modalities are incorporated by the\n29",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 42
        }
    },
    {
        "page_content": "30 5 Discussions\nsecond concern \u2013 a preliminary AR framework to facilitate interactive and col-\nlaborative onsite data analysis regarding volumetric visualization in IPT systems.\nAs a pioneer study in this context, our proposed AR system, especially the de-\nveloped mobile AR app as the third concern, opens a new horizon of deploying\nthis interactive technique into IPT, boosting the complicated data processing and\nvisualizing with an MWT-controlled microwave drying process.\nWe noticed that there are some deficiencies in this preliminary AR framework\nsupporting the visualization of data workflow. Foremost, the dataflow itself does\nnot include all the necessary information related to the MWT-controlled process.\nMoreover, although we have set a precedent for bringing the AR technique to\nbenefit IPT especially in complex visualization analysis, we have not yet evaluated\nour method through a systematic user study to gain constructive and critical\nfeedback. Inaddition, our preliminaryAR framework is embodiedas a mobileapp\non portable smartphones or tablets. It limits the AR interface to small, bounded\nscreens. TheformulationofARinhandheldmobiledevicestosomeextenthinders\nthe essential advantages of AR, which should be tackled by introducing more\nadvanced headsets that can make AR scenes more interactive and immersive.\n5.3 The Complete AR Approach with Evaluation\nWe then developed the complete AR approach with OST HMD for IPT data\nvisualizations to fill the gap between AR technique and the IPT context to an-\nswer research question IV. The practical realization was achieved by an AR app\nsupported in Microsoft HoloLens 2; a typical OST HMD which provides immer-\nsive interaction for users to communicate with the multiple visualizations. To\nevaluate our proof-of-concept, a within-subject user study with counterbalancing\ndesign was implemented based on 20 recruited participants in order to justify the\nhypothetical superiority of the proposed AR approach compared to conventional\ncomputers with 2D screen for IPT data visualization analysis. Our approach of-\nfers a complete conceptualization with the realization of an immersive AR method\nwith the aid of OST HMD towards IPT. Our participants were highly favorable\ntowards its interactive features. While no previous work emphasized this, we\nstructured a systematic AR framework with high interaction capabilities for IPT\nusers.\nAnother important property to be realized is the ubiquity and knowledge trans-\nferability of the proposed AR approach. As most of the participants had little\nexperienceinIPT,wearethereforeencouragedaboutthepotentialforourmethod\nto bring outsiders to this specialized technique for domain supporting. Addition-\nally, although the data used in this study was generated merely from MWT,\nthe diverse genres of IPT have high transferability since they comply with similar\nmechanisms [83]. It is fair to say that the superiority revealed in our AR approach\nis highly transferable to different genres of IPT.",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 43
        }
    },
    {
        "page_content": "31\nFor limitations at this stage, the virtual buttons for switching different figures\nin the AR application were reported as not sensitive enough. Also, although hand\ngestures in AR space are intuitive and highly close to what people commonly do\nin physical reality, some participants reported a tiny difficulty when the app was\ninitiated. In addition, we admit that the experiment itself and the contextual IPT\ndata were monotonous and small scale. Last but not least, the stimuli discussed in\nthis paper had only three visualizations derived from different industrial processes\nsupervised by the specific MWT, which lacks diversity. More manifold data from\nother genres of IPT could be examined to make the results more robust.\n5.4 Practical Visual Search Tasks\nThe final stage was intended to respond to research question V. What did we do\nand achieve in this stage? By sharing the same technique from previous stages\nand being inspired by it, we identified the effects of visual/audio hints in either\nsingle or combined setups, together with instant post-task feedback for AR search\ntasks. We developed an AR approach engaging two modules of the hints and\ngaze-assisted task feedback separately with the aid of Microsoft HoloLens 2. A\ncase study for a visual book-searching task was chosen. Then, a comprehensive\nuser study was designed, where 96 participants were allocated in Study I (n=48)\nand Study II (n=48) and placed randomly into four groups (control (n=12), audio\n(n=12), visual (n=12), and combined (n=12)) in each sub-study. The user study\nwas based on the between-subject principle with within-subject factor.\nOur proposed solution has proved to be advantageous for increasing AR search\ntask performance and simultaneously reducing the cognitive workload on users.\nThe noticeable decrease of TCT and the cognitive task workload was a confir-\nmation of our hypotheses. We also believe the research implication of our study\nhas high generalizability and convertibility to benefit the general AR searching\nspecialization.\nThe limitations should be admitted as well. First, we designed two visual\nbook-searchingtaskstobeperformedusingtwobookcasesthatcontaineddifferent\nbooks, eliminatingmemoryasafactorinthesecondtask. Thethicknessandorder\nof the books might alter the participants\u2019 search speeds and workload. Also,\nthe sample size of our study might not be sufficient. In addition, due to the\nperformance sequence, the gaze review only influenced the second task in our\nstudy. However, in order to better understand the influence of gaze review on\nsearchtasks, therecouldbeanotherdesignsolutionwithacounterbalancingfactor\n[113] where the gaze playback can be recorded from the second task then used for\nthe first task.",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 44
        }
    },
    {
        "page_content": "",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 45
        }
    },
    {
        "page_content": "6 Conclusion\nTo answer the question raised in our thesis title, we are able to say that AR can\nbe used to aid users for tackling domain tasks in both IPT and practice. To bring\nthe AR technique to the specialized IPT field yields a lot of difficulties. As a\nspecialized area, IPT has its particular and traditional mechanisms to deal with\ndomain tasks, where opportunities exist for cutting-edge AR to be imported to\nfill certain gaps. In this thesis, we developed and implemented usable stages to\nharness AR within IPT to benefit domain users.\nFirst, we investigated how IPT experts can effectively work with AR by means\nof modeling users through a systematic survey. We designed a logically-consistent\nsurvey study, recruited 14 domain experts with relevant disciplinary backgrounds,\nthen carried out both a pilot study and a formal study. We displayed the current\nstatus of the use of AR in IPT and explored the high potential of adopting AR\nmore in the future owing to the superiority of interactive visualization. Second,\nwe proposed an initial AR framework including a complete data processing and\nvisualizing workflow in an IPT system. The overall realization of this AR frame-\nwork was conducted by a mobile AR App run on smartphones/tablets, aiding a\nspecific MWT controlled microwave drying process. We successfully proved that\nthis novel system brings new conceptualization in this domain by offering interac-\ntivity and mobility on users\u2019 in-situ data analysis and visualizations. In the final\nstage of our industrial context, we proposed a complete AR approach with OST\nHMD for users to immersively interact with the specific IPT data visualizations\nfor contextual understanding and task performing. The increased understandabil-\nity, reduced TCT, lower error rate, greater usability, and higher recommendation\nlevel of the methodology have been reflected in our work. A within-subject user\nstudy demonstrated the superiority of the proposed AR approach over the current\nstandard IPT visualization analytical environment and proved that immersion in\n3D AR outperforms conventional 2D screen computers in enhancing contextual\nunderstanding and user experience.\nRegarding practical domain task performance, we have described an AR ap-\nproach for visual search tasks with supported visual/audio hints and gaze-assisted\ninstant post-task feedback. Based on our hypotheses, we designed and conducted\na case study of visual book-searching where the gaze playback acted as instant\npost-taskfeedback. Theexperimentalprocedureconsistedofacomprehensiveuser\nstudy (n=96) with two comparative sub-studies. The resulting analysis was based\nmainly on collected NASA TLX answers with TCT measurements as preliminary\n33",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 46
        }
    },
    {
        "page_content": "34 6 Conclusion\nanalytic metrics. We found that both visual and audio hints have a positive effect\nin facilitating task performance and the combination of the two hints works better\nthan either individually, under the condition that there is instant post-task feed-\nback. We pointed out the high generalizability and convertibility of our research\noutput in taking advantage of general AR searching processes.\nFor future work, we will firstly concentrate on upgrading the AR app by im-\nproving the touch sensitivity of the virtual buttons on the interface as well as\neliciting more early guidance for users to better start with the app. In addition,\nanother concern will be to increase the experimental diversity by, for instance, in-\nvolving a larger number of participants from different communities including both\nexperts and outsiders and adding more nonidentical experiments with distinct de-\nsign principles. Last but not least, integrating AR (HMD or mobile devices) with\nintelligent UIs manifesting various visualizations or visual analytics regarding the\nspecific IPT processes can broaden the horizon of the relevant stakeholders. Our\nwork benchmarks the intersection between OST HMD AR and the IPT domain to\nhighlight the future direction of bringing more related and advanced techniques\ninto different industrial scenarios. We hope our research will lead to better strate-\ngic design within this context and bring more interdisciplinary novelty to Industry\n4.0 [116].",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 47
        }
    },
    {
        "page_content": "References\n[1] I. Ismail, J. Gamio, S. A. Bukhari, and W. Yang, \u201cTomography for multi-\nphase flow measurement in the oil industry\u201d, Flow measurement and in-\nstrumentation 16, 145\u2013155 (2005) (cit. on p. 5).\n[2] H. Tapp, A. Peyton, E. Kemsley, and R. Wilson, \u201cChemical engineering\napplications of electrical process tomography\u201d, Sensors and Actuators B:\nChemical 92, 17\u201324 (2003) (cit. on p. 5).\n[3] G. Nolet, Seismic tomography: with applications in global seismology and\nexploration geophysics , Vol. 5 (Springer Science & Business Media, 2012)\n(cit. on p. 5).\n[4] K Primrose, \u201cApplication of process tomography in nuclear waste process-\ning\u201d, inIndustrial Tomography (Elsevier, 2015), pp. 713\u2013725 (cit. on p. 5).\n[5] A. Plaskowski, M. Beck, R Thorn, and T. Dyakowski, Imaging industrial\nflows: applications of electrical process tomography (CRC Press, 1995) (cit.\non p. 5).\n[6] G. Rao, S. Aghajanian, Y. Zhang, L. Jackowska-Strumi\u0142\u0142o, T. Koiranen,\nand M. Fjeld, \u201cMonitoring and Visualization of Crystallization Processes\nUsing Electrical Resistance Tomography: CaCO3 and Sucrose Crystalliza-\ntion Case Studies\u201d, Sensors 22, 4431 (2022) (cit. on p. 5).\n[7] U. Hampel, L. Babout, R. Banasiak, E. Schleicher, M. Soleimani, T. Won-\ndrak, M. Vauhkonen, T. L\u00e4hivaara, C. Tan, B. Hoyle, et al., \u201cA review\non fast tomographic imaging techniques and their potential application in\nindustrial process Control\u201d, Sensors 22, 2309 (2022) (cit. on p. 5).\n[8] M. S. Beck et al., Process tomography: principles, techniques and applica-\ntions(Butterworth-Heinemann, 2012) (cit. on p. 5).\n[9] J. Yao and M. Takei, \u201cApplication of process tomography to multiphase\nflow measurement in industrial and biomedical fields: A review\u201d, IEEE\nSensors Journal 17, 8196\u20138205 (2017) (cit. on p. 5).\n[10] Y. Zhang and M. Fjeld, \u201cCondition monitoring for confined industrial pro-\ncess based on infrared images by using deep neural network and variants\u201d,\nin Proceedings of the 2020 2nd International Conference on Image, Video\nand Signal Processing (2020), pp. 99\u2013106 (cit. on pp. 5, 6).\n35",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 48
        }
    },
    {
        "page_content": "36 6 References\n[11] Y. Zhang and M. Fjeld, \u201c\"I Am Told to Be Happy\": An Exploration of Deep\nLearning in Affective Colormaps in Industrial Tomography\u201d, in 2021 2nd\nInternational Conference on Artificial Intelligence and Information Systems\n(2021), pp. 1\u20135 (cit. on p. 5).\n[12] Y. Zhang, \u201cThe Magic of Vision: Understanding What Happens in the\nProcess\u201d, PhD thesis (Chalmers Tekniska Hogskola (Sweden), 2021) (cit.\non p. 5).\n[13] Y. Zhang, M. Fjeld, A. Said, and M. Fratarcangeli, \u201cTask-based Colormap\nDesign Supporting Visual Comprehension in Process Tomography\u201d, in Eu-\nroVis 2020 - Short Papers, edited by A. Kerren, C. Garth, and G. E. Marai\n(2020) (cit. on p. 5).\n[14] Y. Zhang, M. Fjeld, M. Fratarcangeli, A. Said, and S. Zhao, \u201cAffective\nColormap Design for Accurate Visual Comprehension in Industrial Tomog-\nraphy\u201d, Sensors 21, 4766 (2021) (cit. on p. 5).\n[15] W. Daily, A. Ramirez, A. Binley, and D. LeBrecque, \u201cElectrical resistance\ntomography\u201d, The Leading Edge 23, 438\u2013442 (2004) (cit. on p. 5).\n[16] A. Nowak, M. Wo\u017aniak, Z. Rowi\u0144ska, K. Grudzie\u0144, and A. Romanowski,\n\u201cTowards in-situ process tomography data processing using augmented re-\nality technology\u201d, in Adjunct Proceedings of the 2019 ACM International\nJoint Conference on Pervasive and Ubiquitous Computing and Proceedings\nofthe2019ACMInternationalSymposiumonWearableComputers(2019),\npp. 168\u2013171 (cit. on pp. 5, 12).\n[17] Y. Zhang, Y. Ma, A. Omrani, R. Yadav, M. Fjeld, and M. Fratarcangeli,\n\u201cAutomatic image segmentation for microwave tomography (mwt) from\nimplementation to comparative evaluation\u201d, in Proceedings of the 12th In-\nternational Symposium on Visual Information Communication and Inter-\naction (2019), pp. 1\u20132 (cit. on pp. 6, 18).\n[18] Y Zhang, Y Ma, A Omrani, R Yadav, and M Fjeld, \u201cAutomated microwave\ntomography (Mwt) image segmentation: State-of-the-art implementation\nand evaluation\u201d, Journal of WSCG 2020, 126\u2013136 (2020) (cit. on p. 6).\n[19] L. F. de Souza Cardoso, F. C. M. Q. Mariano, and E. R. Zorzal, \u201cA survey\nof industrial augmented reality\u201d, Computers & Industrial Engineering 139,\n106159 (2020) (cit. on pp. 6\u20138, 11, 17, 29).\n[20] R. T. Azuma, \u201cA survey of augmented reality\u201d, Presence: Teleoperators &\nVirtual Environments 6, 355\u2013385 (1997) (cit. on pp. 6, 7).\n[21] J. Leebmann, \u201cAn augmented reality system for earthquake disaster re-\nsponse\u201d, International Archives of the Photogrammetry, Remote Sensing\nand Spatial Information Sciences 34(2004) (cit. on p. 6).",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 49
        }
    },
    {
        "page_content": "37\n[22] X. Liu11, Y.-H. Sohn, and D.-W. Park, \u201cApplication development with\naugmented reality technique using Unity 3D and Vuforia\u201d, International\nJournal of Applied Engineering Research 13, 15068\u201315071 (2018) (cit. on\np. 6).\n[23] K. Tainaka, Y. Fujimoto, M. Kanbara, H. Kato, A. Moteki, K. Kuraki, K.\nOsamura, T. Yoshitake, and T. Fukuoka, \u201cGuideline and tool for design-\ning an assembly task support system using augmented reality\u201d, in 2020\nIEEE International Symposium on Mixed and Augmented Reality (IS-\nMAR) (IEEE, 2020), pp. 486\u2013497 (cit. on p. 6).\n[24] K. Pfeuffer, Y. Abdrabou, A. Esteves, R. Rivu, Y. Abdelrahman, S. Meit-\nner,A.Saadi,andF.Alt,\u201cARtention:Adesignspaceforgaze-adaptiveuser\ninterfaces in augmented reality\u201d, Computers & Graphics 95, 1\u201312 (2021)\n(cit. on p. 6).\n[25] R. Piening, K. Pfeuffer, A. Esteves, T. Mittermeier, S. Prange, P. Schr\u00f6der,\nand F. Alt, \u201cLooking for Info: Evaluation of Gaze Based Information Re-\ntrieval in Augmented Reality\u201d, in IFIP Conference on Human-Computer\nInteraction (Springer, 2021), pp. 544\u2013565 (cit. on p. 6).\n[26] J. Qian, J. Ma, X. Li, B. Attal, H. Lai, J. Tompkin, J. F. Hughes, and J.\nHuang,\u201cPortal-ble:Intuitivefree-handmanipulationinunboundedsmartphone-\nbased augmented reality\u201d, in Proceedings of the 32nd Annual ACM Sympo-\nsium on User Interface Software and Technology (2019), pp. 133\u2013145 (cit.\non p. 6).\n[27] D. Ma, J. Gausemeier, X. Fan, and M. Grafe, Virtual reality & augmented\nreality in industry (Springer, 2011) (cit. on p. 6).\n[28] P. Fraga-Lamas, T. M. Fern\u00e1ndez-Caram\u00e9s, \u00d3. Blanco-Novoa, and M. A.\nVilar-Montesinos,\u201cAreviewonindustrialaugmentedrealitysystemsforthe\nindustry 4.0 shipyard\u201d, Ieee Access 6, 13358\u201313375 (2018) (cit. on pp. 6, 8,\n11).\n[29] F. De Pace, F. Manuri, and A. Sanna, \u201cAugmented reality in industry 4.0\u201d,\nAm J ComptSci Inform Technol 6, 17 (2018) (cit. on pp. 6, 11).\n[30] J. Cook, PTC Technology Accelerates Watson-Marlow\u2019s Digital Transfor-\nmation Plans , (Feb. 2021) https://www.ptc.com/en/blogs/corporate/\nptc-technology-accelerates-watson-marlow-digital-transformation\n(cit. on p. 7).\n[31] M. R. Marner, R. T. Smith, J. A. Walsh, and B. H. Thomas, \u201cSpatial\nuser interfaces for large-scale projector-based augmented reality\u201d, IEEE\ncomputer graphics and applications 34, 74\u201382 (2014) (cit. on p. 7).",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 50
        }
    },
    {
        "page_content": "38 6 References\n[32] Y. Zhang, A. Nowak, A. Romanowski, and M. Fjeld, \u201cOn-site or Remote\nWorking?: An Initial Solution on How COVID-19 Pandemic May Impact\nAugmented Reality Users\u201d, in Proceedings of the 2022 International Con-\nference on Advanced Visual Interfaces (2022), pp. 1\u20133 (cit. on p. 6).\n[33] R. G. Boboc, F. G\u00eerbacia, and E. V. Butil\u0103, \u201cThe application of augmented\nreality in the automotive industry: A systematic literature review\u201d, Applied\nSciences10, 4259 (2020) (cit. on p. 6).\n[34] M. Billinghurst, A. Clark, G. Lee, et al., \u201cA Survey of Augmented Real-\nity\u201d, Foundations and Trends \u00aein Human\u2013Computer Interaction 8, 73\u2013272\n(2015) (cit. on p. 7).\n[35] C.-C. Teng, N. Jensen, T. Smith, T. Forbush, K. Fletcher, and M. Hoover,\n\u201cInteractive augmented live virtual reality streaming: a health care applica-\ntion\u201d, in Proceedings of the 2nd International Conference on Medical and\nHealth Informatics (2018), pp. 143\u2013147 (cit. on p. 7).\n[36] A. Ferrario, R. Weibel, and S. Feuerriegel, \u201cALEEDSA: Augmented Reality\nfor Interactive Machine Learning\u201d, in Extended Abstracts of the 2020 CHI\nConference on Human Factors in Computing Systems (2020), pp. 1\u20138 (cit.\non p. 7).\n[37] Y. Zhang, A. Nowak, G. Rao, A. Romanowski, and M. Fjeld, \u201cIs Industrial\nTomography Ready for Augmented Reality? A Need-finding Study of How\nAugmented Reality Can Be Adopted by Industrial Tomography Experts\u201d,\nin International Conference on Human-Computer Interaction (Springer,\n2023, in press) (cit. on pp. 7, 8).\n[38] K. Grudzien, \u201cVisualization system for large-scale silo flow monitoring\nbased on ECT technique\u201d, IEEE Sensors Journal 17, 8242\u20138250 (2017)\n(cit. on p. 7).\n[39] D. Kalkofen, E. Mendez, and D. Schmalstieg, \u201cComprehensible visualiza-\ntion for augmented reality\u201d, IEEE transactions on visualization and com-\nputer graphics 15, 193\u2013204 (2008) (cit. on p. 8).\n[40] M. Tonnis, C. Sandor, G. Klinker, C. Lange, and H. Bubb, \u201cExperimental\nevaluation of an augmented reality visualization for directing a car driver\u2019s\nattention\u201d, in Fourth IEEE and ACM International Symposium on Mixed\nand Augmented Reality (ISMAR\u201905) (IEEE, 2005), pp. 56\u201359 (cit. on p. 8).\n[41] R. Pierdicca, E. Frontoni, P. Zingaretti, E. S. Malinverni, F. Colosi, and\nR. Orazi, \u201cMaking visible the invisible. augmented reality visualization for\n3D reconstructions of archaeological sites\u201d, in International Conference on\nAugmented and Virtual Reality (Springer, 2015), pp. 25\u201337 (cit. on p. 8).",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 51
        }
    },
    {
        "page_content": "39\n[42] D. Kahl, M. Ruble, and A. Kr\u00fcger, \u201cInvestigation of Size Variations in\nOptical See-through Tangible Augmented Reality\u201d, in 2021 IEEE Inter-\nnational Symposium on Mixed and Augmented Reality (ISMAR) (IEEE,\n2021), pp. 147\u2013155 (cit. on p. 8).\n[43] L. Besan\u00e7on, P. Issartel, M. Ammi, and T. Isenberg, \u201cMouse, tactile, and\ntangible input for 3D manipulation\u201d, in Proceedings of the 2017 CHI con-\nference on human factors in computing systems (2017), pp. 4727\u20134740 (cit.\non p. 8).\n[44] B. Kress, E. Saeedi, and V. Brac-de-la Perriere, \u201cThe segmentation of the\nHMD market: optics for smart glasses, smart eyewear, AR and VR head-\nsets\u201d, Photonics Applications for Aviation, Aerospace, Commercial, and\nHarsh Environments V 9202, 107\u2013120 (2014) (cit. on p. 8).\n[45] E. Peillard, Y. Itoh, G. Moreau, J.-M. Normand, A. L\u00e9cuyer, and F. Arge-\nlaguet, \u201cCan retinal projection displays improve spatial perception in aug-\nmented reality?\u201d, in 2020 IEEE International Symposium on Mixed and\nAugmented Reality (ISMAR) (IEEE, 2020), pp. 80\u201389 (cit. on p. 8).\n[46] F. A. Khan, V. V. R. M. K. Rao, D. Wu, M. S. Arefin, N. Phillips, J. E.\nSwan, et al., \u201cMeasuring the Perceived Three-Dimensional Location of Vir-\ntualObjectsinOpticalSee-ThroughAugmentedReality\u201d,in2021IEEEIn-\nternationalSymposiumonMixedandAugmentedReality(ISMAR)(IEEE,\n2021), pp. 109\u2013117 (cit. on p. 8).\n[47] L. Heemsbergen, G. Bowtell, and J. Vincent, \u201cConceptualising Augmented\nReality:Fromvirtualdividestomediateddynamics\u201d,Convergence 27,830\u2013\n846 (2021) (cit. on p. 8).\n[48] E. Dubois, L. Nigay, and J. Troccaz, \u201cConsistency in augmented reality\nsystems\u201d, in IFIP International Conference on Engineering for Human-\nComputer Interaction (Springer, 2001), pp. 111\u2013122 (cit. on p. 8).\n[49] T. Masood and J. Egger, \u201cAdopting augmented reality in the age of in-\ndustrial digitalisation\u201d, Computers in Industry 115, 103112 (2020) (cit. on\np. 8).\n[50] P. Fite-Georgel, \u201cIs there a reality in industrial augmented reality?\u201d, in\n2011 10th ieee international symposium on mixed and augmented reality\n(IEEE, 2011), pp. 201\u2013210 (cit. on p. 8).\n[51] S. K. Ong and A. Y. C. Nee, Virtual and augmented reality applications in\nmanufacturing (Springer Science & Business Media, 2013) (cit. on p. 8).\n[52] D. Mourtzis, V. Siatras, and V. Zogopoulos, \u201cAugmented reality visualiza-\ntion of production scheduling and monitoring\u201d, Procedia CIRP 88, 151\u2013156\n(2020) (cit. on pp. 8, 11).",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 52
        }
    },
    {
        "page_content": "40 6 References\n[53] J. B. Alves, B. Marques, C. Ferreira, P. Dias, and B. S. Santos, \u201cComparing\naugmented reality visualization methods for assembly procedures\u201d, Virtual\nReality26, 235\u2013248 (2022) (cit. on pp. 8, 11).\n[54] D.Dunn,O.Tursun,H.Yu,P.Didyk,K.Myszkowski,andH.Fuchs,\u201cStim-\nulating the human visual system beyond real world performance in future\naugmented reality displays\u201d, in 2020 IEEE International Symposium on\nMixed and Augmented Reality (ISMAR) (IEEE, 2020), pp. 90\u2013100 (cit. on\np. 8).\n[55] S. J. Henderson and S. K. Feiner, \u201cAugmented reality in the psychomotor\nphase of a procedural task\u201d, in 2011 10th IEEE international symposium\non mixed and augmented reality (IEEE, 2011), pp. 191\u2013200 (cit. on p. 8).\n[56] Y. Zhang, Y. Xuan, A. Omrani, R. Yadav, and M. Fjeld, \u201cPlaying with\nData: An Augmented Reality Approach to Immersively Interact with Vi-\nsualizations of Industrial Process Tomography\u201d, in IFIP Conference on\nHuman-Computer Interaction (Springer, 2023, Under review ) (cit. on\np. 9).\n[57] J.-Y. Lee, H.-M. Park, S.-H. Lee, T.-E. Kim, and J.-S. Choi, \u201cDesign and\nimplementation of an augmented reality system using gaze interaction\u201d,\nin 2011 International Conference on Information Science and Applications\n(IEEE, 2011), pp. 1\u20138 (cit. on p. 9).\n[58] A. Nowak, Y. Zhang, A. Romanowski, and M. Fjeld, \u201cAugmented Reality\nwith Industrial Process Tomography: To Support Complex Data Analysis\nin 3D Space\u201d, in Adjunct Proceedings of the 2021 ACM International Joint\nConference on Pervasive and Ubiquitous Computing and Proceedings of\nthe 2021 ACM International Symposium on Wearable Computers (2021),\npp. 56\u201358 (cit. on pp. 9, 12).\n[59] H. M. Park, S. H. Lee, and J. S. Choi, \u201cWearable augmented reality system\nusing gaze interaction\u201d, in 2008 7th IEEE/ACM International Symposium\nonMixedandAugmentedReality(IEEE,2008),pp.175\u2013176(cit.onpp.10,\n13).\n[60] M. Lankes and B. Stiglbauer, \u201cGazeAR: Mobile gaze-based interaction\nin the context of augmented reality games\u201d, in International Conference\non Augmented Reality, Virtual Reality and Computer Graphics (Springer,\n2016), pp. 397\u2013406 (cit. on p. 10).\n[61] M. Kyt\u00f6, B. Ens, T. Piumsomboon, G. A. Lee, and M. Billinghurst, \u201cPin-\npointing: Precise head-and eye-based target selection for augmented re-\nality\u201d, in Proceedings of the 2018 CHI Conference on Human Factors in\nComputing Systems (2018), pp. 1\u201314 (cit. on p. 10).",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 53
        }
    },
    {
        "page_content": "41\n[62] Y.-Y. Huang and M. Menozzi, \u201cEffects of viewing distance and age on the\nperformance and symptoms in a visual search task in augmented reality\u201d,\nApplied Ergonomics 102, 103746 (2022) (cit. on p. 10).\n[63] P. Contreras, D. Chimbo, A. Tello, and M. Espinoza, \u201cSemantic web and\naugmented reality for searching people, events and points of interest within\nof a university campus\u201d, in 2017 XLIII Latin American Computer Confer-\nence (CLEI) (IEEE, 2017), pp. 1\u201310 (cit. on pp. 10, 12).\n[64] F. S. Shaikh, \u201cAugmented Reality Search to Improve Searching Using Aug-\nmented Reality\u201d, in 2021 6th International Conference for Convergence in\nTechnology (I2CT) (IEEE, 2021), pp. 1\u20135 (cit. on p. 10).\n[65] C. Reardon, K. Lee, and J. Fink, \u201cCome see this! augmented reality to\nenable human-robot cooperative search\u201d, in 2018 IEEE International Sym-\nposium on Safety, Security, and Rescue Robotics (SSRR) (IEEE, 2018),\npp. 1\u20137 (cit. on p. 10).\n[66] R. Rivu, Y. Abdrabou, K. Pfeuffer, A. Esteves, S. Meitner, and F. Alt,\n\u201cStARe: Gaze-Assisted Face-to-Face Communication in Augmented Re-\nality\u201d, in ACM Symposium on Eye Tracking Research and Applications\n(2020), pp. 1\u20135 (cit. on pp. 10, 13).\n[67] Y. Zhang, A. Nowak, A. Romanowski, and M. Fjeld, \u201cAn Initial Explo-\nration of Visual Cues in Head-mounted Display Augmented Reality for\nBook Searching\u201d, in Proceedings of the 21st International Conference on\nMobile and Ubiquitous Multimedia (2022) (cit. on p. 10).\n[68] S. White, L. Lister, and S. Feiner, \u201cVisual hints for tangible gestures in\naugmented reality\u201d, in 2007 6th IEEE and ACM International Symposium\non Mixed and Augmented Reality (IEEE, 2007), pp. 47\u201350 (cit. on pp. 10,\n13).\n[69] K. Lyons, M. Gandy, and T. Starner, \u201cGuided by voices: An audio aug-\nmented reality system\u201d, in (Georgia Institute of Technology, 2000) (cit. on\npp. 10, 13).\n[70] N. Sawhney and C. Schmandt, \u201cNomadic radio: speech and audio inter-\naction for contextual messaging in nomadic environments\u201d, ACM transac-\ntions on Computer-Human interaction (TOCHI) 7, 353\u2013383 (2000) (cit. on\npp. 10, 13).\n[71] M.Sousa,J.Vieira,D.Medeiros,A.Arsenio,andJ.Jorge,\u201cSleeveAR:Aug-\nmented reality for rehabilitation using realtime feedback\u201d, in Proceedings\nof the 21st international conference on intelligent user interfaces (2016),\npp. 175\u2013185 (cit. on p. 10).",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 54
        }
    },
    {
        "page_content": "42 6 References\n[72] F. Clemente, S. Dosen, L. Lonini, M. Markovic, D. Farina, and C. Cipriani,\n\u201cHumans can integrate augmented reality feedback in their sensorimotor\ncontrol of a robotic hand\u201d, IEEE Transactions on Human-Machine Systems\n47, 583\u2013589 (2016) (cit. on pp. 10, 14).\n[73] J. Vieira, M. Sousa, A. Ars\u00e9nio, and J. Jorge, \u201cAugmented reality for re-\nhabilitation using multimodal feedback\u201d, in Proceedings of the 3rd 2015\nWorkshop on ICTs for improving Patients Rehabilitation Research Tech-\nniques (2015), pp. 38\u201341 (cit. on p. 10).\n[74] Y. Zhang, A. Nowak, X. Yueming, R. Andrzej, and F. Morten, \u201cSee or\nHear? Exploring the Effect of Visual and Audio Hints and Gaze-assisted\nTaskFeedbackforVisualSearchTasksinAugmentedReality\u201d,Proceedings\nof the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies\n2023 (Under review ) (cit. on p. 10).\n[75] A. M. Treisman and G. Gelade, \u201cA feature-integration theory of attention\u201d,\nCognitive psychology 12, 97\u2013136 (1980) (cit. on p. 10).\n[76] F. Bruno, F. Caruso, L. De Napoli, and M. Muzzupappa, \u201cVisualization\nof industrial engineering data visualization of industrial engineering data\nin augmented reality\u201d, Journal of visualization 9, 319\u2013329 (2006) (cit. on\np. 11).\n[77] M. Noghabaei, A. Heydarian, V. Balali, and K. Han, \u201cA Survey Study to\nUnderstand Industry Vision for Virtual and Augmented Reality Applica-\ntions in Design and Construction\u201d, arXiv preprint arXiv:2005.02795 (2020)\n(cit. on pp. 11, 29).\n[78] M. Lorenz, S. Knopp, and P. Klimant, \u201cIndustrial augmented reality: Re-\nquirements for an augmented reality maintenance worker support system\u201d,\nin 2018 IEEE International Symposium on Mixed and Augmented Reality\nAdjunct (ISMAR-Adjunct) (IEEE, 2018), pp. 151\u2013153 (cit. on p. 11).\n[79] S. B\u00fcttner, M. Funk, O. Sand, and C. R\u00f6cker, \u201cUsing head-mounted dis-\nplays and in-situ projection for assistive systems: A comparison\u201d, in Pro-\nceedings of the 9th ACM international conference on pervasive technologies\nrelated to assistive environments (2016), pp. 1\u20138 (cit. on p. 12).\n[80] G.Avalle,F.DePace,C.Fornaro,F.Manuri,andA.Sanna,\u201cAnaugmented\nreality system to support fault visualization in industrial robotic tasks\u201d,\nIEEE Access 7, 132343\u2013132359 (2019) (cit. on p. 12).\n[81] M. Satkowski and R. Dachselt, \u201cInvestigating the Impact of Real-World\nEnvironments on the Perception of 2D Visualizations in Augmented Re-\nality\u201d, in Proceedings of the 2021 CHI Conference on Human Factors in\nComputing Systems (2021), pp. 1\u201315 (cit. on p. 12).",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 55
        }
    },
    {
        "page_content": "43\n[82] R. Mann, S. Stanley, D. Vlaev, E. Wabo, and K. Primrose, \u201cAugmented-\nreality visualization of fluid mixing in stirred chemical reactors using elec-\ntrical resistance tomography\u201d, Journal of Electronic Imaging 10, 620\u2013630\n(2001) (cit. on p. 12).\n[83] Y. Zhang, A. Omrani, R. Yadav, and M. Fjeld, \u201cSupporting Visualization\nAnalysis in Industrial Process Tomography by Using Augmented Reality\u2013A\nCase Study of an Industrial Microwave Drying System\u201d, Sensors 21, 6515\n(2021) (cit. on pp. 12, 18, 30).\n[84] A. Rafiq and B. Ahsan, \u201cSecure and Dynamic Model for Book Searching on\nCloud Computing as Mobile Augmented Reality.\u201d, International Journal of\nModern Education & Computer Science 6(2014) (cit. on p. 12).\n[85] C. Gebhardt, B. Hecox, B. van Opheusden, D. Wigdor, J. Hillis, O. Hilliges,\nand H. Benko, \u201cLearning cooperative personalized policies from gaze data\u201d,\nin Proceedings of the 32nd Annual ACM Symposium on User Interface\nSoftware and Technology (2019), pp. 197\u2013208 (cit. on p. 12).\n[86] C. Trepkowski, D. Eibich, J. Maiero, A. Marquardt, E. Kruijff, and S.\nFeiner, \u201cThe effect of narrow field of view and information density on visual\nsearch performance in augmented reality\u201d, in 2019 IEEE Conference on\nVirtual Reality and 3D User Interfaces (VR) (IEEE, 2019), pp. 575\u2013584\n(cit. on p. 13).\n[87] J. Van Dam, A. Krasne, and J. L. Gabbard, \u201cDrone-based augmented real-\nity platform for bridge inspection: Effect of ar cue design on visual search\ntasks\u201d, in 2020 IEEE Conference on Virtual Reality and 3D User Interfaces\nAbstracts and Workshops (VRW) (IEEE, 2020), pp. 201\u2013204 (cit. on p. 13).\n[88] A. Ajanki, M. Billinghurst, H. Gamper, T. J\u00e4rvenp\u00e4\u00e4, M. Kandemir, S.\nKaski, M. Koskela, M. Kurimo, J. Laaksonen, K. Puolam\u00e4ki, et al., \u201cAn\naugmented reality interface to contextual information\u201d, Virtual reality 15,\n161\u2013173 (2011) (cit. on p. 13).\n[89] F. Lu, S. Davari, L. Lisle, Y. Li, and D. A. Bowman, \u201cGlanceable ar: Eval-\nuating information access methods for head-worn augmented reality\u201d, in\n2020IEEEconferenceonvirtualrealityand3Duserinterfaces(VR)(IEEE,\n2020), pp. 930\u2013939 (cit. on p. 13).\n[90] N.Pathmanathan,M.Becher,N.Rodrigues,G.Reina,T.Ertl,D.Weiskopf,\nand M. Sedlmair, \u201cEye vs. head: Comparing gaze methods for interaction\nin augmented reality\u201d, in ACM Symposium on Eye Tracking Research and\nApplications (2020), pp. 1\u20135 (cit. on p. 13).",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 56
        }
    },
    {
        "page_content": "44 6 References\n[91] P.Sasikumar,L.Gao,H.Bai,andM.Billinghurst,\u201cWearableremotefusion:\nA mixed reality remote collaboration system with local eye gaze and remote\nhand gesture sharing\u201d, in 2019 IEEE International Symposium on Mixed\nand Augmented Reality Adjunct (ISMAR-Adjunct) (IEEE, 2019), pp. 393\u2013\n394 (cit. on p. 13).\n[92] J. Looser, M. Billinghurst, R. Grasset, and A. Cockburn, \u201cAn evaluation\nof virtual lenses for object selection in augmented reality\u201d, in Proceedings\nof the 5th international conference on Computer graphics and interactive\ntechniques in Australia and Southeast Asia (2007), pp. 203\u2013210 (cit. on\np. 13).\n[93] V.Peysakhovich,O.Lefran\u00c3\u00a7ois,F. Dehais,andM.Causse,\u201cTheNeuroer-\ngonomicsofAircraftCockpits:TheFourStagesofEye-TrackingIntegration\nto Enhance Flight Safety\u201d, Safety 4(2018) 10.3390/safety4010008 (cit.\non p. 13).\n[94] C. Curatu, H. Hua, and J. Rolland, \u201cProjection-based head-mounted dis-\nplay with eye-tracking capabilities\u201d, in Novel Optical Systems Design and\nOptimization VIII, Vol. 5875 (International Society for Optics and Photon-\nics, 2005), 58750J (cit. on p. 13).\n[95] E.Nonino,J.Gisler,V.Holzwarth,C.Hirt,andA.Kunz,\u201cSubtleAttention\nGuidance for Real Walking in Virtual Environments\u201d, in 2021 IEEE Inter-\nnational Symposium on Mixed and Augmented Reality Adjunct (ISMAR-\nAdjunct) (IEEE, 2021), pp. 310\u2013315 (cit. on p. 13).\n[96] R.Behringer,J.Park,andV.Sundareswaran,\u201cModel-basedvisualtracking\nfor outdoor augmented reality applications\u201d, in Proceedings. international\nsymposium on mixed and augmented reality (IEEE, 2002), pp. 277\u2013322\n(cit. on p. 13).\n[97] Z. Zhu, V. Branzoi, M. Wolverton, G. Murray, N. Vitovitch, L. Yarnall, G.\nAcharya, S. Samarasekera, and R. Kumar, \u201cAR-mentor: Augmented reality\nbased mentoring system\u201d, in 2014 IEEE international symposium on mixed\nand augmented reality (ISMAR) (IEEE, 2014), pp. 17\u201322 (cit. on p. 13).\n[98] D. Wolf, D. Besserer, K. Sejunaite, M. Riepe, and E. Rukzio, \u201ccare: An\naugmented reality support system for dementia patients\u201d, in The 31st An-\nnual ACM Symposium on User Interface Software and Technology Adjunct\nProceedings (2018), pp. 42\u201344 (cit. on p. 13).\n[99] R. W. Lindeman, H. Noma, and P. G. De Barros, \u201cHear-through and mic-\nthrough augmented reality: Using bone conduction to display spatialized\naudio\u201d, in 2007 6th IEEE and ACM International Symposium on Mixed\nand Augmented Reality (IEEE, 2007), pp. 173\u2013176 (cit. on p. 13).",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 57
        }
    },
    {
        "page_content": "45\n[100] V. Sundareswaran, K. Wang, S. Chen, R. Behringer, J. McGee, C. Tam,\nand P. Zahorik, \u201c3D audio augmented reality: implementation and experi-\nments\u201d, in The Second IEEE and ACM International Symposium on Mixed\nand Augmented Reality, 2003. Proceedings. (IEEE, 2003), pp. 296\u2013297 (cit.\non p. 13).\n[101] A. Marquardt, C. Trepkowski, T. D. Eibich, J. Maiero, and E. Kruijff,\n\u201cNon-visual cues for view management in narrow field of view augmented\nreality displays\u201d, in 2019 IEEE International Symposium on Mixed and\nAugmented Reality (ISMAR) (IEEE, 2019), pp. 190\u2013201 (cit. on p. 13).\n[102] S. Arevalo Arboleda, F. R\u00fccker, T. Dierks, and J. Gerken, \u201cAssisting ma-\nnipulation and grasping in robot teleoperation with augmented reality vi-\nsual cues\u201d, in Proceedings of the 2021 CHI Conference on Human Factors\nin Computing Systems (2021), pp. 1\u201314 (cit. on p. 13).\n[103] A. Mulloni, H. Seichter, and D. Schmalstieg, \u201cUser experiences with aug-\nmented reality aided navigation on phones\u201d, in 2011 10th IEEE Interna-\ntionalSymposiumonMixedandAugmentedReality(IEEE,2011),pp.229\u2013\n230 (cit. on p. 14).\n[104] M. Cidota, S. Lukosch, D. Datcu, and H. Lukosch, \u201cComparing the effect of\naudio and visual notifications on workspace awareness using head-mounted\ndisplaysforremotecollaborationinaugmentedreality\u201d,AugmentedHuman\nResearch1, 1\u201315 (2016) (cit. on p. 14).\n[105] Y. Cao, T. Wang, X. Qian, P. S. Rao, M. Wadhawan, K. Huo, and K.\nRamani, \u201cGhostAR: A time-space editor for embodied authoring of human-\nrobot collaborative task with augmented reality\u201d, in Proceedings of the\n32nd Annual ACM Symposium on User Interface Software and Technology\n(2019), pp. 521\u2013534 (cit. on p. 14).\n[106] V. Paelke, \u201cAugmented reality in the smart factory: Supporting workers in\nan industry 4.0. environment\u201d, in Proceedings of the 2014 IEEE emerging\ntechnology and factory automation (ETFA) (IEEE, 2014), pp. 1\u20134 (cit. on\np. 14).\n[107] C. Liu, S. Huot, J. Diehl, W. Mackay, and M. Beaudouin-Lafon, \u201cEvalu-\nating the benefits of real-time feedback in mobile augmented reality with\nhand-held devices\u201d, in Proceedings of the SIGCHI Conference on Human\nFactors in Computing Systems (2012), pp. 2973\u20132976 (cit. on p. 14).\n[108] M. Zahiri, C. A. Nelson, D. Oleynikov, and K.-C. Siu, \u201cEvaluation of aug-\nmented reality feedback in surgical training environment\u201d, Surgical Inno-\nvation25, 81\u201387 (2018) (cit. on p. 14).",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 58
        }
    },
    {
        "page_content": "46 6 References\n[109] K.Murakami,R.Kiyama,T.Narumi,T.Tanikawa,andM.Hirose,\u201cPoster:\nA wearable augmented reality system with haptic feedback and its perfor-\nmance in virtual assembly tasks\u201d, in 2013 IEEE Symposium on 3D User\nInterfaces (3DUI) (IEEE, 2013), pp. 161\u2013162 (cit. on p. 14).\n[110] A. Riegler, B. Aksoy, A. Riener, and C. Holzmann, \u201cGaze-based interaction\nwith windshield displays for automated driving: Impact of dwell time and\nfeedback design on task performance and subjective workload\u201d, in 12th\nInternational Conference on Automotive User Interfaces and Interactive\nVehicular Applications (2020), pp. 151\u2013160 (cit. on p. 14).\n[111] Y.Zhang,R.Yadav,A.Omrani,andM.Fjeld,\u201cANovelAugmentedReality\nSystem To Support Volumetric Visualization in Industrial Process Tomog-\nraphy\u201d, in Proceedings of the 2021 Conference on Interfaces and Human\nComputer Interaction (2021), pp. 3\u20139 (cit. on pp. 18, 20).\n[112] A. Omrani, R. Yadav, G. Link, and J. Jelonnek, \u201cA Multistatic Uniform\nDiffraction Tomography Algorithm for Microwave Imaging in Multilayered\nMedia for Microwave Drying\u201d, IEEE Transactions on Antennas and Prop-\nagation (2022) (cit. on p. 21).\n[113] J. V. Bradley, \u201cComplete counterbalancing of immediate sequential effects\nin a Latin square design\u201d, Journal of the American Statistical Association\n53, 525\u2013528 (1958) (cit. on pp. 22, 31).\n[114] A. Bangor, P. T. Kortum, and J. T. Miller, \u201cAn empirical evaluation of the\nsystem usability scale\u201d, Intl. Journal of Human\u2013Computer Interaction 24,\n574\u2013594 (2008) (cit. on p. 23).\n[115] A.Romanowski,\u201cBigData-DrivenContextualProcessingMethodsforElec-\ntrical Capacitance Tomography\u201d, IEEE Transactions on Industrial Infor-\nmatics15, 1609\u20131618 (2019) (cit. on p. 29).\n[116] H. Lasi, P. Fettke, H.-G. Kemper, T. Feld, and M. Hoffmann, \u201cIndustry\n4.0\u201d, Business & information systems engineering 6, 239\u2013242 (2014) (cit.\non p. 34).",
        "metadata": {
            "source": "pdf/46.pdf",
            "page": 59
        }
    },
    {
        "page_content": "Event-Based Vision: A Survey\nGuillermo Gallego ,Senior Member, IEEE , Tobi Delbr \u20acuck ,Fellow, IEEE , Garrick Orchard ,\nChiara Bartolozzi ,Member, IEEE , Brian Taba, Andrea Censi, Stefan Leutenegger ,\nAndrew J. Davison, J \u20acorg Conradt ,Senior Member, IEEE ,\nKostas Daniilidis ,Fellow, IEEE , and Davide Scaramuzza ,Senior Member, IEEE\nAbstract\u2014 Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a \ufb01xed rate,\nthey asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the\nbrightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of ms),\nvery high dynamic range (140 dB versus 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced\nmotion blur . Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras,\nsuch as low-latency , high speed, and high dynamic range. However , novel methods are required to process the unconventional output of\nthese sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging \ufb01eld of event-based vision,\nwith a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event\ncameras from their working principle, the actual sensors that are available and the tasks that they have been used for , from low-level vision\n(feature detection and tracking, optic \ufb02ow , etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the\ntechniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors,\nsuch as spiking neural networks. Additionally , we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the\nsearch for a more ef\ufb01cient, bio-inspired way for machines to perceive and interact with the world.\nIndex Terms\u2014 Event cameras, bio-inspired vision, asynchronous sensor, low latency, high dynamic range, low power\n\u00c7\n1I NTRODUCTION AND APPLICATIONS\n\u201cTHEbrain is imagination, and that was exciting to me; I\nwanted to build a chip that could imagine something.\u201d1\nthat is how Misha Mahowald, a graduate student at Caltechin 1986, started to work with Prof. Carver Mead on the stereo\nproblem from a joint biological and engineering perspective.\nA couple of years later, in 1991, the image of a cat in the cover\nof Scienti\ufb01c American [1], acquired by a novel \u201cSilicon Reti-\nna\u201d mimicking the neural architecture of the eye, showed a\nnew, powerful way of doing computations, igniting the\nemerging \ufb01eld of neuromorphic engineering. Today, we still\npursue the same visionary challenge: understanding how\nthe brain works and building one on a computer chip. Cur-\nrent efforts include \ufb02agship billion-dollar projects, such as\nthe Human Brain Project and the Blue Brain Project in\nEurope and the U.S. BRAIN (Brain Research through\nAdvancing Innovative Neurotechnologies) Initiative.\nThis paper provides an overview of the bio-inspired tech-\nnology of silicon retinas, or \u201cevent cameras\u201d, such as [2], [3],\n[4], [5], with a focus on their application to solve classical as\nwell as new computer vision and robotic tasks. Sight is, by far,\nthe dominant sense in humans to perceive the world, and,\ntogether with the brain, learn new things. In recent years, this\ntechnology has attracted a lot of attention from academia and\nindustry. This is due to the availability of prototype event cam-\neras and the advantages that they offer to tackle problems that\nare dif\ufb01cult with standard frame-based image sensors (that\nprovide stroboscopic synchronous sequences of pictures),\nsuch as high-speed motion estimation [6], [7] or high dynamic\nrange (HDR) imaging [8].\nEvent cameras are asynchronous sensors that pose a para-\ndigm shift in the way visual information is acquired. This is\nbecause they sample light based on the scene dynamics,\nrather than on a clock that has no relation to the viewed\nscene. Their advantages are: very high temporal resolution\nand low latency (both in the order of microseconds), very/C15Guillermo Gallego is with the Technische Universit \u20acat Berlin, 10623 Berlin,\nGermany and also with the Einstein Center Digital Future, 10117 Berlin,\nGermany. E-mail: guillermo.gallego@tu-berlin.de.\n/C15Tobi Delbr \u20acuck is with the Department of Information Technology and Elec-\ntrical Engineering, ETH Zurich, 8092 Z \u20acurich, Switzerland, and also with\nthe Institute of Neuroinformatics, University of Zurich and ETH Zurich,\n8057 Z \u20acurich, Switzerland. E-mail: tobi@ini.uzh.ch.\n/C15Garrick Orchard is with Intel Labs, Santa Clara, CA 95054-1549 USA\nE-mail: garrick.orchard@intel.com.\n/C15Chiara Bartolozzi is with the Istituto Italiano di Tecnologia, 16163 Gen-\nova, Italy. E-mail: chiara.bartolozzi@iit.it.\n/C15Brian Taba is with IBM Research, San Jose, CA 95120 USA\nE-mail: btaba@us.ibm.com.\n/C15Andrea Censi is with the Department of Mechanical and Process Engineering,\nETH Zurich, 8092 Z \u20acurich, Switzerland. E-mail: acensi@ethz.ch.\n/C15Stefan Leutenegger and Andrew J. Davison are with Imperial College London,\nSW7 2BU London, U.K. E-mail: s.leutenegger@imperial.ac.uk, ajd@doc.ic.ac.\nuk.\n/C15J\u20acorg Conradt is with the KTH Royal Institute of Technology, 114 28 Stock-\nholm, Sweden. E-mail: jconradt@kth.se.\n/C15Kostas Daniilidis is with the University of Pennsylvania, Philadelphia, PA\n19104 USA. E-mail: kostas@cis.upenn.edu.\n/C15Davide Scaramuzza is with the University of Zurich, 8050 Z \u20acurich, Switzer-\nland. E-mail: sdavide@i\ufb01.uzh.ch.\nManuscript received 13 Apr. 2019; revised 2 Feb. 2020; accepted 22 June 2020.\nDate of publication 10 July 2020; date of current version 3 Dec. 2021.\n(Corresponding author: Guillermo Gallego.)\nRecommended for acceptance by P. Favaro.\nDigital Object Identi\ufb01er no. 10.1109/TPAMI.2020.30084131.https://youtu.be/FKemf6Idkd0?t=67154 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 1, JANUARY 2022\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 0
        }
    },
    {
        "page_content": "high dynamic range (140 dB versus 60 dB of standard cam-\neras), and low power consumption. Hence, event cameras\nhave a large potential for robotics and wearable applications\nin challenging scenarios for standard cameras, such as high\nspeed and high dynamic range. Although event cameras\nhave become commercially available only since 2008 [2], the\nrecent body of literature on these new sensors2as well as\nthe recent plans for mass production claimed by companies,\nsuch as Samsung [5] and Prophesee,3highlight that there is\na big commercial interest in exploiting these novel vision\nsensors for mobile robotic, augmented and virtual reality\n(AR/VR), and video game applications. However, because\nevent cameras work in a fundamentally different way from\nstandard cameras, measuring per-pixel brightness changes\n(called \u201c events \u201d) asynchronously rather than measuring\n\u201cabsolute\u201d brightness at constant rate, novel methods are\nrequired to process their output and unlock their potential.\nApplications of Event Cameras . Typical scenarios where event\ncameras offer advantages over oth er sensing modalities include\nreal-time interaction systems, such as robotics or wearable elec-\ntronics [10], where operation under uncontrolled lighting con-\nditions, latency, and power are important [11]. Event cameras\nare used for object tracking [12], [13], surveillance and monitor-\ning [14], and object/gesture reco gnition [15], [16], [17]. They are\nalso pro\ufb01table for depth estimat ion [18], [19], structured light\n3D scanning [20], optical \ufb02ow estimation [21], [22], HDR image\nreconstruction [8], [23], [24] and Simultaneous Localization and\nMapping (SLAM) [25], [26], [27]. Event-based vision is a grow-\ning \ufb01eld of research, and other applications, such as image\ndeblurring [28] or star tracking [29], [30], will appear as event\ncameras become widely available [9].\n2P RINCIPLE OF OPERATION OF EVENT CAMERAS\nIn contrast to standard cameras, which acquire full images\nat a rate speci\ufb01ed by an external clock (e.g., 30 fps), event\ncameras, such as the Dynamic Vision Sensor ( DVS) [2], [31],\n[32], [33], [34], respond to brightness changes in the scene\nasynchronously and independently for every pixel (Fig. 1b).\nThus, the output of an event camera is a variable data-rate\nsequence of digital \u201cevents\u201d or \u201cspikes\u201d, with each event\nrepresenting a change of brightness (log intensity)4of pre-\nde\ufb01ned magnitude at a pixel at a particular time5(Fig. 1b)\n(Section 2.4). This encoding is inspired by the spiking nature\nof biological visual pathways (Section 3.3).\nEach pixel memorizes the log intensity each time it sends\nan event, and continuously monitors for a change of suf\ufb01-\ncient magnitude from this memorized value (Fig. 1a). When\nthe change exceeds a threshold, the camera sends an event,\nwhich is transmitted from the chip with the x; ylocation, thetime t, and the 1-bit polarity pof the change (i.e., brightness\nincrease (\u201cON\u201d) or decrease (\u201cOFF\u201d)). This event output is\nillustrated in Figs. 1b, 1e and 1f.\nThe events are transmitted from the pixel array to periph-\nery and then out of the camera using a shared digital output\nbus, typically by using address-event representation ( AER)\nreadout [37], [38]. This bus can become saturated, which per-\nturbs the times that events are sent. Event cameras have\nreadout rates ranging from 2 MHz [2] to 1200 MHz [39],\ndepending on the chip and type of hardware interface.\nEvent cameras are data-driven sensors: their output\ndepends on the amount of motion or brightness change in\nthe scene. The faster the motion, the more events per second\nare generated, since each pixel adapts its delta modulator\nsampling rate to the rate of change of the log intensity signal\nthat it monitors. Events are timestamped with microsecond\nresolution and are transmitted with sub-millisecond latency,\nwhich make these sensors react quickly to visual stimuli.\nThe incident light at a pixel is a product of scene illumi-\nnation and surface re\ufb02ectance. If illumination is approxi-\nmately constant, a log intensity change signals a re\ufb02ectance\nchange. These changes in re\ufb02ectance are mainly the result\nof the movement of objects in the \ufb01eld of view. That is why\nthe DVS brightness change events have a built-in invariance\nto scene illumination [2].\nComparing Bandwidths of DVS Pixels and Frame-Based Cam-\nera. Although DVS pixels are fast, like any physical\nFig. 1. Summary of the DAVIS camera [4], comprising an event-based\ndynamic vision sensor (DVS [2]) and a frame-based active pixel sensor\n(APS ) in the same pixel array , sharing the same photodiode in each\npixel. (a) Simpli\ufb01ed circuit diagram of the DAVIS pixel (DVS pixel in red,\nAPS pixel in blue). (b) Schematic of the operation of a DVS pixel, con-\nverting light into events. (c)-(d) Pictures of the DAVIS chip and USB cam-\nera. (e) A white square on a rotating black disk viewed by the DAVIS\nproduces grayscale frames and a spiral of events in space-time. Events\nin space-time are color-coded, from green (past) to red (present). (f)\nFrame and overlaid events of a natural scene; the frames lag behind the\nlow-latency events (colored according to polarity). Images adapted\nfrom [4], [35]. A more in-depth comparison of the DVS, DAVIS, and ATIS\npixel designs can be found in [36].\n2.https://github.com/uzh-rpg/event-based_vision_resources [9]\n3.http://rpg.i\ufb01.uzh.ch/ICRA17_event_vision_workshop.html\n4.Brightness is a perceived quantity; for brevity we use it to refer to\nlog intensity since they correspond closely for uniformly-lighted scenes.\n5.Nomenclature: \u201cEvent cameras\u201d output data-driven events that\nsignal a place and time. This nomenclature has evolved over the past\ndecade: originally they were known as address-event representation\n(AER) silicon retinas, and later they became event-based cameras. In\ngeneral, events can signal any kind of information (intensity, local spa-\ntial contrast, etc.), but over the last \ufb01ve years or so, the term \u201cevent cam-\nera\u201d has unfortunately become practically synonymous with the\nparticular representation of brightness change output by DVS\u2019s.GALLEGO ET AL.: EVENT-BASED VISION: A SURVEY 155",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 1
        }
    },
    {
        "page_content": "transducer, they have a \ufb01nite b andwidth: if the incoming light\nintensity varies too quickly, the f ront-end photoreceptor circuits\n\ufb01lter out the variations [40]. The rise and fall time that is analo-\ngous to the exposure time in standard image sensors is the\nreciprocal of this bandwidth. Fig. 2 shows an example of mea-\nsured DVS pixel frequency response (DVS128 in [2]). The mea-\nsurement setup (Fig. 2a) uses a sinusoidally-varying generated\nsignal to measure the response. Fig. 2b shows that, at low fre-\nquencies, the DVS pixel produces a certain number of events\nper cycle. Above some cutoff fr equency, the variations are \ufb01l-\ntered out by the photoreceptor dynamics, and thus the number\nof events per cycle drops. This cu toff frequency is a monotoni-\ncally increasing function of light intensity. At the brighter light\nintensity, the DVS pixel bandwidth is about 3 kHz, equivalent\nto an exposure time of about 300 ms. At 1000 /C2lower intensity,\nthe DVS bandwidth is reduced to about 300 Hz. Even when the\nLED brightness is reduced by a factor of 1,000, the frequency\nresponse of DVS pixels is ten times higher than the 30 Hz\nNyquist frequency from a 60 fps image sensor. Also, the frame-\nbased camera aliases frequencies above the Nyquist frequency\nback to the baseband, whereas the DVS pixel does not due to\nthe continuous time response.\n2.1 Event Camera Designs\nThis section presents the most common event camera\ndesigns. The actual devices (commercial or prototype cam-\neras such as the DAVIS240) are summarized in Section 2.5.\nThe \ufb01rst silicon retina was developed by Mahowald and\nMead at Caltech during the period 1986-1992, in Ph.D. thesis\nwork [41] that was awarded the prestigious Clauser prize.6\nMahowald and Mead\u2019s sensor had logarithmic pixels, was\nmodeled after the three-layer Ku\ufb02er retina, and produced as\noutput spike events using the AER protocol. However, it suf-\nfered from several shortcomings: each wire-wrapped retina\nboard required precise adjustment of biasing potentiometers;\nthere was considerable mismatch between the responses of\ndifferent pixels; and pixels were too large to be a device of\npractical use. Over the next decade the neuromorphic com-\nmunity developed a series of silicon retinas. These develop-\nments are summarized in [36], [38], [42], [43].\nTheDVS event camera [2] had its genesis in a frame-based\nsilicon retina design where the continuous-time photorecep-\ntor was capacitively coupled to a readout circuit that was\nreset each time the pixel was sampled [44]. More recentevent camera technology has been reviewed in the electron-\nics and neuroscience literature [10], [36], [38], [45], [46], [47].\nAlthough surprisingly many applications can be solved by\nonly processing DVS events (i.e., brightness changes), it\nbecame clear that some also require some form of static out-\nput (i.e., \u201cabsolute\u201d brightness). To address this shortcom-\ning, there have been several developments of cameras that\nconcurrently output dynamic and static information.\nThe Asynchronous Time Based Image Sensor (ATIS ) [3], [48]\nhas pixels that contain a DVS subpixel (called change detec-\ntion CD) that triggers another subpixel to read out the abso-\nlute intensity (exposure measurement EM). The trigger\nresets a capacitor to a high voltage. The charge is bled away\nfrom this capacitor by another photodiode. The brighter the\nlight, the faster the capacitor discharges. The ATIS intensity\nreadout transmits two more events coding the time between\ncrossing two threshold voltages, as in [49]. This way, only\npixels that change provide their new intensity values. The\nbrighter the illumination, the shorter the time between these\ntwo events. The ATIS achieves large static dynamic range\n(>120 dB). However, the ATIS has the disadvantage that\npixels are at least double the area of DVS pixels. Also, in\ndark scenes the time between the two intensity events can\nbe long and the readout of intensity can be interrupted by\nnew events ([50] proposes a workaround to this problem).\nThe widely-used Dynamic and Active Pixel Vision Sensor\n(DAVIS ) [4], [51] illustrated in Fig. 1 combines a conven-\ntional active pixel sensor ( APS) [52] in the same pixel with\nDVS. The advantage over ATIS is a much smaller pixel size\nsince the photodiode is shared and the readout circuit only\nadds about 5 percent to the DVS pixel area. Intensity (APS)\nframes can be triggered at a constant frame rate or on\ndemand, by analysis of DVS events, although the latter is\nseldom exploited.7However, the APS readout has limited\ndynamic range (55dB) and like a standard camera, it is\nredundant if the pixels do not change.\nSince the ATIS and DAVIS pixel designs include a DVS\npixel (change detector) [36] we often use the term \u201cDVS\u201d to\nrefer to the binary-polarity event output or circuitry, regard-\nless of whether it is from a DVS, ATIS or DAVIS design.\n2.2 Advantages of Event Cameras\nEvent cameras offer numerous potential advantages over\nstandard cameras:\nHigh Temporal Resolution . monitoring of brightness\nchanges is fast, in analog circuitry, and the read-out of the\nevents is digital, with a 1 MHz clock, i.e., events are detected\nand timestamped with microsecond resolution. Therefore,\nevent cameras can capture very fast motions, without suf-\nfering from motion blur typical of frame-based cameras.\nLow Latency . Each pixel works independently and there is\nno need to wait for a global exposure time of the frame: as\nsoon as the change is detected, it is transmitted. Hence,\nevent cameras have minimal latency: about 10 ms on the lab\nbench, and sub-millisecond in the real world.\nLow Power . Because event cameras transmit only bright-\nness changes, and thus remove redundant data, power is\nonly used to process changing pixels. At the die level, most\nFig. 2. \u201cEvent transfer function\u201d from a single DVS pixel in response to\nsinusoidal LED stimulation. The background events cause additional ON\nevents at very low frequencies. The 60 fps camera curve shows the\ntransfer function including aliasing from frequencies above the Nyquist\nfrequency . Figure adapted from [2].\n6.http://www.gradof\ufb01ce.caltech.edu/current/clauser7.https://github.com/SensorsINI/jaer/blob/master/src/eu/\nseebetter/ini/chips/davis/DavisAutoShooter.java156 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 1, JANUARY 2022",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 2
        }
    },
    {
        "page_content": "cameras use about 10 mW, and there are prototypes that\nachieve less than 10 mW. Embedded event-camera systems\nwhere the sensor is directly interfaced to a processor have\nshown system-level power consumption (i.e., sensing plus\nprocessing) of 100mW or less [17], [53], [54], [55].\nHigh Dynamic Range (HDR) . The very high dynamic\nrange of event cameras ( >120 dB) notably exceeds the 60\ndB of high-quality, frame-based cameras, making them able\nto acquire information from moonlight to daylight. It is due\nto the facts that the photoreceptors of the pixels operate in\nlogarithmic scale and each pixel works independently, not\nwaiting for a global shutter. Like biological retinas, DVS pix-\nels can adapt to very dark as well as very bright stimuli.\n2.3 Challenges Due to the Novel Sensing Paradigm\nEvent cameras represent a paradigm shift in acquisition of\nvisual information. Hence, they pose the challenge of design-\ning novel methods (algorithms and hardware) to process the\nacquired data and extract information from it in order to\nunlock the advantages of the camera. Speci\ufb01cally:\n1) Coping with different space-time output: The output of\nevent cameras is fundamentally different from that\nof standard cameras: events are asynchronous and\nspatially sparse, whereas images are synchronous\nand dense. Hence, frame-based vision algorithms\ndesigned for image sequences are not directly appli-\ncable to event data.\n2) Coping with different photometric sensing: In contrast to\nthe grayscale information that standard cameras pro-\nvide, each event contains binary (increase/decrease)\nbrightness change information. Brightness changes\ndepend not only on the scene brightness, but also on\nthe current and past relative motion between the\nscene and the camera.\n3) Coping with noise and dynamic effects: All vision sen-\nsors are noisy because of the inherent shot noise in\nphotons and from transistor circuit noise, and they\nalso have non-idealities. This situation is especially\ntrue for event cameras, where the process of quantiz-\ning temporal contrast is complex and has not been\ncompletely characterized.\nTherefore, new methods need to rethink the space-time,\nphotometric and stochastic nature of event data. This poses\nthe following questions: What is the best way to extract\ninformation from the events relevant for a given task? and\nHow can noise and non-ideal effects be modeled to better\nextract meaningful information from the events?\n2.4 Event Generation Model\nAn event camera [2] has independent pixels that respond to\nchanges in their log photocurrent L\u00bc:log\u00f0I\u00de(\u201cbrightness\u201d).\nSpeci\ufb01cally, in a noise-free scenario, an event ek\u00bc:\u00f0xk;tk;pk\u00de\nis triggered at pixel xk\u00bc:\u00f0xk;yk\u00de>and at time tkas soon as the\nbrightness increment since the last event at the pixel, i.e.,\nDL\u00f0xk;tk\u00de\u00bc:L\u00f0xk;tk\u00de/C0L\u00f0xk;tk/C0Dtk\u00de; (1)\nreaches a temporal contrast threshold /C6C(Fig. 1b), i.e.,\nDL\u00f0xk;tk\u00de\u00bcpkC; (2)where C> 0,Dtkis the time elapsed since the last event at\nthe same pixel, and the polarity pk2f \u00fe 1;/C01gis the sign of\nthe brightness change [2].\nThe contrast sensitivity Cis determined by the pixel bias\ncurrents [56], [57], which set the speed and threshold vol-\ntages of the change detector in Fig. 1 and are generated by\nan on-chip digitally-programmed bias generator. The\nsensitivity Ccan be estimated knowing these currents [56].\nIn practice, positive (\u201cON\u201d) and negative (\u201cOFF\u201d) events\nmay be triggered according to different thresholds, C\u00fe;C/C0.\nTypical DVS\u2019s [2], [5] can set thresholds between 10 to 50\npercent illumination change. The lower limit on Cis deter-\nmined by noise and pixel-to-pixel mismatch (variability);\nsetting Ctoo low results in a storm of noise events, starting\nfrom pixels with low values of C. Experimental DVS\u2019s with\nhigher photoreceptor gain are capable of lower thresholds,\ne.g., 1 percent [58], [59], [60]; however these values are only\nobtained under very bright illumination and ideal condi-\ntions. Fundamentally, the pixel must react to a small change\nin the photocurrent in spite of the shot noise present in this\ncurrent. This shot noise limitation sets the relation between\nthreshold and speed of the DVS under a particular illumina-\ntion and desired detection reliability condition [60], [61].\nEvents and the Temporal Derivative of Brightness . Eq. (2)\nstates that event camera pixels set a threshold on magnitude\nof the brightness change since the last event happened. For\na small Dtk, such an increment (2) can be approximated\nusing Taylor\u2019s expansion by DL\u00f0xk;tk\u00de/C25@L\n@t\u00f0xk;tk\u00deDtk,\nwhich allows us to interpret the events as providing infor-\nmation about the temporal derivative\n@L\n@t\u00f0xk;tk\u00de/C25pkC\nDtk: (3)\nThis is an indirect way of measuring brightness, since with\nstandard cameras we are used to measuring absolute\nbrightness. Note that DVS events are triggered by a change\nin brightness magnitude (2), not by the brightness deriva-\ntive (3) exceeding a threshold. The above interpretation\nmay be taken into account to design physically-grounded\nevent-based algorithms, such as [7], [23], [24], [28], [62], [63],\n[64], [65], as opposed to algorithms that simply process\nevents as a collection of points with vague photometric\nmeaning.\nEvents are Caused by Moving Edges . Assuming constant\nillumination, linearizing (2) and using the brightness con-\nstancy assumption one can show that events are caused by\nmoving edges. For small Dt, the intensity increment (2) can\nbe approximated by8\nDL/C25/C0 r L/C1vDt; (4)\nthat is, it is caused by a brightness gradient rL\u00f0xk;tk\u00de\u00bc\n\u00f0@xL;@yL\u00de>moving with velocity v\u00f0xk;tk\u00deon the image plane,\nover a displacement Dx\u00bc:vDt.\nProbabilistic Event Generation Models . Eq. (2) is an ideal-\nized model for the generation of events. A more realistic\n8.Eq. (4) can be shown [66] by substituting the brightness constancy\nassumption (i.e., optical \ufb02ow constraint)@L\n@t\u00f0x\u00f0t\u00de;t\u00de\u00fer L\u00f0x\u00f0t\u00de;t\u00de/C1_x\u00f0t\u00de\u00bc\n0;with image-point velocity v/C17_x, in Taylor\u2019s approximation\nDL\u00f0x;t\u00de\u00bc:L\u00f0x;t\u00de/C0L\u00f0x;t/C0Dt\u00de/C25@L\n@t\u00f0x;t\u00deDt.GALLEGO ET AL.: EVENT-BASED VISION: A SURVEY 157",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 3
        }
    },
    {
        "page_content": "model takes into account sensor noise and transistor mis-\nmatch, yielding a mixture of frozen and temporally varying\nstochastic triggering conditions represented by a probability\nfunction, which is itself a complex function of local illumina-\ntion level and sensor operating parameters. The measure-\nment of such probability density was shown in [2] (for the\nDVS128), suggesting a normal distribution centered at the\ncontrast threshold C. The 1 swidth of the distribution is typi-\ncally 2-4 percent temporal contrast. This event generation\nmodel can be included in emulators [73] and simulators [74]\nof event cameras, and in event processing algorithms [24],\n[66]. Other probabilistic event generation models have been\nproposed, such as: the likelihood of event generation being\nproportional to the magnitude of the image gradient [75] (for\nscenes where large intensity gradients are the source of most\nevent data), or the likelihood being modeled by a mixture\ndistribution to be robust to sensor noise [7]. Future even\nmore realistic models may include the refractory period (i.e.,\nthe duration in time that the pixel ignores log brightness\nchanges after it has generated an event; the larger the refrac-\ntory period the fewer events are produced by fast moving\nobjects), and bus congestion [76].\n2.5 Event Camera Availability\nTable 1 summarizes the most popular or recent cameras.\nThe numbers therein are approximate since they were not\nmeasured using a common testbed. Event camera character-\nistics are considerably different from other CMOS image\nsensor ( CIS) technology, and so there is a need for an agree-\nment on standard speci\ufb01cations to be better used by\nresearchers. As Table 1 shows, since the \ufb01rst practical event\ncamera [2] there has been a trend mainly to increase spatial\nresolution, increase readout speed, and add features, such\nas: gray level output (in ATIS and DAVIS), integration with\nan Inertial Measurement Unit ( IMU ) [77] and multi-camera\ntimestamp synchronization [78]. IMUs act as a vestibular\nsense that may improve camera pose estimation, as in\nvisual-inertial odometry. Only recently has the focus turned\nmore towards the dif\ufb01cult task of reducing pixel size for\neconomical mass production of sensors with large pixel\narrays. In this respect, 3D wafer stacking fabrication has the\nbiggest impact in reducing pixel size and increasing the \ufb01ll\nfactor.Pixel Size . The most widely used event cameras have\nquite large pixels: 40 mm (DVS128), 30 mm (ATIS), 18.5 mm\n(DAVIS240, DAVIS346) (Table 1). The smallest published\nDVS pixel [68] is 4.86 mm; while conventional global shutter\nindustrial APS are typically in the range of 2 mmt o4 mm.\nLow spatial resolution is certainly a limitation for applica-\ntion, although many of the seminal publications are based on\nthe128/C2128pixel DVS128 [2]. The DVS with largest pub-\nlished array size has only about 1Mpixel spatial resolution\n(1280/C2960pixels [39]). Event camera pixel size has shrunk\npretty closely following feature size scaling, which is\nremarkable considering that a DVS pixel is a mixed-signal\ncircuit, which generally do not scale following technology.\nHowever, achieving even smaller pixels is dif\ufb01cult and may\nrequire abandoning the strictly asynchronous circuit design\nphilosophy that the cameras started with [79]. Camera cost is\nconstrained by die size (since silicon costs about $5-$10/cm2\nin mass production), and optics (designing new mass pro-\nduction miniaturized optics to \ufb01t a different sensor format\ncan cost tens of millions of dollars).\nFill Factor . A major obstacle for early event camera mass\nproduction prospects was the limited \ufb01ll factor of the pixels\n(i.e., the ratio of a pixel\u2019s light sensitive area to its total area).\nBecause the pixel circuit is complex, a smaller pixel area can\nbe used for the photodiode that collects light. For example, a\npixel with 20 percent \ufb01ll factor throws away 4 out of 5 pho-\ntons. Obviously this is not acceptable for optimum perfor-\nmance; nonetheless, even the earliest event cameras could\nsense high contrast features under moonlight illumina-\ntion [2]. Early CIS sensors dealt with this problem by includ-\ning microlenses that focused the light onto the pixel\nphotodiode. What is probably better, however, is to use\nback-side illumination technology ( BSI). BSI \ufb02ips the chip so\nthat it is illuminated from the back, so that in principle the\nentire pixel area can collect photons. Nearly all smartphone\ncameras are now back illuminated, but the additional cost of\nBSI fabrication has meant that only recently BSI event cam-\neras were demonstrated [39], [68], [69], [80]. BSI also brings\nproblems: light can create additional \u2018parasitic\u2019 photocur-\nrents that lead to spurious \u2018leak\u2019 events [56].\nCost. Currently, a practical obstacle to adoption of event\ncamera technology is the high cost of several thousand dol-\nlars per camera, similar to the situation with early time ofTABLE 1\nComparison of Commercial or Prototype Event Cameras\nValues are approximate since there is no standard measurement testbed.158 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 1, JANUARY 2022",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 4
        }
    },
    {
        "page_content": "\ufb02ight, structured lighting and thermal cameras. The high\ncosts are due to non-recurring engineering costs for the sili-\ncon design and fabrication (even when much of it is pro-\nvided by research funding) and the limited samples\navailable from prototype runs. It is anticipated that this\nprice will drop precipitously once this technology enters\nmass production, as shown by the \u201cSamsung SmartThings\nVision\u201d consumer-grade home monitoring device: it con-\ntains an event camera [5] and sells for 100 dollars.\n3E VENT PROCESSING\nOne of the key questions of the paradigm shift posed by\nevent cameras is how to extract meaningful information\nfrom the event data to ful\ufb01ll a given task. This is a very\nbroad question, since the answer is application dependent,\nand it drives the algorithmic design of the task solver.\nEvent cameras acquire information in an asynchronous\nand sparse way, with high temporal resolution and low\nlatency. Hence, the temporal aspect, specially latency, plays\nan essential role in the way events are processed. Depend-\ning on how many events are processed simultaneously, two\ncategories of algorithms can be distinguished: ( i) methods\nthat operate on an event-by-event basis , where the state of the\nsystem (the estimated unknowns) can change upon the\narrival of a single event, thus achieving minimum latency,\nand ( ii) methods that operate on groups or packets of events ,\nwhich introduce some latency. Discounting latency consid-\nerations, methods based on groups (i.e., temporal windows)\nof events can still provide a state update upon the arrival of\neach event if the window slides by one event. Hence, the\ndistinction between both categories is more subtle: an event\nalone does not provide enough information for estimation,\nand so additional information, in the form of past events or\nextra knowledge, is needed. We review this categorization.\nOrthogonally, depending on how events are processed,\nwe can distinguish between model-based approaches and\nmodel-free (i.e., data-driven, machine learning) approaches.\nAssuming events are processed in an optimization frame-\nwork, another classi\ufb01cation concerns the type of objective\nor loss function used: geometric- versus temporal- versus\nphotometric-based (e.g., a function of the event polarity or\nthe event activity). Each category presents methods with\nadvantages and disadvantages and current research focuses\non exploring the possibilities that each method can offer.\n3.1 Event Representations\nEvents are processed and often transformed into alternative\nrepresentations (Fig. 3) that facilitate the extraction of mean-\ningful information (\u201cfeatures\u201d) to solve a given task. Herewe review popular representations of event data. Several of\nthem arise from the need to aggregate the little information\nconveyed by individual events in the absence of additional\nknowledge. Some representations are simple, hand-crafted\ndata transformations whereas others are more elaborate.\nIndividual events ek\u00bc:\u00f0xk;tk;pk\u00deare used by event-by-\nevent processing methods, such as probabilistic \ufb01lters and\nSpiking Neural Networks ( SNNs ) (Section 3.3). The \ufb01lter or\nSNN has additional information, built up from past events\nor given by additional knowledge, that is fused with the\nincoming event asynchronously to produce an output.\nExamples include: [7], [24], [62], [84], [85].\nEvent Packet . Events E\u00bc:fekgNe\nk\u00bc1in a spatio-temporal\nneighborhood are processed together to produce an output.\nPrecise timestamp and polarity information is retained by\nthis representation. Choosing the appropriate packet size\nNeis critical to satisfy the assumptions of the algorithm\n(e.g., constant motion speed during the span of the packet),\nwhich varies with the task. Examples are [18], [19], [86], [87].\nEvent Frame/Image or 2D Histogram . The events in a spa-\ntio-temporal neighborhood are converted in a simple way\n(e.g., by counting events or accumulating polarity pixel-\nwise) into an image (2D grid) that can be fed to image-based\ncomputer vision algorithms. Some algorithms may work in\nspite of the different statistics of event frames and natural\nimages. Such histograms can provide a natural activity-\ndriven sample rate; see [88] for methods to accumulate such\nframes for computing \ufb02ow. However, this practice is not\nideal in the event-based paradigm because it quantizes\nevent timestamps, can discard sparsity (but see [89]), and\nthe resulting images are highly sensitive to the number of\nevents used. Nevertheless the high impact of event frames\nin the literature [23], [26], [64], [88], [90], [91] is clear because\n(i) they are a simple way to convert an unfamiliar event\nstream into a familiar 2D representation containing spatial\ninformation about scene edges, which are the most informa-\ntive regions in natural images, ( ii) they inform not only\nabout the presence of events but also about their absence\n(which is informative), ( iii) they have an intuitive interpreta-\ntion (e.g., an edge map, a brightness increment image) and\n(iv) they are the data structure compatible with conven-\ntional computer vision.\nTime Surface (TS) . A TS is a 2D map where each pixel stores\na single time value (e.g., the timestamp of the last event at that\npixel [92], [93]). Thus events are converted into an image\nwhose \u201cintensity\u201d is a function of the motion history at that\nlocation, with larger values corresponding to a more recent\nmotion. TSs are called Motion History Images in classical com-\nputer vision [94]. They explicitly expose the rich temporal\ninformation of the events and can be updated asynchronously.\nFig. 3. Several event representations (Section 3.1) of the slider_depth sequence [81]. From let to right: events in space time, colored according to\npolarity (positive in blue, negative in red). Event frame (brightness increment image DL\u00f0x\u00de). Time surface with last timestamp per pixel (darker pixels\nindicate recent time), only for negative events. Interpolated voxel-grid ( 240/C2180/C210voxels), colored according to polarity , from dark (negative) to\nbright (positive). Motion-compensated event image [82] (sharp edges obtained by event accumulation are darker than pixels with no events, in white) .\nReconstructed intensity image by [8]. Grid-like representations are compatible with conventional computer vision methods [83].GALLEGO ET AL.: EVENT-BASED VISION: A SURVEY 159",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 5
        }
    },
    {
        "page_content": "Using an exponential kernel, TSs emphasize recent events over\npast events. To achieve invariance to motion speed, normaliza-\ntion is proposed [95], [96]. Compared to other grid-like repre-\nsentations of events, TSs highly compress information as they\nonly keep one timestamp per pixel, thus their effectiveness\ndegrades on textured scenes, in which pixels spike frequently.\nTo make TSs less sensitive to noise, each pixel value may be\ncomputed by \ufb01ltering the events in a space-time window [97].\nMore examples include [21], [98], [99], [100].\nVoxel Grid . is a space-time (3D) histogram of events,\nwhere each voxel represents a particular pixel and time\ninterval. This representation preserves better the temporal\ninformation of the events by avoiding to collapse them on a\n2D grid (Fig. 3). If polarity is used the voxel grid is an intui-\ntive discretization of a scalar \ufb01eld (polarity p\u00f0x; y; t\u00deor\nbrightness variation @L\u00f0x; y; t\u00de=@t) de\ufb01ned on the image\nplane, with absence of events marked by zero polarity. Each\nevent\u2019s polarity may be accumulated on a voxel [101], [102]\nor spread among its closest voxels using a kernel [8], [103],\n[104]. Both schemes quantize event timestamps but the lat-\nter (interpolated voxel grid) provides sub-voxel accuracy.\n3D Point Set . Events in a spatio-temporal neighbor-\nhood are treated as points in 3D space, \u00f0xk;yk;tk\u00de2R3.\nThus the temporal dimension becomes a geometric one.\nIt is a sparse representation, and is used on point-based\ngeometric processing methods, such as plane \ufb01tting [21]\nor PointNet [105].\nPoint Sets on Image Plane . Events are treated as an evolv-\ning set of 2D points on the image plane. It is a popular repre-\nsentation among early shape tracking methods based on\nmean-shift or ICP [106], [107], [108], [109], [110], where\nevents provide the only data needed to track edge patterns.\nMotion-compensated event image [111], [112]: is a represen-\ntation that depends not only on events but also on motion\nhypothesis. The idea of motion compensation is that, as an\nedge moves on the image plane, it triggers events on the\npixels it traverses; the motion of the edge can be estimated\nby warping the events to a reference time and maximizing\ntheir alignment, producing a sharp image (i.e., histogram)\nof warped events (IWE) [112]. Hence, this representation\n(IWE) suggests a criterion to measure how well events \ufb01t a\ncandidate motion: the sharper the edges produced by warp-\ning events, the better the \ufb01t [82]. Moreover, the resulting\nmotion-compensated images have an intuitive meaning\n(i.e., the edge patterns causing the events) and provide a\nmore familiar representation of visual information than the\nevents. In a sense, motion compensation reveals a hidden\n(\u201cmotion-invariant\u201d) map of edges in the event stream. The\nimages may be useful for further processing, such as feature\ntracking [64], [113]. There are motion-compensated versions\nof point sets [114], [115] and time surfaces [116], [117].\nReconstructed Images . Brightness images obtained by\nimage reconstruction (Section 4.5) can be interpreted as a\nmore motion-invariant representation than event frames or\nTSs, and be used for inference [8] yielding \ufb01rst-rate results.\nA general framework for converting event data into some\nof the above grid-based representations is presented in [83].\nIt also studies how the choice of representation passed to an\narti\ufb01cial neural network ( ANN ) affects task performance\nand consequently proposes to automatically learn the repre-\nsentation that maximizes such performance.3.2 Methods for Event Processing\nEvent processing systems consist of several stages: pre-proc-\nessing (input adaptation), core processing (feature extraction\nand analysis) and post-processing (output creation). The\nevent representations in Section 3.1 may occur at different\nstages: for example, in [111] an event packet is used at pre-\nprocessing, and motion-compensated event images are the\ninternal representation at the core processing stage.\nThe methods used to process events are in\ufb02uenced by the\nchoice of representation and hardware platform available.\nThese three factors in\ufb02uence each other. For example, it is\nnatural to use dense representations and design algorithms\naccordingly that are executed on standard processors (e.g.,\nCPUs or GPUs). At the same time, it is also natural to pro-\ncess events one-by-one on SNNs (Section 3.3) that are imple-\nmented on neuromorphic hardware (Section 5.1), in search\nfor more ef\ufb01cient and low-latency solutions. Major expo-\nnents of event-by-event methods are \ufb01lters (deterministic or\nprobabilistic) and SNNs. For events processed in packets\nthere are also many methods: hand-crafted feature extrac-\ntors, deep neural networks ( DNNs ), etc. Next, we review\nsome of the most common methods.\nEvent-by-Event\u2013Based Methods . Deterministic \ufb01lters, such\nas (space-time) convolutions and activity \ufb01lters have been\nused for noise reduction, feature extraction [118], image\nreconstruction [62], [119] and brightness \ufb01ltering [63], among\nother applications. Probabilistic \ufb01lters (Bayesian methods),\nsuch as Kalman- and particle \ufb01lters have been used for pose\ntracking in SLAM systems [7], [24], [25], [75], [84]. These\nmethods rely on the availability of additional information\n(typically \u201cappearance\u201d information, e.g., grayscale images\nor a map of the scene), which may be provided by past events\nor by additional sensors. Then, each incoming event is com-\npared against such information and the resulting mismatch\nprovides innovation to update the \ufb01lter state. Filters are a\ndominant class of methods for event-by-event processing\nbecause they naturally ( i) handle asynchronous data, thus\nproviding minimum processing latency, preserving the\nsensor\u2019s characteristics, and ( ii) aggregate information from\nmultiple small sources (e.g., events).\nThe other dominant class of methods takes the form of a\nmulti-layer ANN (whether spiking or not) containing many\nparameters which must be computed from the event data.\nNetworks trained with unsupervised learning typically act\nas feature extractors for a classi\ufb01er (e.g., SVM), which still\nrequires some labeled data for training [15], [93], [120]. If\nenough labeled data is available, supervised learning meth-\nods such as backpropagation can be used to train a network\nwithout the need for a separate classi\ufb01er. Many approaches\nuse packets of events during training (deep learning on\nframes), and later convert the trained network to an SNN\nthat processes data event-by-event [121], [122], [123], [124],\n[125]. Event-by-event model-free methods have mostly\nbeen applied to classify objects [15], [93], [121], [122] or\nactions [16], [17], [126], and have targeted embedded appli-\ncations [121], often using custom SNN hardware [15], [17]\n(Section 5.1). SNNs trained with deep learning typically\nprovide higher accuracy than those relying on unsupervised\nlearning for feature extraction, but there is growing interest\nin \ufb01nding ef\ufb01cient ways to implement supervised learning\ndirectly in SNNs [126], [127] and in embedded devices [128].160 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 1, JANUARY 2022",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 6
        }
    },
    {
        "page_content": "Methods for Groups of Events . Because each event carries\nlittle information and is subject to noise, several events are\noften processed together to yield a suf\ufb01cient signal-to-noise\nratio for the problem considered. Methods for groups of\nevents use the above representations (event packet, event\nframe, etc.) to gather the information contained in the events\nin order to estimate the problem unknowns, usually with-\nout requiring additional data. Hence, events are processed\ndifferently depending on their representation.\nMany representations just perform data pre-processing\nto enable the re-utilization of image-based computer vision\ntools. In this respect, event frames are a practical repres-\nentation that has been used by multiple methods on various\ntasks. In [90], [129] event frames allow to re-utilize tradi-\ntional stereo methods, providing modest results. They also\nprovide an adaptive frame rate signal that is pro\ufb01table for\ncamera pose estimation [26] (by image alignment) or optical\n\ufb02ow computation [88] (by block matching). Event frames\nare also a simple yet effective input for image-based learn-\ning methods (DNNs, SVMs, Random Forests) [22], [91],\n[130], [131]. Few works design algorithms taking into\naccount their photometric meaning (4). This was done in\n[23], showing that such a simple representation allows to\njointly compute several visual quantities of interest (optical\n\ufb02ow, brightness, etc.). Intensity increment images (4) are\nalso used for feature tracking [64], image deblurring [28] or\ncamera tracking [65].\nBecause time surfaces (TSs) are sensitive to scene edges and\nthe direction of motion they have been utilized for many\ntasks involving motion analysis and shape recognition. For\nexample, \ufb01tting local planes to the TS yields optical \ufb02ow\ninformation [21], [132]. TSs are used as building blocks of\nhierarchical feature extractors, similar to neural networks,\nthat aggregate information from successively larger space-\ntime neighborhoods and is then passed to a classi\ufb01er for rec-\nognition [93], [97]. TSs provide proxy intensity images for\nmatching in stereo methods [100], [133], where the photo-\nmetric matching criterion becomes temporal: matching pix-\nels based on event concurrence and similarity of event\ntimestamps across image planes. Recently, TSs have been\nprobed as input to convolutional ANNs ( CNNs ) to compute\noptical \ufb02ow [22], where the network acts both as feature\nextractor and velocity regressor. TSs are popular for corner\ndetection using adaptations of image-based methods (Har-\nris, FAST) [95], [98], [99] or new learning-based ones [96].\nHowever, their performance degrades on highly textured\nscenes [99] due to the \u201cmotion overwriting\u201d problem [94].\nMethods working on voxel grids include variational opti-\nmization and ANNs (e.g., DNNs). They require more mem-\nory and often more computations than methods working on\nlower dimensional representations but are able to provide\nbetter results because temporal information is better pre-\nserved. In these methods voxel grids are used as an internal\nrepresentation [101] (e.g., to compute optical \ufb02ow) or as the\nmultichannel input/output of a DNN [103], [104]. Thus,\nvoxel grids are processed by means of convolutions [103],\n[104] or the operations derived from the optimality condi-\ntions of an objective function [101].\nOnce events have been converted to grid-like representa-\ntions, countless tools from conventional vision can be\napplied to extract information: from feature extractors (e.g.,CNNs) to similarity metrics (e.g., cross-correlation) that\nmeasure the goodness of \ufb01t or consistency between data\nand task-model hypothesis (the degree of event alignment,\netc.). Such metrics are used as objective functions for classi-\n\ufb01cation (SVMs, CNNs), clustering, data association, motion\nestimation, etc. In the neuroscience literature there are\nefforts to design metrics that act directly on spikes (e.g.,\nevent stream), to avoid the issues that arise due to data\nconversion.\nDeep learning methods for groups of events consist of a\ndeep neural network (DNN). Sample applications include\nclassi\ufb01cation [134], [135], image reconstruction [8], [102],\nsteering angle prediction [91], [136], and estimation of opti-\ncal \ufb02ow [22], [103], [137], depth [137] or ego-motion [103].\nThese methods differentiate themselves mainly in the repre-\nsentation of the input and in the loss functions optimized\nduring training. Several representations have been used,\nsuch as event images [91], [131], TSs [22], [117], [137], voxel\ngrids [103], [104] or point sets [105] (Section 3.1). While loss\nfunctions in classi\ufb01cation tasks use manually annotated\nlabels, networks for regression tasks from events may be\nsupervised by a third party ground truth (e.g., a pose) [91],\n[131] or by an associated grayscale image [22] to measure\nphotoconsistency, or be completely unsupervised (depend-\ning only on the training input events) [103], [137]. Loss func-\ntions for unsupervised learning from events are studied\nin [82]. In terms of architecture, most networks have an\nencoder-decoder structure, as in Fig. 4. Such a structure\nallows the use of convolutions only, thus minimizing the\nnumber of network weights. Moreover, a loss function can\nbe applied at every spatial scale of the decoder.\nFinally, motion compensation is a technique to estimate the\nparameters of the motion that best \ufb01ts a group of events. It\nhas a continuous-time warping model that allows to exploit\nthe \ufb01ne temporal resolution of events (Section 3.1), and\nhence departs from conventional image-based algorithms.\nMotion compensation can be used to estimate ego-motion\n[111], [112], optical \ufb02ow [103], [112], [114], [138], depth [19],\n[82], [112], motion segmentation [116], [138], [139] or feature\nmotion for VIO [113], [115]. The technique in [87] also has a\ncontinuous-time motion model, albeit not used for motion\ncompensation but rather to fuse event data with IMU data.\nTo \ufb01nd the parameters of the continuous-time motion mod-\nels [82], [87], standard optimization methods, e.g., conjugate\ngradient or Gauss-Newton, may be applied.\nFig. 4. Events in a space-time volume are converted into an interpolated\nvoxel grid (left) that is fed to a DNN to compute optical \ufb02ow and ego-\nmotion in an unsupervised manner [103]. Thus, modern tensor-based\nDNN architectures are re-utilized using novel loss functions (e.g., motion\ncompensation) adapted to event data.GALLEGO ET AL.: EVENT-BASED VISION: A SURVEY 161",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 7
        }
    },
    {
        "page_content": "Thenumber of events per group (i.e., size of the spatio-tem-\nporal neighborhood) is an important hyper-parameter of\nmany methods. It highly depends on the processing algo-\nrithm and the available resources, and accepts multiple\nselection strategies [11], [88], [102], [111], such as constant\nnumber of events, constant observation time (i.e., constant\nframe rate), or more adaptive ones (thresholding the num-\nber of events in regions of the image plane) [88]. Utilizing a\nconstant number of events \ufb01ts naturally with the cam-\nera\u2019s output rate but it does not account for spatial varia-\ntions of the rate. A constant frame rate selects a varying\nnumber of events, which may be too few or too many,\ndepending on the scene. Criteria more adapted to the\nscene dynamics (in time and space) are often preferred\nbut nontrivial to design.\n3.3 Biologically Inspired Visual Processing\nBiological principles and computational primitives drive\nthe design of event camera pixels and some of the event-\nprocessing algorithms (and hardware), such as Spiking\nNeural Networks ( SNNs ).\nVisual Pathways . The DVS [2] was inspired by the func-\ntion of biological visual pathways, which have \u201ctransient\u201d\npathways dedicated to processing dynamic visual informa-\ntion in the so-called \u201cwhere\u201d pathway. Animals ranging\nfrom insects to humans all have these transient pathways.\nIn humans, the transient pathway occupies about 30 percent\nof the visual system. It starts with transient ganglion cells,\nwhich are mostly found in retina outside the fovea. It con-\ntinues with magno layers of the thalamus and particular\nsublayers of area V1. It then continues to area MT and MST,\nwhich are part of the dorsal pathway where many motion\nselective cells are found [45]. The DVS corresponds to the\npart of the transient pathway(s) up to retinal ganglion cells.\nSimilarly, the grayscale (EM) events of the ATIS correspond\nto the \u201csustained\u201d or \u201cwhat\u201d pathway through the parvo\nlayers of the brain [36], [43].\nEvent Processing by SNNs . Arti\ufb01cial neurons, such as\nLeaky-Integrate and Fire or Adaptive Exponential, are\ncomputational primitives inspired in neurons found in the\nmammalian\u2019s visual cortex. They are the basic building\nblocks of arti\ufb01cial SNNs. A neuron receives input spikes\n(\u201cevents\u201d) from a small region of the visual space (a receptive\n\ufb01eld), which modify its internal state (membrane potential)\nand produce an output spike (action potential) when the\nstate surpasses a threshold. Neurons are connected in a hier-\narchical way, forming an SNN. Spikes may be produced by\npixels of the event camera or by neurons of the SNN. Infor-\nmation travels along the hierarchy, from the event camera\npixels to the \ufb01rst layers of the SNN and then through to\nhigher (deeper) layers. Most \ufb01rst layer receptive \ufb01elds are\nbased on Difference of Gaussians (selective to center-sur-\nround contrast), Gabor \ufb01lters (selective to oriented edges),\nand their combinations. The receptive \ufb01elds become increas-\ningly more complex as information travels deeper into the\nnetwork. In ANNs, the computation performed by inner\nlayers is approximated as a convolution. One common\napproach in arti\ufb01cial SNNs is to assume that a neuron will\nnot generate any output spikes if it has not received any\ninput spikes from the preceding SNN layer. This assumption\nallows computation to be skipped for such neurons. Theresult of this visual processing is almost simultaneous with\nthe stimulus presentation [140], which is very different from\ntraditional CNNs, where convolution is computed simulta-\nneously at all locations at \ufb01xed time intervals.\nTasks . Bio-inspired models have been adopted for several\nlow-level visual tasks. For example, event-based optical \ufb02ow\ncan be estimated by using spatio-temporally oriented \ufb01l-\nters [92], [118], [141] that mimic the working principle of\nreceptive \ufb01elds in the primary visual cortex [142], [143]. The\nsame type of oriented \ufb01lters have been used to implement a\nspike-based model of selective attention [144] based on the bio-\nlogical proposal from [145]. Bio-inspired models from binoc-\nular vision, such as recurrent lateral connectivity and\nexcitatory-inhibitory neural connections [146], have been\nused to solve the event-based stereo correspondence prob-\nlem [41], [147], [148], [149], [150] or to control binocular ver-\ngence on humanoid robots [151]. The visual cortex has also\ninspired the hierarchical feature extraction model proposed\nin [152], which has been implemented in SNNs and used for\nobject recognition . The performance of such networks\nimproves the better they extract information from the precise\ntiming of the spikes [153]. Early networks were hand-crafted\n(e.g., Gabor \ufb01lters) [53], but recent efforts let the network\nbuild receptive \ufb01elds through brain-inspired learning, such\nas Spike-Timing Dependent Plasticity ( STDP ), yielding bet-\nter recognition rates [120]. This research is complemented by\napproaches where more computationally inspired types of\nsupervised learning, such as back-propagation, are used in\ndeep networks to ef\ufb01ciently implement spiking deep convo-\nlutional networks [127], [154], [155], [156], [157]. The advan-\ntages of the above methods over their traditional vision\ncounterparts are lower latency and higher ef\ufb01ciency.\n4A LGORITHMS /APPLICATIONS\nIn this section, we review several works on event-based\nvision, presented according to the task addressed. We start\nwith low-level vision on the image plane, such as feature\ndetection, tracking, and optical \ufb02ow estimation. Then, we\ndiscuss tasks that pertain to the 3D structure of the scene,\nsuch as depth estimation, visual odometry ( VO) and histori-\ncally related subjects, e.g., intensity image reconstruction.\nFinally, we consider motion segmentation, recognition and\ncoupling perception with control.\n4.1 Feature Detection and Tracking\nFeature detection and tracking on the image plane are fun-\ndamental building blocks of many vision tasks such as\nvisual odometry, object segmentation and scene under-\nstanding. Event cameras make it possible to track asynchro-\nnously, adapted to the dynamics of the scene and with low\nlatency, high dynamic range and low power (Section 2.2).\nThus, they allow to track in the \u201cblind\u201d time between the\nframes of a standard camera. To do so, the methods devel-\noped need to deal with the unique space-time and photo-\nmetric characteristics of the visual signal: events report only\nbrightness changes, asynchronously (Section 2.3).\nChallenges . Since events represent brightness changes,\nwhich depend on motion direction, one of the main chal-\nlenges of feature detection and tracking with event cameras\nis overcoming the variation of scene appearance caused by162 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 1, JANUARY 2022",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 8
        }
    },
    {
        "page_content": "such motion dependency (Fig. 5). Tracking requires the\nestablishment of correspondences between events (or fea-\ntures built from the events) at different times (i.e., data asso-\nciation), which is dif\ufb01cult due to the varying appearance.\nThe second main challenge consists of dealing with\nsensor noise and possible event clutter caused by the cam-\nera motion.\nLiterature Review . Early event-based feature methods\nwere very simple and focused on demonstrating the low-\nlatency and low-processing requirements of event-driven\nvision systems. Hence they assumed a stationary camera\nscenario and tracked moving objects as clustered blob-like\nsources of events [6], [12], [14], [106], [158], circles [159] or\nlines [54]. Only pixels that generated events needed to be\nprocessed. Simple Gaussian correlation \ufb01lters suf\ufb01ced to\ndetect blobs of events, which could be modeled by Gaussian\nMixtures [160]. For tracking, each incoming event was asso-\nciated to the nearest existing blob/feature and used to asyn-\nchronously update its parameters (location, size, etc.).\nCircles [159] and lines [54] were treated as blobs in the\nHough transform space. These methods were used in traf\ufb01c\nmonitoring and surveillance [14], [106], [160], high-speed\nrobotic tracking [6], [12] and particle tracking in \ufb02uids [158]\nor microrobotics [159]. However, they worked only for a\nlimited class of object shapes.\nTracking of more complex, high-contrast user-de\ufb01ned\nshapes has been demonstrated using event-by-event adapta-\ntions of the Iterative Closest Point (ICP) algorithm [107],\ngradient descent [108], Mean-shift and Monte-Carlo meth-\nods [161], or particle \ufb01ltering [162]. The iterative methods\nin [107], [108] used a nearest-neighbor strategy to associate\nincoming events to the target shape and update its transfor-\nmation parameters, showing very high-speed tracking\n(200kHz equivalent frame rate). Other works [161] handled\ngeometric transformations of the target shape ( aka\u201ckernel\u201d)\nby matching events against a pool of rotated and scaled ver-\nsions of it. The prede\ufb01ned kernels tracked the object with-\nout overlapping themselves due to a built-in repulsion\nmechanism. Complex objects, such as faces or human bod-\nies, have been tracked with part-based shape models [163],\nwhere objects are represented as a set of basic elements\nlinked by springs [164]. The part trackers simply follow\nincoming blobs of events generated by ellipse-like shapes,\nand the elastic energy of this virtual mechanical system pro-\nvides a quality criterion for tracking. In most trackingmethods events are treated as individual points (without\npolarity) and update the system\u2019s state asynchronously,\nwith minimal latency. The performance of the methods\nstrongly depends on the tuning of several model parame-\nters, which is done experimentally according to the object to\ntrack [161], [163].\nThe previous methods require a priori knowledge or user\ninput to determine the objects to track. This restriction is\nvalid for scenarios like tracking cars on a highway or balls\napproaching a goal, where knowing the objects greatly sim-\npli\ufb01es the computations. But when the space of objects\nbecomes larger, methods to determine more realistic features\nbecome necessary. The features proposed in [109], [114] con-\nsist of local edge patterns that are represented as point sets.\nIncoming events are registered to them by means of some\nform of ICP. Other methods [27], [113] proposed to re-utilize\nwell-known feature detectors [165] and trackers [166] on\npatches of motion-compensated event images (Section 3.1),\nproviding good results. All these methods allowed to track\nfeatures for cameras moving in natural scenes, hence\nenabling ego-motion estimation in realistic scenarios [110],\n[113], [115]. Features built from motion-compensated events\n(in image form [113] or point-set form [114]) provide a use-\nful representation of edge patterns. However, they depend\non motion direction, and, therefore, trackers suffer from\ndrift as event appearance changes over time [64]. To track\nwith no drift, motion-invariant features are needed.\nCombining Events and Frames . Data association (Fig. 5)\nsimpli\ufb01es if the absolute intensity of the pattern to be\ntracked (Fig. 5c, i.e., a motion-invariant representation or\n\u201cmap\u201d of the feature) is available. This is the approach fol-\nlowed by works that leverage the strengths of a combined\nframe- and event-based sensor ( /C18a la DAVIS [4]). The algo-\nrithms in [64], [109], [110] automatically detect arbitrary\nedge patterns (features) on the frames and track them asyn-\nchronously with events. The feature location is given by the\nHarris corner detector [165] and the feature descriptor is\ngiven by the edge pattern around the corner: [109], [110]\nconvert Canny edges to point sets used as templates for ICP\ntracking, thus they assume events are mostly triggered at\nstrong edges. In contrast, the edge pattern in [64] is given by\nthe frame intensities, and tracking consists of \ufb01nding the\nmotion parameters that minimize the photometric error\nbetween the events and their frame prediction using a\ngenerative model (4). A comparison of \ufb01ve feature track-\ners is provided in [64], showing that the generative\nm o d e li sm o s ta c c u r a t e ,w i t hs u b - p i x e lp e r f o r m a n c e ,\nalbeit it is computationally expensive. Finally, [64] also\nshows the interesting fact that an event-based sensor suf-\n\ufb01ces: frames can be replaced by images reconstructed\nfrom events (Section 4.5) and s till achieve similar detec-\ntion and tracking results.\nCorner Detection and Tracking . Since event cameras natu-\nrally respond to edges in the scene, they shorten the detec-\ntion of lower-level primitives such as keypoints or \u201ccorners\u201d.\nSuch primitives identify pixels of interest around which local\nfeatures can be extracted without suffering from the aperture\nproblem, and therefore provide reliable tracking informa-\ntion. The method in [167] computes corners as the intersec-\ntion of two moving edges, which are obtained by \ufb01tting\nplanes in the space-time stream of events. To deal with event\nFig. 5. The challenge of data association. Panels (a) and (b) show events\nfrom a scene (c) under two different motion directions: (a) diagonal and\n(b) up-down. Intensity increment images (a) and (b) are obtained by accu-\nmulating event polarities over a short time interval: pixels that do not\nchange intensity are represented in gray , whereas pixels that increased\nor decreased intensity are represented in bright and dark, respectively .\nClearly, it is not easy to establish event correspondences between (a)\nand (b) due to the changing appearance of the edge patterns in (c) with\nrespect to the motion. Image adapted from [64].GALLEGO ET AL.: EVENT-BASED VISION: A SURVEY 163",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 9
        }
    },
    {
        "page_content": "noise, least-squares is supplemented by a sampling tech-\nnique similar to RANSAC. This method of \ufb01tting planes\nlocally to time surfaces has also been pro\ufb01table to estimate\noptical \ufb02ow [21] and \u201cevent lifetime\u201d [132], which are\nobtained from the coef\ufb01cients of the planes. Recently, exten-\nsions of popular frame-based keypoint detectors, such as\nHarris [165] and FAST [168], have been developed for event\ncameras [95], [98], [99], by operating on time surfaces (TSs)\nas if they were natural intensity images. In [98] the TS is\nbinarized before applying the derivative \ufb01lters of Harris\u2019\ndetector. To speed up detection, [99] replaces the derivative\n\ufb01lters with pixelwise comparisons on two concentric circles\nof the TS around the current event. Moving corners produce\nlocal TSs with two clearly separated regions: recent versus\nold events. Hence, corners are obtained by searching for arcs\nof contiguous pixels with higher TS values than the rest. The\nmethod in [95] improves the detector in [99] and proposes a\nstrategy to track the corners. Assuming corners follow con-\ntinuous trajectories on the image plane and the detected\nevent corners are accurate, these are threaded by proximity\nalong trajectories, following a tree-based hypothesis graph.\nThe above TS-based hand-crafted corner detectors suffer\nfrom variations of the TS due to changes in motion direction.\nTo overcome them, [96] proposes a data-driven method to\nlearn the TS appearance of intensity-image corners. To this\nend, a grayscale input (from DAVIS or ATIS camera) pro-\nvides the supervisory signal to label the corners. As a trade-\noff between accuracy and speed, a random forest classi\ufb01er is\nused. Event corners \ufb01nd multiple applications, such as visual\nodometry or ego-motion segmentation [169]; yet there are\nonly a few demonstrations.\nOpportunities . In spite of the abundance of detection and\ntracking methods, they are rarely evaluated on common\ndatasets for performance comparison. Establishing bench-\nmark datasets [170] and evaluation procedures will foster\nprogress in this and other topics. Also, in most algorithms,\nparameters are de\ufb01ned experimentally according to the\ntracking target. It would be desirable to have adaptive\nparameter tuning to increase the range of operation of the\ntrackers. Learning-based feature detection and tracking\nmethods also offer considerable room for research.\n4.2 Optical Flow Estimation\nOptical \ufb02ow estimation is the problem of computing the\nvelocity of objects on the image plane without knowledge\nabout the scene geometry or motion. The problem is ill-\nposed and thus requires regularization to become tractable.\nEvent-based optical \ufb02ow estimation is challenging\nbecause of the unfamiliar way in which events encode\nvisual information (Section 2). In conventional cameras opti-\ncal \ufb02ow is obtained by analyzing two consecutive images.\nThese provide spatial and temporal derivatives that are\nsubstituted in the brightness constancy assumption (p. 12),\nwhich together with smoothness assumptions provide\nenough equations to solve for the \ufb02ow at each image pixel.\nIn contrast, events provide neither absolute brightness nor\nspatially continuous data. Each event does not carry enough\ninformation to determine \ufb02ow, and so events need to be\naggregated to produce an estimate, which leads to the\nunusual question of where in the x-y-t-space of the image\nplane spanned by the events is \ufb02ow computed. Ideally onewould like to know the \ufb02ow \ufb01eld over the whole space,\nwhich deems computationally expensive. In practice, opti-\ncal \ufb02ow is computed only at speci\ufb01c points: at the event\nlocations, or at images with arti\ufb01cially-chosen times. Never-\ntheless, computing \ufb02ow from events is attractive because\nthey represent edges, which are the parts of the scene where\n\ufb02ow estimation is less ambiguous, and because their \ufb01ne\ntiming information allows measuring high speed \ufb02ow [11].\nFinally, another challenge is to design a \ufb02ow estimation\nalgorithm that is biologically plausible, i.e., compatible with\nwhat is known from neuroscience about early processing in\nthe primate visual cortex, and that can be implemented ef\ufb01-\nciently in neuromorphic processors.\nLiterature Review . Table 2 lists some event-based optical\n\ufb02ow methods, categorized according to different criteria.\nEarly works [172] tried to adapt classical approaches in\ncomputer vision to event-based data (Fig. 6b). These are\nbased on the brightness constancy assumption [166], and\ndiscussion focused on whether events carried enough infor-\nmation to estimate \ufb02ow with such approaches [118]. Events\nallow to estimate the temporal derivative of brightness (3),\nand so additional assumptions were needed to approximate\nthe spatial derivative rLin order to apply such classical\nmethods [166]. However, due to the potentially very small\nnumber of events generated at each pixel as an edge crosses\nover it, it is dif\ufb01cult to estimate derivatives ( rL;@L=@t) reli-\nably [118], which leads gradient-based methods like [172] to\ninconclusive \ufb02ow estimates. Approaches that consider the\nlocal distribution of events in the x-y-t-space, as in [21], are\nmore robust and therefore preferred.\nThe method in [21] reasons about the local distribution of\nevents geometrically, in terms of time surfaces and planar\napproximations. As an edge moves it produces events that\nresemble points on a surface in space-time (the time surface,\nSection 3). The surface slopes in the x-tandy-tcross sections\nencode the edge motion, thus optical \ufb02ow is estimated by\n\ufb01tting planes to the surface and reading the slopes from the\nplane coef\ufb01cients. In spite of providing only normal \ufb02owTABLE 2\nClassi\ufb01cation of Several Optical Flow Methods According\nto Their Output and Design\nReference N/F? S/D? Model? Bio?\nDelbruck [92], [171] Normal Sparse Model Yes\nBenosman et al. [171], [172] Full Sparse Model No\nOrchard et al. [141] Full Sparse ANN Yes\nBenosman et al. [21], [171] Normal Sparse Model No\nBarranco et al. [173] Normal Sparse Model No\nBrosch et al. [118] Normal Sparse Model Yes\nBardow et al. [101] Full Dense Model No\nLiuet al. [88] Full Sparse Model No\nGallego [112], Stoffregen [138] Full Sparse Model No\nHaessig et al. [174] Normal Sparse ANN Yes\nZhu et al. [22], [103] Full Dense ANN No\nYeet al. [137] Full Dense ANN No\nParedes-Vall /C19es [85] Full Sparse ANN Yes\nSome methods provide full motion \ufb02ow (F) whereas others only its component\nnormal to the local brightness edge (N). The output may be a dense (D) \ufb02ow\n\ufb01eld (i.e., optical \ufb02ow for every pixel at some time) or sparse (S) (i.e., \ufb02ow com-\nputed at selected pixels). According to their design, methods may be model-\nbased or model-free (Arti\ufb01cial Neural Network - ANN), and neuro-biologically\ninspired or not.164 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 1, JANUARY 2022",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 10
        }
    },
    {
        "page_content": "(i.e., the component of the optical \ufb02ow perpendicular to the\nedge), the method works even in the case of only a few gen-\nerated events. Of course, the goodness of \ufb01t depends on the\nsize of the spatio-temporal neighborhood (this remark gen-\neralizes to other methods). If the neighborhood is too small\nthen the plane \ufb01t may become arbitrary. If the neighbor-\nhood is too large then the event stream may not be well\napproximated by a local plane.\nA hierarchical architecture for optical \ufb02ow estimation\nbuilding on experimental \ufb01ndings of the primate visual\nsystem is proposed in [118]. It applies a set of spatio-tem-\nporal \ufb01lters on the event stream to yield selectivity to dif-\nferent motion speeds and directions ( /C18al aG a b o r\ufb01 l t e r s )\nwhile maintaining the sparse representation of events.\nSuch \ufb01lters are formally equivalent to spatio-temporal\ncorrelation detectors. Other biologically-inspired methods\n[85], [141] can also be interpreted as \ufb01lter banks sampling\nthe event stream along different spatio-temporal orienta-\ntions; [141] and [118] de\ufb01ne hand-crafted \ufb01lters, whereas\n[85] learns them from event data using a novel STDP\nrule. The SNN in [141] detects motion patterns by delay-\ning events through synaptic connections and employing\nneurons as coincidence detect ors. Its neurons are sensitive\nto 8 speeds and 8 directions (i.e., 64 velocities) over recep-\ntive \ufb01elds of 5/C25pixels. These methods are implement-\nable in neuromorphic hardware, offering low-power,\nef\ufb01cient computations.\nMethods like [23], [101] estimate optical \ufb02ow jointly with\nother quantities, notably image intensity, so that the quanti-\nties involved bring in well-known equations and boost each\nother towards convergence. Knowing image intensity, or\nequivalently ( rL;@L=@t), is desirable since it can be used on\nthe brightness constancy law to provide constraints on the\noptical \ufb02ow. In this respect, [101] combines multiple equa-\ntions ((2), brightness constancy, smoothness priors, etc.) as\npenalty terms into an objective function that is optimized\nvia calculus of variations. The method \ufb01nds the optical \ufb02ow\nand image intensity on the image plane that minimizes the\nobjective function, i.e., that best explains the distribution of\nevents in the x-y-t-space (using a voxel grid). Thus, it out-\nputs a dense \ufb02ow (i.e., \ufb02ow at every pixel). Flow vectors at\npixels where no events were produced (i.e., regions of\nhomogeneous brightness) are due to the smoothness priors,\nthus they are less reliable than those computed at pixels\nwhere events were triggered (i.e., at edges).\nThe method in [88] estimates optical \ufb02ow by computing\nevent frames (Section 3) at an adaptive rate and applyingvideo coding techniques (block matching). It can be inter-\npreted as \ufb01nding the optical \ufb02ow vector that best matches\nthe distributions of events within two cuboids (collapsed\ninto event frames). Thus, the optical \ufb02ow problem is posed\nas that of \ufb01nding event correspondences, i.e., events trig-\ngered by the same scene point (at different times). The\nmethod de\ufb01nes two sets of events (\u201cblocks\u201d) and a similar-\nity metric to compare them. It is assumed that the appear-\nance of event frames do not change signi\ufb01cantly for short\ntimes and hence simple metrics, such as sum of absolute\ndistances, suf\ufb01ce to compare them. The method can be\nimplemented in FPGA, trading off ef\ufb01ciency for accuracy.\nThe framework in [82], [112], [138] computes optical \ufb02ow\nby maximizing the sharpness of image patches obtained by\nwarping cuboids of events, producing motion-compensated\nimages (Section 3). It can be interpreted as applying an\nadaptive \ufb01lter to the events, where the \ufb01lter coef\ufb01cients\nde\ufb01ne the spatio-temporal direction that maximizes the \ufb01l-\nter\u2019s response. Motion compensation was also used to com-\npute \ufb02ow in [114], albeit using point sets.\nRecently, deep learning methods have emerged [22],\n[103], [137]. These are based on the availability of large\namounts of event data paired with an ANN. In [22], an\nencoder-decoder CNN is trained using a self-supervised\nscheme to estimate dense optical \ufb02ow. The loss function\nmeasures the error between DAVIS grayscale images aligned\nusing the \ufb02ow produced by the network. The trained net-\nwork is able to accurately predict optical \ufb02ow from events\nonly, passed as time surfaces and event frames. The\nwork [137] presents the \ufb01rst monocular ANN architecture to\nestimate dense optical \ufb02ow, depth and ego-motion (i.e.,\nlearning structure from motion) from events only. The input\nto the ANN consists of events over multiple time slices, given\nas event frames and time surfaces with average timestamps.\nThis reduces event noise and preserves the structure of the\nevent stream better than [22]. The network is trained unsu-\npervised, measuring the photometric error between the\nevents in neighboring time slices aligned using the estimated\n\ufb02ow. Later, [22] was extended to unsupervised learning of\n\ufb02ow and ego-motion in [103] using a motion-compensation\nloss function in terms of time surfaces.\nEvaluation . Optical \ufb02ow estimation is computationally\nexpensive. Some methods [22], [101], [103], [137] require a\nGPU, while other approaches are more lightweight [88],\nalbeit not as accurate. Few algorithms [21], [88], [118], [141]\nhave been pushed to hardware logic circuits that of\ufb02oad\nCPU and minimize latency. The review [171] compared\nsome early event-based optical \ufb02ow methods [21], [92], [172],\nbut only on \ufb02ow \ufb01elds generated by a rotating camera, i.e.,\nlacking motion parallax and occlusion. For newer methods,\nthere are multiple trade offs (accuracy versus ef\ufb01ciency ver-\nsus latency) that have not been properly quanti\ufb01ed yet.\nOpportunities . Comprehensive datasets with accurate\nground truth optical \ufb02ow in multiple scenarios (varying tex-\nture, speed, parallax, occlusions, illumination, etc.) and a\ncommon evaluation methodology would be essential to\nassess progress and reproducibility in this paramount low-\nlevel vision task. Providing ground truth event-based optical\n\ufb02ow in real scenes is challenging, especially for moving\nobjects not conforming to the motion \ufb01eld induced by the\ncamera\u2019s ego-motion. A thorough quantitative comparison\nFig. 6. Two optical \ufb02ow estimation examples. (a) and (b): indoor \ufb02ying\nscene [175]. In (a), events (polarity shown in red/blue) are overlaid on a\ngrayscale frame from a DAVIS. (b) shows the sparse optical \ufb02ow (col-\nored according to magnitude and direction) computed using [166] on\nbrightness increment images. (c) A different scene: dense optical \ufb02ow of\na \ufb01dget spinner spinning at 750/C14/s in a dark environment [103]. Events\nenable the estimation of optical \ufb02ow in challenging scenarios.GALLEGO ET AL.: EVENT-BASED VISION: A SURVEY 165",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 11
        }
    },
    {
        "page_content": "of existing event-based optical \ufb02ow methods would help\nidentify key ideas to develop improved methods.\n4.3 3D Reconstruction Monocular and Stereo\nDepth estimation with event cameras is a broad \ufb01eld. It can\nbe divided according to the considered scenario and camera\nsetup or motion, which determine the problem assumptions.\nInstantaneous Stereo . Most works on depth estimation with\nevent cameras target the problem of \u201cinstantaneous\u201d stereo,\ni.e., 3D reconstruction using events on a very short time (ide-\nally on a per-event basis) from two or more synchronized\ncameras that are rigidly attached. Being synchronized, the\nevents from different image planes share a common clock.\nThese works follow the classical two-step stereo solution: \ufb01rst\nsolve the event correspondence problem across image planes\n(i.e., epipolar matching) and then triangulate the location of\nthe 3D point [176]. The main challenge is \ufb01nding correspond-\nences between events; it is the computationally intensive\nstep. Events are matched ( i) using traditional stereo metrics\n(e.g., normalized cross-correlation) on event frames [129],\n[177] or time surfaces [133] (Section 3), and/or ( ii) by exploit-\ning simultaneity and temporal correlations of the events\nacross sensors [133], [178], [179]. These approaches are local,\nmatching events by comparing their neighborhoods since\nevents cannot be matched based on individual timestamps\n[154], [180]. Additional constraints, such as the epipolar con-\nstraint [181], ordering, uniqueness, edge orientation and\npolarity may be used to reduce matching ambiguities and\nfalse correspondences, thus improving depth estimation [18],\n[154], [182]. Event matching can also be done by comparing\nlocal context descriptors [183], [184] of the spatial distribution\nof events on both stereo image planes.\nGlobal approaches produce better depth estimates (i.e.,\nless sensitive to ambiguities) than local approaches by con-\nsidering additional regularity constraints. In this category,\nwe \ufb01nd extensions of Marr and Poggio\u2019s cooperative stereo\nalgorithm [146] for the case of event cameras [41], [148],\n[149], [150], [185]. These approaches consist of a network of\ndisparity sensitive neurons that receive events from both\ncameras and perform various operations (ampli\ufb01cation,\ninhibition) that implement matching constraints (unique-\nness, continuity) to extract disparities. They use not only the\ntemporal similarity to match events but also their spatio-\ntemporal neighborhoods, with iterative nonlinear opera-\ntions that result in an overall globally-optimal solution. A\ndiscussion of cooperative stereo is provided in [43]. Also in\nthis category are [186], [187], [188], which use Belief Propa-\ngation on a Markov Random Field or semiglobal match-\ning [189] to improve stereo matching. These methods are\nprimarily based on optimization, trying to de\ufb01ne a well-\nbehaved energy function whose minimizer is the correct\ncorrespondence map. The energy function incorporates reg-\nularity constraints, which enforce coupling of correspond-\nences at neighboring points and therefore make the solution\nmap less sensitive to ambiguities than local methods, at the\nexpense of computational effort. A table comparing differ-\nent stereo methods is provided in [190]; however, it should\nbe interpreted with caution since the methods were not\nbenchmarked on the same dataset.\nRecently, brute-force space-sweeping using dedicated\nhardware (a GPU) has been proposed [191]. The method isbased on ideas similar to [19], [112]: the correct depth manifests\nas \u201cin focus\u201d voxels of displaced events in the Disparity Space\nImage [19], [192]. In contrast, other approaches pair event cam-\neras with neuromorphic processors (Section 5.1) to produce\nfully event-based low-power (100 mW), high-speed stereo sys-\ntems [149], [190]. There is an ef\ufb01ciency versus accuracy trade-\noff that has not been quanti\ufb01ed yet.\nMost of the methods above are demonstrated in scenes\nwith static cameras and few moving objects, so that corre-\nspondences are easy to \ufb01nd due to uncluttered event data.\nEvent matching happens with low latency, at high rate\n(/C241kHz) and consuming little power, which shows that\nevent cameras are promising for high-speed 3D reconstruc-\ntions of moving objects or in uncluttered scenes.\nMonocular Depth Estimation . Depth estimation with a sin-\ngle event camera has been shown in [19], [25], [112]. It is a\nsigni\ufb01cantly different problem from previous ones because\ntemporal correlation between events across multiple image\nplanes cannot be exploited. These methods recover a semi-\ndense 3D reconstruction of the scene (i.e., 3D edge map) by\nintegrating information from the events of a moving camera\nover time, and therefore require knowledge of camera\nmotion. Hence they do not pursue instantaneous depth esti-\nmation, but rather depth estimation for SLAM [193].\nThe method in [25] is part of a pipeline that uses three \ufb01l-\nters operating in parallel to jointly estimate the motion of\nthe event camera, a 3D map of the scene, and the intensity\nimage. Their depth estimation approach requires using an\nadditional quantity\u2014the intensity image\u2014to solve for data\nassociation. In contrast, [19] (Fig. 7) proposes a space-sweep\nmethod that leverages the sparsity of the event stream to\nperform 3D reconstruction without having to establish\nevent matches or recover the intensity images. It back-proj-\nects events into space, creating a ray density volume [194],\nand then \ufb01nds scene structure as local maxima of ray den-\nsity. It is computationally ef\ufb01cient and used for VO in [26].\nOpportunities . Although there are many methods for\nevent-based depth estimation, it is dif\ufb01cult to compare their\nperformance since they are not evaluated on the same data-\nset. In this sense, it would be desirable to ( i) provide a com-\nprehensive dataset and testbed for event-based depth\nevaluation and ( ii) benchmark many existing methods on\nthe dataset, to be able to compare their performance.\n4.4 Pose Estimation and SLAM\nAddressing the Simultaneous Localization and Mapping\n(SLAM) problem with event cameras has been dif\ufb01cult\nbecause most methods and concepts developed for conven-\ntional cameras (feature detection, matching, iterative image\nalignment, etc.) are not applicable or were not available;\nevents are fundamentally different from images. The chal-\nlenge is therefore to design new SLAM techniques that are\nable to unlock the camera\u2019s advantages (Sections 2.3 and 2.2),\nshowing their usefulness to tackle dif\ufb01cult scenarios for cur-\nrent frame-based cameras. Historically, the design goal of\nsuch techniques has focused on preserving the low-latency\nnature of the data, i.e., being able to produce a state estimate\nfor every incoming event (Section 3). However, each event\ndoes not contain enough information to estimate the state\nfrom scratch (e.g., the six degrees of freedom (DOF) pose of a\ncalibrated camera), and so the goal becomes that each event166 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 1, JANUARY 2022",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 12
        }
    },
    {
        "page_content": "be able to asynchronously update the state of the system.\nProbabilistic (Bayesian) \ufb01lters [195] are popular in event-\nbased SLAM [7], [24], [75], [196] because they naturally \ufb01t\nwith this description. Their main adaptation for event cam-\neras consists of designing sensible likelihood functions based\non the event generation process (Section 2.4).\nSince events are caused by the apparent motion of inten-\nsity edges, the majority of maps emerging from SLAM sys-\ntems naturally consist only of scene edges, i.e., semi-dense\nmaps (Fig. 8 and [19]). However, note that an event camera\ndoes not directly measure intensity gradients but only tem-\nporal changes (2), and so the presence, orientation and\nstrength of edges (on the image plane and in 3D) must be\nestimated together with the camera\u2019s motion. The strength\nof the intensity gradient at a scene point is correlated with\nthe \ufb01ring rate of events corresponding to that point, and it\nenables reliable tracking [86]. Edge information for tracking\nmay also be obtained from gradients of brightness maps [7],\n[24], [25] used in generative models (Section 2.4).\nThe event-based SLAM problem in its most general set-\nting (6-DOF motion and natural 3D scenes) is a challenging\nproblem that has been addressed step-by-step in scenarios\nwith increasing complexity. Three complexity axes can be\nidenti\ufb01ed: dimensionality of the problem, type of motion\nand type of scene. The literature is dominated by methods\nthat address the localization subproblem \ufb01rst (i.e., motion\nestimation) because it has fewer degrees of freedom to esti-\nmate. Regarding the type of motion, solutions for con-\nstrained motions, such as rotational or planar (both being 3-\nDOF), have been investigated before addressing the most\ncomplex case of a freely moving camera (6-DOF) . Solutions\nfor arti\ufb01cial scenes in terms of photometry (high contrast)and/or structure (line-based or 2D maps) have been pro-\nposed before focusing on the most dif\ufb01cult case: natural\nscenes (3D and with arbitrary photometric variations).\nSome proposed solutions require additional sensing (e.g.,\nRGB-D) to reduce the complexity of the problem. This, how-\never, introduces some of the bottlenecks present in frame-\nbased systems (e.g., latency and motion blur). Table 3 classi-\n\ufb01es the related work using these complexity axes.\nTracking and Mapping . Let us focus on methods that\naddress the tracking-and-mapping problem. Cook et al. [23]\nproposed a generic message-passing algorithm within an\ninteracting network to jointly estimate ego-motion, image\nintensity and optical \ufb02ow from events. However, the system\nwas restricted to rotational motion. Joint estimation is\nappealing because it allows to employ as many equations as\npossible relating the variables (e.g., (4) and rotational prior)\nin the hope of \ufb01nding a better solution to the problem.\nAn event-based 2D SLAM system was presented in [196]\nby extension of [84], and thus it was restricted to planar\nmotion and high-contrast scenes. The method used a parti-\ncle \ufb01lter for tracking, with the event likelihood function\ninversely related to the the reprojection error of the event\nwith respect to the map. The map of scene edges was con-\ncurrently built; it consisted of an occupancy map [195], with\neach pixel representing the probability that the pixel trig-\ngered events. The method was extended to 3D in [197], but\nit relied on an external RGB-D sensor attached to the event\ncamera for depth estimation. The depth sensor introduced\nbottlenecks, which deprived the system of the low latency\nand high-speed advantages of event cameras.\nThe \ufb01lter-based approach in [24] showed how to simulta-\nneously track the 3D orientation of a rotating event camera\nand create high-resolution panoramas of natural scenes. It\noperated probabilistic \ufb01lters in parallel for both subtasks. A\npanoramic gradient was built using per-pixel Kalman \ufb01l-\nters, each one estimating the orientation and strength of the\nscene edge at its location. This gradient map was then\nupgraded to an absolute intensity one with super-resolution\nand HDR properties by Poisson integration. SLAM during\nrotational motion was also presented in [86], where camera\nFig. 7. Example of monocular depth estimation with a hand-held event\ncamera. (a) Scene, (b) semi-dense depth map, pseudo-colored from red\n(close) to blue (far). Image courtesy of [19].\nFig. 8. Event-based SLAM. (a) Reconstructed scene from [81], with the\nreprojected semi-dense map colored according to depth and overlaid on\nthe events (in gray), showing the good alignment between the map and\nthe events. (b) Estimated camera trajectory (several methods) and\nsemi-dense 3D map (i.e., point cloud). Image courtesy of [87].TABLE 3\nEvent-Based Methods for Pose Tracking and/or\nMapping With an Event Camera\nReference Dim Track Depth Scene Event Additional requirements\nCook [23] 2D @ \u0081 natural @ rotational motion only\nWeikersdorfer [196] 2D @ \u0081 B&W @ scene parallel to motion\nKim [24] 2D @ \u0081 natural @ rotational motion only\nGallego [111] 2D @ \u0081 natural @ rotational motion only\nReinbacher [86] 2D @ \u0081 natural @ rotational motion only\nCensi [75] 3D @ \u0081 B&W \u0081 attached depth sensor\nWeikersdorfer [197] 3D @@ natural \u0081 attached RGB-D sensor\nMueggler [198] 3D @ \u0081 B&W @ 3D map of lines\nGallego [7] 3D @ \u0081 natural \u0081 3D map of the scene\nRebecq [19] 3D \u0081 @ natural @ pose information\nKueng [110] 3D @@ natural \u0081 intensity images\nKim [25] 3D @@ natural @ image reconstruction\nRebecq [26] 3D @@ natural @ /C0\nThe type of motion is noted with labels \u201c2D\u201d (3-DOF motions, e.g., planar or\nrotational) and \u201c3D\u201d (free 6-DOF motion in 3D space). Columns indicate\nwhether the method performs tracking (\u201cTrack\u201d) and depth estimation\n(\u201cDepth\u201d) using only events (\u201cEvent\u201d), the type of scene considered (\u201cScene\u201d),\nand any additional requirements. Only [25], [26] address the most general\nscenario using only events.GALLEGO ET AL.: EVENT-BASED VISION: A SURVEY 167",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 13
        }
    },
    {
        "page_content": "tracking was performed by minimization of a photometric\nerror at the event locations given a probabilistic edge map.\nThe map was simultaneously built, and each map point rep-\nresented the probability of events being generated at that\nlocation [196]. Hence it was a panoramic occupancy map\nmeasuring the strength of the scene edges.\nRecently, solutions to the full problem of event-based 3D\nSLAM for 6-DOF motions and natural scenes, not relying on\nadditional sensing, have been proposed [25], [26] (Table 3).\nThe approach in [25] extends [24] and consists of three inter-\nleaved probabilistic \ufb01lters to perform pose tracking as well\nas depth and intensity estimation. However, it suffers from\nlimited robustness (especially during initialization) due to\nthe assumption of uncorrelated depth, intensity gradient,\nand camera motion. Furthermore, it is computationally\nintensive, requiring a GPU for real-time operation. In con-\ntrast, the semi-dense approach in [26] shows that intensity\nreconstruction is not needed for depth estimation or pose\ntracking. The approach has a geometric foundation: it per-\nforms space sweeping for 3D reconstruction [19] and edge-\nmap alignment (non-linear optimization with few events\nper frame) for pose tracking. The resulting SLAM system\nruns in real-time on a CPU.\nTrading off latency for ef\ufb01ciency, probabilistic \ufb01lters [24],\n[25], [196] can operate on small groups of events. Other\napproaches are natively designed for groups, based for\nexample on non-linear optimization [26], [111], [112], and\nrun in real time on the CPU. Processing multiple events\nsimultaneously is also bene\ufb01cial to reduce noise.\nOpportunities . The above-mentioned SLAM methods lack\nloop-closure capabilities to reduce drift. Currently, the\nscales of the scenes on which event-based SLAM has been\ndemonstrated are considerably smaller than those of frame-\nbased SLAM. However, trying to match both scales may not\nbe a sensible goal since event cameras may not be used to\ntackle the same problems as standard cameras; both sensors\nare complementary, as argued in [7], [27], [64], [75]. Stereo\nevent-based SLAM is another unexplored topic, as well as\ndesigning more accurate, ef\ufb01cient and robust methods than\nthe existing monocular ones. Robustness of SLAM systems\ncan be improved by sensor fusion with IMUs [27], [193].\n4.5 Image Reconstruction\nEvents represent brightness changes, and so, in ideal condi-\ntions (noise-free scenario, perfect sensor response, etc.) inte-\ngration of the events yields \u201cabsolute\u201d brightness. This is\nintuitive, since events are just a non-redundant (i.e.,\n\u201ccompressed\u201d) per-pixel way of encoding the visual content\nin the scene. Event integration or, more generically, image\nreconstruction (Fig. 9) can be interpreted as \u201cdecompressing\u201d\nthe visual data encoded in the event stream. Due to the very\nhigh temporal resolution of the events, brightness images can\nbe reconstructed at very high frame rate (e.g., 2 kHz to\n5 kHz [8], [199]) or even continuously in time [62].\nAs the literature reveals, the insight about image recon-\nstruction from events is that it requires regularization. Event\ncameras have independent pixels that report brightness\nchanges, and, consequently, per-pixel integration of such\nchanges during a time interval only produces brightness\nincrement images. To recover the absolute brightness at the\nend of the interval, an offset image (i.e., the brightness imageat the start of the interval) would need to be added to the\nincrement [81], [200]. Surprisingly, some works have used\nspatial and/or temporal smoothing [62], [119], [199], [201] to\nreconstruct brightness starting from a zero initial condition,\ni.e., without knowledge of the offset image. Other forms of\nregularization, using learned features from natural scenes [8],\n[102], [104], [199] are also effective.\nLiterature Review . Image reconstruction from events was\n\ufb01rst established in [23] under rotational camera motion and\nstatic scene assumptions. These assumptions together with\nthe brightness constancy (4) were used in a message-passing\nalgorithm between pixels in a network of visual maps to\njointly estimate several quantities, such as scene brightness.\nAlso under the above motion and scene assumptions, [24]\nshowed how to reconstruct high-resolution panoramas\nfrom the events, and they popularized the idea of event-\nbased HDR image reconstruction. Each pixel of the pan-\noramic image used a Kalman \ufb01lter to estimate the bright-\nness gradient (based on (4)), which was then integrated\nusing Poisson reconstruction to yield absolute brightness.\nThe method in [203] exploited the constrained motion of a\nplatform rotating around a single axis to reconstruct images\nthat were then used for stereo depth estimation.\nMotion restrictions were then replaced by regularizing\nassumptions to enable image reconstruction for generic\nmotions and scenes [101]. In this work, image brightness and\noptical \ufb02ow were simultaneously estimated using a varia-\ntional framework that contained several penalty terms (on\ndata \ufb01tting (1) and smoothness of the solution) to best\nexplain a space-time volume of events discretized as a voxel\ngrid. This method was the \ufb01rst to show reconstructed video\nfrom events in dynamic scenes. Later [119], [199], [201]\nshowed that image reconstruction was possible even without\nhaving to estimate motion. This was done using a variational\nimage denoising approach based on time surfaces [119],\n[201] or using sparse signal processing with a patch-based\nlearned dictionary that mapped events to image gradients,\nwhich were then Poisson-integrated [199]. Concurrently, the\nVO methods in [25], [26] extended the image reconstruction\ntechnique in [24] to 6-DOF camera motions by using the com-\nputed scene depth and poses: [25] used a robust variational\nregularizer to reduce noise and improve contrast of the\nreconstructed image, whereas [26] showed image recon-\nstruction as an ancillary result, since it was not needed to\nachieve VO. Recently, [62] proposed a temporal smoothing\n\ufb01lter for image reconstruction and for continuously fusing\nevents and frames. The \ufb01lter acted independently on every\npixel, thus showing that no spatial regularization on the\nFig. 9. Image reconstruction. In the scenario of a car driving out of a tun-\nnel the frames from a consumer camera (Huawei P20 Pro) (Left) suffer\nfrom under- or over-exposure, while events capture a broader dynamic\nrange of the scene, which is recovered by image reconstruction methods\n(Middle). Events also enable the reconstruction of high-speed scenes,\nsuch as a exploding mug (Right). Images courtesy of [8], [202].168 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 1, JANUARY 2022",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 14
        }
    },
    {
        "page_content": "image plane was needed to recover brightness, although it\nnaturally reduced noise and artefacts at the expense of\nsacri\ufb01cing some real detail. More recently, [8], [104] has\npresented a deep learning approach that achieves consi-\nderable gains over previous methods and mitigates visual\nartefacts. Re\ufb02ecting back on earlier works, the motion rest-\nrictions or hand-crafted regularizers that enabled image\nreconstruction have been replaced by perceptual, data-\ndriven priors from natural scenes that consequently pro-\nduced more natural-looking images. Note that image recon-\nstruction methods used in VO or SLAM [23], [24], [25]\nassume static scenes, whereas methods with weak or no\nmotion assumptions [8], [62], [101], [104], [119], [199], [201]\nare naturally used to reconstruct videos of arbitrary (e.g.,\ndynamic) scenes.\nBesides image reconstruction from events, another cate-\ngory of methods tackles the problem of fusing events and\nframes (e.g., from the DAVIS [4]), thus enhancing the bright-\nness information from the frames with high temporal resolu-\ntion and HDR properties of events [28], [62], [200]. These\nmethods also do not rely on motion knowledge and are ulti-\nmately based on (2). The method in [200] performs direct\nevent integration between frames, pixel-wise. However, the\nfused brightness becomes quickly corrupted by event noise\n(due to non-ideal effects, sensitivity mismatch, missing\nevents, etc.), and so fusion is reset with every incoming\nframe. To mitigate noise, events and frames are fused in [62]\nusing a per-pixel, temporal complementary \ufb01lter that is\nhigh-pass in the events and low-pass in the frames. It is an\nef\ufb01cient solution that takes into account the complementary\nsensing modality of events and frames: frames carry slow-\nvarying brightness information (i.e., low temporal fre-\nquency), whereas events carry \u201cchange\u201d information (i.e.,\nhigh frequency). The fusion method in [28] exploits the high\ntemporal resolution of the events to additionally remove\nmotion blur from the frames, producing high frame-rate,\nsharp video from a single blurry frame and events. It is based\non a double integral model (one integral to recover bright-\nness and another one to remove blur) within an optimization\nframework. A limitation of the above methods is that they\nstill suffer from artefacts due to event noise. These might be\nmitigated if combined with learning-based approaches [8].\nApplications . Image reconstruction implies that, in princi-\nple, it is possible to convert the events into brightness images\nand then apply mature computer vision algorithms [8],\n[104], [204]. This can have a high impact on both, event- and\nframe-based communities. The resulting images capture\nhigh-speed motions and HDR scenes, which may be bene\ufb01-\ncial in some applications, but it comes at the expense of\ncomputational cost, latency and power consumption.\nDespite image reconstruction having been useful to sup-\nport tasks such as recognition [199], SLAM [25] or optical\n\ufb02ow estimation [101], there are also works in the literature,\nsuch as [97], [103], [112], [137], showing that it is not\nneeded to ful\ufb01ll such tasks. One of the most valuable\naspects of image reconstruction is that it provides scene rep-\nresentations (e.g., appearance maps [7], [24]) that are more\ninvariant to motion than events and also facilitate establish-\ning event correspondences, which is one of the biggest chal-\nlenges of some event data processing tasks, such as feature\ntracking [64].4.6 Motion Segmentation\nSegmentation of moving objects viewed by a stationary\nevent camera is simple because events are solely imputable\nto the motion of the objects (assuming constant illumina-\ntion) [106], [108], [161]. The challenges arise in the scenario\nof a moving camera because events are triggered every-\nwhere on the image plane [13], [116], [139] (Fig. 10), pro-\nduced by moving objects and the static scene (whose\napparent motion is induced by the camera\u2019s ego-motion)\nand the goal is to infer this causal classi\ufb01cation for each\nevent. However, each event carries very little information,\nand therefore it is challenging to perform the mentioned\nper-event classi\ufb01cation.\nOvercoming these challenges has been done by tackling\nsegmentation scenarios of increasing complexity, obtained\nby reducing the amount of additional information given to\nsolve the problem. Such additional information adopts the\nform of known object shape or known motion, i.e., the algo-\nrithm knows \u201cwhat object to look for\u201d or \u201cwhat type of\nmotion it expects\u201d and objects are segmented by detecting\n(in-)consistency with respect to the expectation. The less\nadditional information is provided, the more unsupervised\nthe problem becomes (e.g., clustering). In such a case, seg-\nmentation is enabled by the key insight that moving objects\nproduce distinctive traces of events on the image plane and\nit is possible to infer the trajectories of the objects that gener-\nate those traces, yielding the segmented objects [139]. Like\nclustering, this is a joint optimization problem in the motion\nparameters of the objects (i.e., the \u201cclusters\u201d) and the event-\nobject associations (i.e., the segmentation).\nLiterature Review . Considering known object shape, [13]\npresents a method to detect and track a circle in the pres-\nence of event clutter caused by the moving camera. It is\nbased on the Hough transform using optical \ufb02ow informa-\ntion extracted from temporal windows of events. The\nmethod was extended in [162] using a particle \ufb01lter to\nimprove tracking robustness: the duration of the observa-\ntion window was dynamically selected to accommodate for\nsudden motion changes due to accelerations of the object.\nMore generic object shapes were detected and tracked\nby [169] using event corners (Section 4.1) as geometric prim-\nitives. In this method, additional knowledge of the robot\njoints controlling the camera motion was required.\nSegmentation has been addressed by [116], [138], [139]\nunder mild assumptions leveraging the idea of motion-com-\npensated event images [111] (Section 3). Essentially this\ntechnique associates events that produce sharp edges when\nwarped according to a motion hypothesis. The simplest\nFig. 10. The iCub humanoid robot from IIT has two event cameras in the\neyes. Here, it segments and tracks a ball under event clutter produced\nby the motion of the head. Right: space-time visualization of the events\non the image frame, colored according to polarity (positive in green,\nnegative in red). Image courtesy of [162].GALLEGO ET AL.: EVENT-BASED VISION: A SURVEY 169",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 15
        }
    },
    {
        "page_content": "hypothesis is a linear motion model (i.e., constant optical\n\ufb02ow), yet it is suf\ufb01ciently expressive: for short times, scenes\nmay be described as collections of objects producing events\nthat \ufb01t different linear motion models. Such a scene descrip-\ntion is what the cited segmentation algorithms seek for. Spe-\nci\ufb01cally, the method in [138] \ufb01rst \ufb01ts a linear motion-\ncompensation model to the dominant events, then removes\nthese and \ufb01ts another linear model to the remaining events,\ngreedily. Thus, it clusters events according to optical \ufb02ow,\nyielding motion-compensated images with sharp object con-\ntours. Similarly, [116] detects moving objects in clutter by \ufb01t-\nting a motion-compensation model to the dominant events\n(i.e., the background) and detecting inconsistencies with\nrespect to it (i.e., the objects). They test the method in chal-\nlenging scenarios inaccessible to standard cameras (HDR,\nhigh-speed) and release a dataset. The work in [139] pro-\nposes an iterative clustering algorithm that jointly estimates\nthe event-object associations (i.e., segmentation) and the\nmotion parameters of the objects (i.e., clusters) that produce\nsharpest motion-compensated event images. It allows for\ngeneral parametric motion models [112] to describe each\nobject and produces better results than greedy methods [116],\n[138]. In [117] a learning-based approach for segmentation\nusing motion-compensation is proposed: ANNs are used to\nestimate depth, ego-motion, segmentation masks of inde-\npendently moving objects and object 3D velocities. An event-\nbased dataset is provided for supervised learning, which\nincludes accurate pixel-wise motion masks of 3D-scanned\nobjects that are reliable even in poor lighting conditions and\nduring fast motion.\nSegmentation is a paramount topic in frame-based vision,\nyet it is rather unexplored in event-based vision. As more\ncomplex scenes are addressed and more advanced event-\nbased vision techniques are developed, more works target-\ning this challenging problem are expected to appear.\n4.7 Recognition\nAlgorithms . Recognition algorithms for event cameras have\ngrown in complexity, from template matching of simple\nshapes to classifying arbitrary edge patterns using either tra-\nditional machine learning on hand-crafted features or mod-\nern deep learning methods. This evolution aims at endowing\nrecognition systems with more expressibility (i.e., approxi-\nmation capacity) and robustness to data distortions.\nEarly research with event-based sensors began with\ntracking a moving object using a static sensor. An event-\ndriven update of the position of a model of the object shape\nwas used to detect and track objects with a known simple\nshape, such as a blob [6], circle [53], [205] or line [54]. Simple\nshapes can also be detected by matching against a prede-\n\ufb01ned template, which removes the need to describe the\ngeometry of the object. This template matching approach was\nimplemented using convolutions in early hardware [53].\nFor more complex objects, templates can be used to\nmatch low level features instead of the entire object, after\nwhich a classi\ufb01er can be used to make a decision based on\nthe distribution of features observed [93]. Nearest Neighbor\nclassi\ufb01ers are typically used, with distances calculated in\nfeature space. Accuracy can be improved by increasing fea-\nture invariance, which can be achieved using a hierarchicalmodel where feature complexity increases in each layer.\nWith a good choice of features, only the \ufb01nal classi\ufb01er needs\nto be retrained when switching tasks. This leads to the prob-\nlem of selecting which features to use. Hand-crafted orienta-\ntion features were used in early works, but far better results\nare obtained by learning the features from the data itself. In\nthe simplest case, each template can be obtained from an\nindividual sample, but such templates are sensitive to noise\nin the sample data [15]. One may follow a generative\napproach, learning features that enable to accurately recon-\nstruct the input, as was done in [122] with a Deep Belief Net-\nwork (DBN). More recent work obtains features by\nunsupervised learning, clustering the event data and using\nthe center of each cluster as a feature [93]. During inference,\neach event is associated to its closest feature, and a classi\ufb01er\noperates on the distributions of features observed. With the\nrise of deep learning in frame-based computer vision, many\nhave sought to leverage deep learning tools for event-based\nrecognition, using back-propagation to learn features. This\napproach has the advantage of not requiring a separate clas-\nsi\ufb01er at the output, but the disadvantage of requiring far\nmore labeled data for training. Image recognition with\nevents also suffers from the practical problem of the avail-\nability of training data in the event domain [206]. In [207]\nthe authors use wormhole learning , a semi-supervised\napproach in which, starting from a detector in the RGB\ndomain, one is able to train a detector in the event domain;\nmoreover, in a second phase the teacher becomes the stu-\ndent, and some of the illumination invariance of the event\nsensor is transferred to the RGB-only detector.\nMost learning-based approaches convert events/spikes\ninto (dense) tensors, a convenient representation for image-\nbased hierarchical models, e.g., ANNs (Fig. 11). There are\ndifferent ways the value of each tensor element can be com-\nputed (Section 3.1). Simple methods use the time surfaces,\nor event histogram frames. A more robust method uses\ntime surfaces with exponential decay [93] or with average\ntimestamps [97]. Image reconstruction methods (Section 4.5)\nmay also be used. Some recognition approaches rely on con-\nverting spikes to frames during inference [134], [199], while\nothers convert the trained ANN to an SNN which can oper-\nate directly on the event data [121]. Similar ideas can be\napplied for tasks other than recognition [22], [91]. As neuro-\nmorphic hardware advances (Section 5.1), there is increas-\ning interest in learning directly in SNNs [127] or even\ndirectly in the neuromorphic hardware itself [128].\nFig. 11. Recognition of moving objects. (a) A DAVIS240C sensor with\nFPGA attached tracks and sends regions of events to IBM\u2019s TrueNorth\nNS1e evaluation platform for classi\ufb01cation. Results on a street scene\nshow red boxes around tracked and classi\ufb01ed cars. (b) In [121] very high\nspeed object recognition (browsing a full deck of 52 cards in just 0.65s)\nwas illustrated with event-driven convolutional neural networks.170 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 1, JANUARY 2022",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 16
        }
    },
    {
        "page_content": "Tasks . Early tasks focused on detecting the presence of a\nsimple shape (such as a circle) from a static sensor [6], [53],\n[205], but soon progressed to the classi\ufb01cation of more com-\nplex shapes, such as card pips [121] (Fig. 11b), block let-\nters [15] and faces [93], [199]. A popular task throughout\nhas been the classi\ufb01cation of hand-written digits. Inspired\nby the role it has played in frame-based computer vision, a\nfew event-based MNIST datasets have been generated from\nthe original MNIST dataset [58], [208]. These datasets\nremain a good test for algorithm development, with many\nalgorithms now achieving over 98 percent accuracy on the\ntask [97], [126], [127], [209], [210], [211], but few would pro-\npose digit recognition as a strength of event-based vision.\nMore dif\ufb01cult tasks involve either more dif\ufb01cult objects,\nsuch as the Caltech-101 and Caltech-256 datasets (both of\nwhich are still considered easy by computer vision) or more\ndif\ufb01cult scenarios, such as recognition from on-board a\nmoving vehicle [97]. Very few works tackle these tasks so\nfar, and those that do typically fall back on generating event\nframes and processing them using a traditional deep learn-\ning framework.\nA key challenge for recognition is that event cameras\nrespond to relative motion in the scene (Section 2.3), and\nthus require either the object or the camera to be moving. It is\ntherefore unlikely that event cameras will be a strong choice\nfor recognizing static or slow moving objects, although little\nhas been done to combine the advantages of frame- and\nevent-based cameras for these applications. The event-based\nappearance of an object is highly dependent on the above-\nmentioned relative motion (Fig. 5), thus tight control of the\ncamera motion could be used to aid recognition [208].\nSince the camera responds to dynamic signals, obvious\napplications include recognizing objects by the way they\nmove [212], or recognizing dynamic scenes such as gestures\nor actions [16], [17]. These tasks are typically more challeng-\ning than static object recognition because they include a time\ndimension, but this is exactly where event cameras excel.\nOpportunities . Event cameras exhibit many alluring prop-\nerties, but event-based recognition has a long way to go if it\nis to compete with modern frame-based approaches. While it\nis important to compare event- and frame-based methods,\none must remember that each sensor has its own strengths.\nThe ideal acquisition scenario for a frame-based sensor con-\nsists of both the sensor and object being static, which is the\nworst possible scenario for event cameras. For event-based\nrecognition to \ufb01nd widespread adoption, it will need to \ufb01nd\napplications which play to its strengths. Such applications\nare unlikely to be similar to well established computer vision\nrecognition tasks which play to the frame-based sensor\u2019s\nstrengths. Instead, such applications are likely to involve\nresource constrained recognition of dynamic sequences, or\nrecognition from on-board a moving platform. Finding and\ndemonstrating the use of event-based sensors in such appli-\ncations remains an open challenge.\nAlthough event-based datasets have improved in quality\nin recent years, there is still room for improvement. Data\ncollection and annotation is a tiresome and thankless task,\nbut developing an easy-to-use pipeline for collecting and\nannotating event-based data would be a signi\ufb01cant contri-\nbution to the \ufb01eld, especially if the tools can mature to the\nstage where the task can be outsourced to laymen.4.8 Neuromorphic Control\nIn living creatures, most information processing happens\nthrough spike-based representation: spikes encode the sen-\nsory data; spikes perform the computation; and spikes\ntransmit actuator \u201ccommands\u201d. Therefore, biology shows\nthat the event-based paradigm is, in principle, applicable\nnot just to perception and inference, but also to control.\nNeuromorphic-Vision-Driven Control Architecture . In this\ntype of architecture (Fig. 12), there is a neuromorphic sen-\nsor, an event-based estimator, and a traditional controller.\nThe estimator computes a state, and the controller computes\nthe control based on the provided state. The controller is not\naware of the asynchronicity of the architecture.\nNeuromorphic-vision-driven control architectures have\nbeen demonstrated since the early days of neuromorphic\ncameras, and they have proved the two advantages of low\nlatency and computational ef\ufb01ciency. The earliest demon-\nstrators were the spike-based convolutional target tracking\ndemo in the CAVIAR project [53] and the \u201crobot goalie\u201d\ndescribed in [6], [12]. Another early example was the pencil-\nbalancing robot [54]. In that demonstrator two DVS\u2019s\nobserved a pencil as inverted pendulum placed on a small\nmovable cart. The pencil\u2019s state in 3D was estimated in\nbelow 1ms latency. A simple hand tuned PID controller\nkept the pencil balanced upright. It was also demonstrated\non an embedded system, thereby establishing the ability to\nrun on severely constrained computing resources.\nEvent-Based Control Theory . Event-based techniques can\nbe motivated from the perspective of control and decision\ntheory. Using a biological metaphor, event-based control\ncan be understood as a form of what economics calls rational\ninattention [213]: more information allows for better deci-\nsions, but if there are costs associated to obtaining or proc-\nessing the information, it is rational to take decisions with\nonly partial information available.\nIn event-based control, the control signal is changed asyn-\nchronously [214]. There are several variations of the concept\ndepending on how the \u201ccontrol events\u201d are generated. One\nimportant distinction is between event-triggered control and\nself-triggered control [215]. In event-based control the events areFig. 12. Control architectures based on neuromorphic events. In a neuro-\nmorphic-vision-driven control architecture (a), a neuromorphic sensor\nproduces events, an event-based perception system produces state esti-\nmates, and a traditional controller is called asynchronously to compute\nthe control signal. In a native neuromorphic-based architecture (b), the\nevents generate directly changes in control. Finally, (c) shows an archi-\ntecture in which the task informs the events that are generated.GALLEGO ET AL.: EVENT-BASED VISION: A SURVEY 171",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 17
        }
    },
    {
        "page_content": "generated \u201cexogenously\u201d based on certain condition; for\nexample, a \u201crecompute control\u201d request might be triggered\nwhen the trajectory\u2019s tracking error exceeds a threshold. In\nself-triggered control , the controller decides by itself when is\nthe next time it should be called based on the situation. For\nexample, a controller might decide to \u201csleep\u201d for longer if\nthe state is near the target, or to recompute the control signal\nsooner if it is required.\nThe advantages of event-based control are usually justi\ufb01ed\nconsidering a trade-off between computation / communica-\ntion cost and control performance. The basic consideration is\nthat, while the best control performance is obtained by recom-\nputing the control in\ufb01nitely often (for an in\ufb01nite cost), there\nare strongly diminishing returns. A solid principle of control\ntheory is that the control frequency depends on the time con-\nstant of the plant and the sensor: it does not make sense to\nchange the control much quicker than the new incoming\ninformation or the speed of the actuators. This motivates\nchoosing control frequencies that are comparable with the\nplant dynamics and adapt to the situation. For example, one\ncan show that an event-triggered controller achieves the same\nperformance with a fraction of the computation; or, con-\nversely, a better performance with the same amount of com-\nputation. In some cases (scalar linear Gaussian) these trade-\noffs can be obtained in closed form [216], [217]. (Analogously,\ncertain trade-offs can be obtained in closed form for\nperception [218].)\nUnfortunately, the large literature in event-based control\nis of restricted utility for the embodied neuromorphic set-\nting. Beyond the super\ufb01cial similarity of dealing with\n\u201cevents\u201d the settings are quite different. For example, in net-\nwork-based control, one deals with typically low-dimen-\nsional states and occasional events\u2014the focus is on making\nthe most of each single event. By contrast, for an autono-\nmous vehicle equipped with event cameras, the problem is\ntypically how to \ufb01nd useful signals in potentially millions\nof events per second. Particularizing the event-based con-\ntrol theory to the neuromorphic case is a relatively young\navenue of research [219], [220], [221], [222]. The challenges\nlie in handling the non-linearities typical of the vision\nmodality, which prevents clean closed-form results.\nOpen Questions in Neuromorphic Control . Finally, we\ndescribe some of open problems in this topic.\nTask-Driven Sensing . In animals, perception has value\nbecause it is followed by action, and the information col-\nlected is actionable information that helps with the task. A sig-\nni\ufb01cant advance would be the ability for a controller to\nmodulate the sensing process based on the task and the con-\ntext. In current hardware there is limited software-modu-\nlated control for the sensing processing, though it is\npossible to modulate some of the hardware biases. Integra-\ntion with region-of-interest mechanisms, heterogeneous\ncamera bias settings, etc. would provide additional \ufb02exibil-\nity and more computationally ef\ufb01cient control.\nThinking Fast and Slow . Existing research has focused on\nobtaining low-latency control, but there has been little work\non how to integrate this sensorimotor level into the rest of an\nagent\u2019s cognitive architecture. Using again a bio-inspired met-\naphor, and following Kahneman [223], the fast/instinctive/\n\u201cemotional\u201d system must be integrated with the slower/\ndeliberative system.5E VENT-BASED SYSTEMS AND APPLICATIONS\n5.1 Neuromorphic Computing\nNeuromorphic engineering tries to capture some of the\nunparalleled computational power and ef\ufb01ciency of the\nbrain by mimicking its structure and function. Typically\nthis results in a massively parallel hardware accelerator for\nSNNs (Section 3.3), which is how we will de\ufb01ne a neuro-\nmorphic processor. Since the neuron spikes within such a\nprocessor are inherently asynchronous, a neuromorphic\nprocessor is the best computational partner for an event\ncamera. Neuromorphic processors act on the events injected\nby the event camera directly, without conversion, and offer\nbetter data-processing locality (spatially and temporally)\nthan standard architectures such as CPUs, yielding low\npower and low latency computer vision systems.\nNeuromorphic processors may be categorized by their\nneuron model implementat ions (Table 4), which are\nbroadly divided between analog neurons (Neurogrid,\nBrainScaleS, ROLLS, DYNAP-se), digital neurons (True-\nNorth, Loihi, ODIN) and software neurons (SpiNNaker).\nSome architectures also support on-chip learning (Loihi,\nODIN, DYNAP-le). When evaluating a neuromorphic pro-\ncessor for an event-based vision system, the following cri-\nteria should be considered in addition to the processor\u2019s\nfunctionality and performance: ( i) the software develop-\nment ecosystem: a minimal toolchain includes an API to\ncompose and train a network, a compiler to prepare the\nnetwork for the hardware, and a runtime library to deploy\nthe network in hardware, ( ii) event-based vision systems\ntypically require that a processor be available as a stand-\nalone system suitable for mobi le applications, and not just\nhosted in a remote server, ( iii) the availability of neuro-\nmorphic processors.\nSeveral developments are necessary to enable a more\nwidespread use of these processors, such as: ( i) developing\na more user-friendly ecosystem (an easier way to program\nthe desired method for deployment in hardware), ( ii)\nenabling more processing capabilities of the hardware plat-\nform, ( iii) increasing the availability of devices beyond early\naccess programs targeted at selected partners.\nThe following processors (Table 4) have the most mature\ndeveloper work\ufb02ows, combined with the widest availability\nof standalone systems. More details are given in [229], [230].TABLE 4\nComparison Between Selected Neuromorphic Processors,\nOrdered by Neuron Model Type\nProcessor SpiNNaker TrueNorth Loihi DYNAP Braindrop\nReference [224] [225] [226] [227] [228]\nManufacturer U. Manchester IBM Intel aiCTX Stanford U.\nYear 2011 2014 2018 2017 2018\nNeuron model Software Digital Digital Analog Analog\nOn-chip learning Yes No Yes No No\nCMOS technol. 130nm 28nm 14nm 180nm 28nm\nNeurons/chip 4 k* 1024 k 131 k 1 k 4 k\nNeurons/core 255* 256 1024 256 4096\nCores/chip 16* 4096 128 4 1\nSynapses/chip 16 M 268 M 130 M 128 k 16 M\nBoards 4- or 48-chip 1- or 16-chip 4- or 8-chip, 1-chip 1-chip\nSoftware stack sPyNNaker CPE/Eedn Nengo cAER Nengo\nPACMAN NSCP Nx SDK libcAER172 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 1, JANUARY 2022",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 18
        }
    },
    {
        "page_content": "SpiNNaker (Spiking Neural Network Architecture) uses gen-\neral-purpose ARM cores to simulate biologically realistic\nmodels of the human brain [231]. SpiNNaker implements\nneurons as software running on the cores, sacri\ufb01cing hard-\nware acceleration to maximize model \ufb02exibility. The SpiN-\nNaker has been coupled with event cameras for stereo\ndepth estimation [149], [232], optic \ufb02ow computation [232],\n[233], and for object tracking [234] and recognition [235].\nTrueNorth uses digital neurons to perform real-time infer-\nence. Each chip simulates 1 M (million) neurons and 256 M\nsynapses, distributed among 4,096 neurosynaptic cores.\nThere is no on-chip learning, so networks are trained of\ufb02ine\nusing a GPU or other processor [236].\nTrueNorth has been paired with event cameras to pro-\nduce end-to-end, low power and low-latency event-based\nvision systems for gesture recognition [17], stereo recon-\nstruction [190] and optical \ufb02ow estimation [174].\nLoihi uses digital neurons to perform real-time inference\nand online learning. Each chip simulates up to 131 thousand\nspiking neurons and 130 M synapses. A learning engine in\neach neuromorphic core updates each synapse using rules\nthat includes STDP and reinforcement learning [226]. Non-\nspiking networks can be trained in TensorFlow and approx-\nimated by spiking networks for Loihi using the Nengo Deep\nLearning toolkit from Applied Brain Research [237].\nDYNAP . The Dynamic Neuromorphic Asynchronous\nProcessor has two variants, one optimized for scalable infer-\nence (Dynap-se), and another for online learning (Dynap-le).\nBraindrop prototypes a single core of the 1 M-neuron Brain-\nstorm system [228]. It is programmed using Nengo [238] and\nimplements the Neural Engineering Framework [239].\n5.2 Applications in Real-Time On-Board Robotics\nAs event-based vision sensors often produce signi\ufb01cantly\nless data per time interval compared to traditional cameras,\nmultiple applications can be envisioned where extracting\nrelevant vision information can happen in real-time within\na simple computing system directly connected to the sensor,\navoiding USB connection. Fig. 13 shows an example of\nsuch, where a dual-core ARM micro controller running at\n200 MHz with 136 kB on-board SRAM fetches and processes\nevents in real-time. The combined embedded system of sen-\nsor and micro controller here operate a simple wheeled robot\nin tasks such as line following, active and passive object\ntracking, distance estimation, and simple mapping [240].A different example of near-sensor processing (\u201cedge\ncomputing\u201d) is the Speck SoC,9which combines a DVS and\nthe Dynap-se neuromorphic CNN processor. Its peak power\nconsumption is less than 1mW and latency is less than\n30ms. Application domains are low-power, continuous\nobject detection, surveillance, and automotive systems.\nEvent cameras have also been used on-board quadrotors\nwith limited computational resources, both for autonomous\nlanding [241] or \ufb02ight [27] (Fig. 13b), in challenging scenes.\n6D ISCUSSION\nEvent-based vision is a topic that spans many \ufb01elds, such as\ncomputer vision, robotics and neuromorphic engineering.\nEach community focuses on exploiting different advantages\nof the event-based paradigm. Some focus on the low power\nconsumption for \u201calways on\u201d or embedded applications on\nresource-constrained platforms; others favor low latency to\nenable highly reactive systems, and others prefer the avail-\nability of information to better perceive the environment\n(high temporal resolution and HDR), with fewer constraints\non computational resources.\nEvent-based vision is an emerging technology in the era\nof mature frame-based camera hardware and software.\nComparisons are, in some terms, unfair since they are not\ncarried out under the same maturity level. Nevertheless\nevent cameras show potential, able to overcome some of the\nlimitations of frame-based cameras, reaching new scenarios\npreviously inaccessible. There is considerable room for\nimprovement (research and development), as pointed out\nin numerous opportunities throughout the paper.\nThere is no agreement on what the best method (and\nrepresentation) to process events is, notably because it\ndepends on the application. There are different trade-offs\ninvolved, such as latency versus power consumption and\naccuracy, or sensitivity versus bandwidth and processing\ncapacity. For example, reducing the contrast threshold and/\nor increasing the resolution produces more events, which\nwill be processed by an algorithm and platform with \ufb01nite\ncapacity. A challenging research area is to quantify such\ntrade-offs and to develop techniques to dynamically adjust\nthe sensor and/or algorithm parameters for optimal\nperformance.\nAnother big challenge is to develop bio-inspired systems\nthat are natively event-based end-to-end (from perception\nto control and actuation) that are also more ef\ufb01cient and\nlong-term solutions than synchronous, frame-based sys-\ntems. Event cameras pose the challenge of rethinking per-\nception, control and actuation, and, in particular, the\ncurrent main stream of deep learning methods in computer\nvision: adapting them or transferring ideas to process\nevents while being as top-performing. Active vision (pair-\ning perception and control) is specially relevant on event\ncameras because the events distinctly depends on motion,\nwhich may be due to the actuation of a robot.\nEvent cameras can be seen as an entry point for more ef\ufb01-\ncient, near-sensor processing, such that only high-level,\nnon-redundant information is transmitted, thus reducing\nbandwidth, latency and power consumption. This could be\nFig. 13. (a) Embedded DVS128 on Pushbot as standalone closed-loop\nperception-computation-action system, used in navigation and obstacle-\navoidance tasks [240]. (b) Drone with a down-looking DAVIS, used for\nautonomous \ufb02ight [27]. The high speed and dynamic range of events are\nleveraged to operate in dif\ufb01cult illumination conditions. The same visual-\ninertial odometry algorithm [27] is also demonstrated on high-speed sce-\nnarios, such as an event camera spinning tied to a rope.\n9.https://www.speck.ai/GALLEGO ET AL.: EVENT-BASED VISION: A SURVEY 173",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 19
        }
    },
    {
        "page_content": "done by pairing an event camera with hardware on the\nsame sensor device (Speck in Section 5.2), or by alternative\nbio-inspired imaging sensors, such as cellular processor\narrays [242] which every pixel has a processor that allows to\nperform several types of computations with the brightness\nof the pixel and its neighbors.\n7C ONCLUSION\nEvent cameras are revolutionary sensors that offer many\nadvantages over traditional, frame-based cameras, such as\nlow latency, low power, high speed and high dynamic range.\nHence, they have a large potential for computer vision and\nrobotic applications in challenging scenarios currently inac-\ncessible to traditional cameras. We have provided an over-\nview of the \ufb01eld of event-based vision, covering perception,\ncomputing and control, with a focus on the working princi-\nple of event cameras and the algorithms developed to unlock\ntheir outstanding properties in selected applications, from\nlow-level vision to high-level vision. Neuromorphic percep-\ntion and control are emerging topics; and so, there are plenty\nof opportunities, as we have pointed out throughout the text.\nMany challenges remain ahead, and we hope that this paper\nprovides an introductory exposition of the topic, as a step in\nhumanity\u2019s longstanding quest to build intelligent machines\nendowed with a more ef\ufb01cient, bio-inspired way of perceiv-\ning and interacting with the world.\nACKNOWLEDGMENTS\nThe work of G. Gallego and D. Scaramuzza was supported\nby the SNSF-ERC Starting Grant and the Swiss National Sci-\nence Foundation through the National Center of Compe-\ntence in Research (NCCR) Robotics. The authors would like\nto thank all the people who contributed to this paper. The\nauthors would like to thank the event camera manufac-\nturers for providing the values in Table 1 and for discussing\nthe dif\ufb01culties in their comparison due to the lack of a com-\nmon testbed. In particular, we thank Hyunsurk Eric Ryu\n(Samsung Electronics), Chenghan Li (iniVation), Davide\nMigliore (Prophesee), Marc Osswald (Insightness) and Prof.\nChen (CelePixel). We are also thankful to all members of\nour research laboratories, for discussion and comments on\nearly versions of this document. We thank the Editors and\nanonymous reviewers of IEEE TPAMI for their suggestions,\nwhich led us to improve the paper.\nREFERENCES\n[1] M. Mahowald and C. Mead, \u201cThe silicon retina,\u201d Sci. Amer. ,\nvol. 264, no. 5, pp. 76\u201383, May 1991.\n[2] P. Lichtsteiner, C. Posch, and T. Delbruck, \u201cA 128 /C2128 120 dB 15\nms latency asynchronous temporal contrast vision sensor,\u201d IEEE\nJ. Solid-State Circuits , vol. 43, no. 2, pp. 566\u2013576, Feb. 2008.\n[3] C. Posch, D. Matolin, and R. Wohlgenannt, \u201cA QVGA 143 dB\ndynamic range frame-free PWM image sensor with lossless\npixel-level video compression and time-domain CDS,\u201d IEEE J.\nSolid-State Circuits , vol. 46, no. 1, pp. 259\u2013275, Jan. 2011.\n[4] C. Brandli, R. Berner, M. Yang, S. -C. Liu, and T. Delbruck, \u201cA 240 x 180\n130 dB 3 ms latency global shutter spatiotemporal vision sensor,\u201d\nIEEE J. Solid-State Circuits , vol. 49, no. 10, pp. 2333\u20132341, Oct. 2014.\n[5] B. Son et al., \u201cA 640 x 480 dynamic vision sensor with a 9 mm pixel\nand 300Meps address-event representation,\u201d in Proc. IEEE Int.\nSolid-State Circuits Conf. , 2017, pp. 66\u201367.\n[6] T. Delbruck and P. Lichtsteiner, \u201cFast sensory motor control\nbased on event-based hybrid neuromorphic-procedural system,\u201d\ninProc. IEEE Int. Symp. Circuits Syst. , 2007, pp. 845\u2013848.[7] G. Gallego, J. E. A. Lund, E. Mueggler, H. Rebecq, T. Delbruck,\nand D. Scaramuzza, \u201cEvent-based, 6-DOF camera tracking from\nphotometric depth maps,\u201d IEEE Trans. Pattern Anal. Mach. Intell. ,\nvol. 40, no. 10, pp. 2402\u20132412, Oct. 2018.\n[8] H. Rebecq, R. Ranftl, V. Koltun, and D. Scaramuzza, \u201cHigh speed\nand high dynamic range video with an event camera,\u201d IEEE\nTrans. Pattern Anal. Mach. Intell. , early access, Dec. 31, 2019,\ndoi:10.1109/TPAMI.2019.2963386.\n[9] Accessed: Jun. 2020. [Online]. Available: https://github.com/uzh-\nrpg/event-based_vision_resources\n[10] T. Delbruck, \u201cNeuromorphic vision sensing and processing,\u201d in\nProc. Eur. Solid-State Device Res. Conf. , 2016, pp. 7\u201314.\n[11] S.-C. Liu, B. Rueckauer, E. Ceolini, A. Huber, and T. Delbruck,\n\u201cEvent-driven sensing for ef\ufb01cien t perception: Vision and audition\nalgorithms,\u201d IEEE Signal Process. Mag. , vol. 36, no. 6, pp. 29\u201337,\nNov 2019.\n[12] T. Delbruck and M. Lang, \u201cRobotic goalie with 3ms reaction time\nat 4% CPU load using event-based dynamic vision sensor,\u201d\nFront. Neurosci. , vol. 7, 2013, Art. no. 223.\n[13] A. Glover and C. Bartolozzi, \u201cEvent-driven ball detection and\ngaze \ufb01xation in clutter,\u201d in Proc. IEEE Int. Conf. Intell. Robots\nSyst., 2016, pp. 2203\u20132208.\n[14] M. Litzenberger et al. , \u201cEstimation of vehicle speed based on\nasynchronous data from a silicon retina optical sensor,\u201d in Proc.\nIEEE Intell. Transp. Syst. Conf. , 2006, pp. 653\u2013658.\n[15] G. Orchard, C. Meyer, R. Etienne-Cummings, C. Posch, N. Thakor,\nand R. Benosman, \u201cHFirst: A temporal approach to object rec-\nognition,\u201d IEEE Trans. Pattern Anal. Mach. Intell. ,v o l .3 7 ,n o .1 0 ,\npp. 2028\u20132040, Oct. 2015.\n[16] J. H. Lee et al. , \u201cReal-time gesture interface based on event-\ndriven processing from stereo silicon retinas,\u201d IEEE Trans.\nNeural Netw. Learn. Syst. , vol. 25, no. 12, pp. 2250\u20132263, Dec.\n2014.\n[17] A. Amir et al., \u201cA low power, fully event-based gesture recogni-\ntion system,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. ,\n2017, pp. 7388\u20137397.\n[18] P. Rogister, R. Benosman, S.-H. Ieng, P. Lichtsteiner, and\nT. Delbruck, \u201cAsynchronous event-based binocular stereo\nmatching,\u201d IEEE Trans. Neural Netw. Learn. Syst. ,v o l .2 3 ,\nno. 2, pp. 347\u2013353, Feb. 2012.\n[19] H. Rebecq, G. Gallego, E. Mueggler, and D. Scaramuzza, \u201cEMVS:\nEvent-based multi-view stereo\u20143D reconstruction with an event\ncamera in real-time,\u201d Int. J. Comput. Vis. ,v o l .1 2 6 ,n o .1 2 ,\npp. 1394\u20131414, 2018.\n[20] N. Matsuda, O. Cossairt, and M. Gupta, \u201cMC3D: Motion contrast\n3D scanning,\u201d in Proc. IEEE Int. Conf. Comput. Photography , 2015,\npp. 1\u201310.\n[21] R. Benosman, C. Clercq, X. Lagorce, S.-H. Ieng, and C. Bartolozzi,\n\u201cEvent-based visual \ufb02ow,\u201d IEEE Trans. Neural Netw. Learn. Syst. ,\nvol. 25, no. 2, pp. 407\u2013417, Feb. 2014.\n[22] A. Z. Zhu, L. Yuan, K. Chaney, and K. Daniilidis, \u201cEV-FlowNet:\nSelf-supervised optical \ufb02ow estimation for event-based cam-\neras,\u201d in Proc. Robot.: Sci. Syst. , 2018, pp. 1\u20139, doi: 10.15607/\nRSS.2018.XIV.062.\n[23] M. Cook, L. Gugelmann, F. Jug, C. Krautz, and A. Steger,\n\u201cInteracting maps for fast visual interpretation,\u201d in Proc. Int. Joint\nConf. Neural Netw. , 2011, pp. 770\u2013776.\n[24] H. Kim, A. Handa, R. Benosman, S.-H. Ieng, and A. J. Davison,\n\u201cSimultaneous mosaicing and tracking with an event camera,\u201d\ninProc. Brit. Mach. Vis. Conf. , 2014, pp. 1\u201312.\n[25] H. Kim, S. Leutenegger, and A. J. Davison, \u201cReal-time 3D recon-\nstruction and 6-DoF tracking with an event camera,\u201d in Proc. Eur.\nConf. Comput. Vis. , 2016, pp. 349\u2013364.\n[26] H. Rebecq, T. Horstsch \u20acafer, G. Gallego, and D. Scaramuzza,\n\u201cEVO: A geometric approach to event-based 6-DOF parallel\ntracking and mapping in real-time,\u201d IEEE Robot. Autom. Lett. ,\nvol. 2, no. 2, pp. 593\u2013600, Apr. 2017.\n[27] A. Rosinol Vidal, H. Rebecq, T. Horstschaefer, and D. Scaramuzza,\n\u201cUltimate SLAM? Combining events, images, and IMU for robust\nvisual SLAM in HDR and high speed scenarios,\u201d IEEE Robot.\nAutom. Lett. , vol. 3, no. 2, pp. 994\u20131001, Apr. 2018.\n[28] L. Pan, C. Scheerlinck, X. Yu, R. Hartley, M. Liu, and Y. Dai,\n\u201cBringing a blurry frame alive at high frame-rate with an event\ncamera,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2019,\npp. 6813\u20136822.\n[29] G. Cohen et al. , \u201cEvent-based sensing for space situational\nawareness,\u201d in Proc. Adv. Maui Opt. Space Surveillance Technol.\nConf. , 2017, pp. 1\u201313.174 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 1, JANUARY 2022",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 20
        }
    },
    {
        "page_content": "[30] T.-J. Chin, S. Bagchi, A. Eriksson, and A. van Schaik, \u201cStar track-\ning using an event camera,\u201d in Proc. IEEE Conf. Comput. Vis. Pat-\ntern Recognit. Workshops , 2019, pp. 1646\u20131655.\n[31] P. Lichtsteiner and T. Delbruck, \u201c64 x 64 event-driven logarith-\nmic temporal derivative silicon retina,\u201d in Proc. IEEE Workshop\nCharge-Coupled Devices Adv. Image Sensors , 2005, pp. 157\u2013160.\n[32] P. Lichtsteiner and T. Delbruck, \u201cA 64x64 AER logarithmic tem-\nporal derivative silicon retina,\u201d in Proc. Res. Microelectron. Elec-\ntron., 2005, pp. 202\u2013205.\n[33] P. Lichtsteiner, \u201cAn AER temporal contrast vision sensor,\u201d PhD\nThesis, Dept. Phys., ETH Zurich, Zurich, Switzerland, 2006.\n[34] P. Lichtsteiner, C. Posch, and T. Delbruck, \u201cA 128 x 128 120 dB\n30 mW asynchronous vision sensor that responds to relative\nintensity change,\u201d in Proc. IEEE Int. Solid-State Circuits Conf. ,\n2006, pp. 2060\u20132069.\n[35] D. Neil, \u201cDeep neural networks and hardware systems for event-\ndriven data,\u201d PhD dissertation, Dept. Inf. Tech. Elec, Eng. ETH-\nZurich, Zurich, Switzerland, 2017.\n[36] C. Posch, T. Serrano-Gotarredona, B. Linares-Barranco, and\nT. Delbruck, \u201cRetinomorphic event-based vision sensors: Bioins-\npired cameras with spiking output,\u201d Proc. IEEE , vol. 102, no. 10,\npp. 1470\u20131484, Oct. 2014.\n[37] K. A. Boahen, \u201cA burst-mode word-serial address-event link-I:\nTransmitter design,\u201d IEEE Trans. Circuits Syst. I, Reg. Papers ,\nvol. 51, no. 7, pp. 1269\u20131280, Jul. 2004.\n[38] S.-C. Liu, T. Delbruck, G. Indiveri, A. Whatley, and R. Douglas,\nEvent-Based Neuromorphic Systems . Hoboken, NJ, USA: Wiley,\n2015.\n[39] Y. Suh et al., \u201cA 1280x960 dynamic vision sensor with a 4.95- mm\npixel pitch and motion artifact minimization,\u201d in Proc. IEEE Int.\nSymp. Circuits Syst. , 2020, pp. 1\u20135.\n[40] T. Delbruck, Y. Hu, and Z. He, \u201cV2E: From video frames to real-\nistic DVS event camera streams,\u201d 2020, arXiv:2006.07722v1 .\n[41] M. Mahowald, \u201cVLSI analogs of neuronal visual processing: A\nsynthesis of form and function,\u201d PhD dissertation, Dept. Com-\nput. Neural Syst., California Inst. Technol., Pasadena, CA, USA,\nMay 1992.\n[42] C. Dong-il\u201cDan\u201d and T. Lee, \u201cA review of bioinspired vision sen-\nsors and their applications,\u201d Sensors Mater. , vol. 27, no. 6,\npp. 447\u2013463, 2015.\n[43] L. Steffen, D. Reichard, J. Weinland, J. Kaiser, A. R \u20aconnau, and\nR. Dillmann, \u201cNeuromorphic stereo vision: A survey of bio-\ninspired sensors and algorithms,\u201d Front. Neurorobot. , vol. 13,\n2019, Art. no. 28.\n[44] T. Delbruck and C. A. Mead, \u201cTime-derivative adaptive silicon\nphotoreceptor array,\u201d in Proc. SPIE Infrared Sensors: Detect. Elec-\ntron. Signal Process. , vol. 1541, 1991, pp. 92\u201399.\n[45] S.-C. Liu and T. Delbruck, \u201cNeuromorphic sensory systems,\u201d\nCurrent Opinion Neurobiol. , vol. 20, no. 3, pp. 288\u2013295, 2010.\n[46] T. Delbruck, B. Linares-Barranco, E. Culurciello, and C. Posch,\n\u201cActivity-driven, event-based vision sensors,\u201d in Proc. IEEE Int.\nSymp. Circuits Syst. , 2010, pp. 2426\u20132429.\n[47] T. Delbruck, \u201cFun with asynchronous vision sensors and proc-\nessing,\u201d in P r o c .E u r .C o n f .C o m p u t .V i s .W o r k s h o p s , 2012, pp. 506\u2013515.\n[48] C. Posch, D. Matolin, and R. Wohlgenannt, \u201cA QVGA 143 dB\ndynamic range asynchronous address-event PWM dynamic\nimage sensor with lossless pixel-level video compression,\u201d in\nProc. IEEE Int. Solid-State Circuits Conf. , 2010, pp. 400\u2013401.\n[49] E. Culurciello, R. Etienne-Cummings, and K. A. Boahen, \u201cA bio-\nmorphic digital image sensor,\u201d IEEE J. Solid-State Circuits ,v o l .3 8 ,\nno. 2, pp. 281\u2013294, Feb. 2003.\n[50] G. Orchard, D. Matolin, X. Lagorce, R. Benosman, and C. Posch,\n\u201cAccelerated frame-free time-encoded multi-step imaging,\u201d in\nProc. IEEE Int. Symp. Circuits Syst. , 2014, pp. 2644\u20132647.\n[51] R. Berner, C. Brandli, M. Yang, S.-C. Liu, and T. Delbruck, \u201cA\n240x180 10mw 12 ms latency sparse-output vision sensor for\nmobile applications,\u201d in Proc. Symp. VLSI Circuits , 2013,\npp. C186\u2013C187.\n[52] E. R. Fossum, \u201cCMOS image sensors: Electronic camera-on-a-\nchip,\u201d IEEE Trans. Electron Devices , vol. 44, no. 10, pp. 1689\u20131698,\nOct. 1997.\n[53] R. Serrano-Gotarredona et al., \u201cCAVIAR: A 45k neuron, 5M syn-\napse, 12G connects/s AER hardware sensory-processing-learning-\nactuating system for high-speed visual object recognition and\ntracking,\u201d IEEE Trans. Neural Netw. , vol. 20, no. 9, pp. 1417\u20131438,\nSep. 2009.[54] J. Conradt, M. Cook, R. Berner, P. Lichtsteiner, R. J. Douglas, and\nT. Delbruck, \u201cA pencil balancing robot using a pair of AER\ndynamic vision sensors,\u201d in Proc. IEEE Int. Symp. Circuits Syst. ,\n2009, pp. 781\u2013784.\n[55] H. Xu, Y. Gao, F. Yu, and T. Darrell, \u201cEnd-to-end learning of\ndriving models from large-scale video datasets,\u201d in Proc. IEEE\nConf. Comput. Vis. Pattern Recognit. , 2017, pp. 3530\u20133538.\n[56] Y. Nozaki and T. Delbruck, \u201cTemperature and parasitic photo-\ncurrent effects in dynamic vision sensors,\u201d IEEE Trans. Electron\nDevices , vol. 64, no. 8, pp. 3239\u20133245, Aug. 2017.\n[57] Y. Nozaki and T. Delbruck, \u201cAuthors reply to comment on temper-\nature and parasitic photocurrent effects in dynamic vision\nsensors,\u201d IEEE Trans. Electron Devices , vol. 65, no. 7, pp. 3083\u20133083,\nJul. 2018.\n[58] T. Serrano-Gotarredona and B. Linares-Barranco, \u201cA 128 /C2128\n1.5% contrast sensitivity 0.9% FPN 3 ms latency 4 mW asynchro-\nnous frame-free dynamic vision sensor using transimpedance\npreampli\ufb01ers,\u201d IEEE J. Solid-State Circuits , vol. 48, no. 3, pp. 827\u2013838,\nMar. 2013.\n[59] M. Yang, S.-C. Liu, and T. Delbruck, \u201cA dynamic vision sensor\nwith 1% temporal contrast sensitivity and in-pixel asynchronous\ndelta modulator for event encoding,\u201d IEEE J. Solid-State Circuits ,\nvol. 50, no. 9, pp. 2149\u20132160, Sep. 2015.\n[60] D. P. Moeys et al., \u201cA sensitive dynamic and active pixel vision\nsensor for color or neural imaging applications,\u201d IEEE Trans.\nBiomed. Circuits Syst. , vol. 12, no. 1, pp. 123\u2013136, Feb. 2018.\n[61] A. Rose, Vision: Human and Electronic . New York, NY, USA: Ple-\nnum, 1973.\n[62] C. Scheerlinck, N. Barnes, and R. Mahony, \u201cContinuous-time\nintensity estimation using event cameras,\u201d in Proc. Asian Conf.\nComput. Vis. , 2018, pp. 308\u2013324.\n[63] C. Scheerlinck, N. Barnes, and R. Mahony, \u201cAsynchronous spa-\ntial image convolutions for event cameras,\u201d IEEE Robot. Autom.\nLett., vol. 4, no. 2, pp. 816\u2013822, Apr. 2019.\n[64] D. Gehrig, H. Rebecq, G. Gallego, and D. Scaramuzza, \u201cEKLT:\nAsynchronous photometric feature tracking using events and\nframes,\u201d Int. J. Comput. Vis. , vol. 128, pp. 601\u2013618, 2020.\n[65] S. Bryner, G. Gallego, H. Rebecq, and D. Scaramuzza, \u201cEvent-\nbased, direct camera tracking from a photometric 3D map using\nnonlinear optimization,\u201d in Proc. IEEE Int. Conf. Robot. Autom. ,\n2019, pp. 325\u2013331.\n[66] G. Gallego, C. Forster, E. Mueggler, and D. Scaramuzza, \u201cEvent-\nbased camera pose tracking using a generative event model,\u201d\n2015, arXiv:1510.01972v1 .\n[67] Prophesee Evaluation Kits, 2020. [Online]. Available: https://\nwww.prophesee.ai/event-based-evk/\n[68] T. Finateu et al., \u201cA 1280 x 720 back-illuminated stacked temporal\ncontrast event-based vision sensor with 4.86 mm pixels, 1.066GEPS\nreadout, programmable event-rate controller and compressive\ndata-formatting pipeline,\u201d in Proc. IEEE Int. Solid-State Circuits\nConf. , 2020, pp. 112\u2013114.\n[69] H. E. Ryu, \u201cIndustrial DVS design; key features and applications.\u201d\nJun. 2019. [Online]. Available: http://rpg.i\ufb01.uzh.ch/docs/\nCVPR19workshop/CVPRW19_Eric_Ryu_Samsung.pdf\n[70] M. Guo, J. Huang, and S. Chen, \u201cLive demonstration: A 768 x 640\npixels 200Meps dynamic vision sensor,\u201d in Proc. IEEE Int. Symp.\nCircuits Syst. , 2017, pp. 1\u20131.\n[71] S. Chen and M. Guo, \u201cLive demonstration: CeleX-V: A 1M pixel\nmulti-mode event-based sensor,\u201d in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit. Workshops , 2019, pp. 1682\u20131683.\n[72] Insightness Event-based Sensor Modules, 2020. [Online]. Avail-\nable: http://www.insightness.com/technology/\n[73] M. L. Katz, K. Nikolic, and T. Delbruck, \u201cLive demonstration:\nBehavioural emulation of event-based vision sensors,\u201d in Proc.\nIEEE Int. Symp. Circuits Syst. , 2012, pp. 736\u2013740.\n[74] H. Rebecq, D. Gehrig, and D. Scaramuzza, \u201cESIM: An open event\ncamera simulator,\u201d in Proc. Conf. Robot. Learn. , 2018, pp. 969\u2013982.\n[75] A. Censi and D. Scaramuzza, \u201cLow-latency event-based visual\nodometry,\u201d in Proc. IEEE Int. Conf. Robot. Autom. , 2014, pp. 703\u2013710.\n[76] M. Yang, S.-C. Liu, and T. Delbruck, \u201cAnalysis of encoding degra-\ndation in spiking sensors due to spike delay variation,\u201d IEEE Trans.\nCircuits Syst. I, Reg. Papers , vol. 64, no. 1, pp. 145\u2013155, Jan. 2017.\n[77] T. Delbruck, V. Villanueva, and L. Longinotti, \u201cIntegration of\ndynamic vision sensor with inertial measurement unit for elec-\ntronically stabilized event-based vision,\u201d in Proc. IEEE Int. Symp.\nCircuits Syst. , 2014, pp. 2636\u20132639.GALLEGO ET AL.: EVENT-BASED VISION: A SURVEY 175",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 21
        }
    },
    {
        "page_content": "[78] R. Berner, \u201cHighspeed USB2.0 AER interfaces,\u201d Master\u2019s thesis,\nDept. Elect. Inf. Eng. (D-ITET), ETH Zurich, Zurich, Switzerland,\n2006.\n[79] iniVation, \u201cUnderstandingthe performance of neuromorphic\nevent-based vision sensors,\u201d May 2020. [Online]. Available:\nhttps://inivation.com/dvp/white-papers/\n[80] G. Taverni et al., \u201cFront and back illuminated dynamic and active\npixel vision sensors comparison,\u201d IEEE Trans. Circuits Syst., II,\nExp. Briefs , vol. 65, no. 5, pp. 677\u2013681, May 2018.\n[81] E. Mueggler, H. Rebecq, G. Gallego, T. Delbruck, and D. Scara-\nmuzza, \u201cThe event-camera dataset and simulator: Event-based\ndata for pose estimation, visual odometry, and SLAM,\u201d Int. J.\nRobot. Res. , vol. 36, no. 2, pp. 142\u2013149, 2017.\n[82] G. Gallego, M. Gehrig, and D. Scaramuzza, \u201cFocus is all you need:\nLoss functions for event-based vision,\u201d in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit. , 2019, pp. 12272\u201312281.\n[83] D. Gehrig, A. Loquercio, K. Derpanis, and D. Scaramuzza, \u201cEnd-\nto-end learning of representations for asynchronous event-based\ndata,\u201d in Proc. Int. Conf. Comput. Vis. , 2019, pp. 5632\u20135642.\n[84] D. Weikersdorfer and J. Conradt, \u201cEvent-based particle \ufb01ltering\nfor robot self-localization,\u201d in Proc. IEEE Int. Conf. Robot. Biomi-\nmetics , 2012, pp. 866\u2013870.\n[85] F. Paredes-Vall /C19es, K. Y. W. Scheper, and G. C. H. E. de Croon,\n\u201cUnsupervised learning of a hierarchical spiking neural network\nfor optical \ufb02ow estimation: From events to global motion\nperception,\u201d IEEE Trans. Pattern Anal. Mach. Intell. , vol. 42, no. 8,\npp. 2051\u20132064, Aug. 2020.\n[86] C. Reinbacher, G. Munda, and T. Pock, \u201cReal-time panoramic\ntracking for event cameras,\u201d in Proc. IEEE Int. Conf. Comput. Pho-\ntography , 2017, pp. 1\u20139.\n[87] E. Mueggler, G. Gallego, H. Rebecq, and D. Scaramuzza,\n\u201cContinuous-time visual-inertial odometry for event cameras,\u201d\nIEEE Trans. Robot. , vol. 34, no. 6, pp. 1425\u20131440, Dec. 2018.\n[88] M. Liu and T. Delbruck, \u201cAdaptive time-slice block-matching\noptical \ufb02ow algorithm for dynamic vision sensors,\u201d in Proc. Brit.\nMach. Vis. Conf. , 2018, pp. 1\u201312.\n[89] A. Aimar et al. , \u201cNullHop: A \ufb02exible convolutional neural net-\nwork accelerator based on sparse representations of feature\nmaps,\u201d IEEE Trans. Neural Netw. Learn. Syst. , vol. 30, no. 3,\npp. 644\u2013656, Mar. 2019.\n[90] J. Kogler, C. Sulzbachner, and W. Kubinger, \u201cBio-inspired stereo\nvision system with silicon retina imagers,\u201d in Proc. Int. Conf. Com-\nput. Vis. Syst. , 2009, pp. 174\u2013183.\n[91] A. I. Maqueda, A. Loquercio, G. Gallego, N. Garc /C19\u0131a, and\nD. Scaramuzza, \u201cEvent-based vision meets deep learning on\nsteering prediction for self-driving cars,\u201d in Proc. IEEE Conf.\nComput. Vis. Pattern Recognit. , 2018, pp. 5419\u20135427.\n[92] T. Delbruck, \u201cFrame-free dynamic digital vision,\u201d in Proc. Int.\nSymp. Secure-Life Electron. , 2008, pp. 21\u201326.\n[93] X. Lagorce, G. Orchard, F. Gallupi, B. E. Shi, and R. B. Benosman,\n\u201cHOTS: A hierarchy of event-based time-surfaces for pattern rec-\nognition,\u201d IEEE Trans. Pattern Anal. Mach. Intell. , vol. 39, no. 7,\npp. 1346\u20131359, Jul. 2017.\n[94] M. A. R. Ahad, J. K. Tan, H. Kim, and S. Ishikawa, \u201cMotion his-\ntory image: Its variants and applications,\u201d Mach. Vis. Appl. ,v o l .2 3 ,\nno. 2, pp. 255\u2013281, Mar. 2012.\n[95] I. Alzugaray and M. Chli, \u201cAsynchronous corner detection and\ntracking for event cameras in real time,\u201d IEEE Robot. Autom. Lett. ,\nvol. 3, no. 4, pp. 3177\u20133184, Oct. 2018.\n[96] J. Manderscheid, A. Sironi, N. Bourdis, D. Migliore, and V. Lepetit,\n\u201cSpeed invariant time surface for learning to detect corner points\nwith event-based cameras,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit. , 2019, pp. 10237\u201310246.\n[97] A. Sironi, M. Brambilla, N. Bourdis, X. Lagorce, and R. Benos-\nman, \u201cHATS: Histograms of averaged time surfaces for robust\nevent-based object classi\ufb01cation,\u201d in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit. , 2018, pp. 1731\u20131740.\n[98] V. Vasco, A. Glover, and C. Bartolozzi, \u201cFast event-based Harris\ncorner detection exploiting the advantages of event-driven cam-\neras,\u201d in Proc. IEEE Int. Conf. Intell. Robots Syst. , 2016, pp. 4144\u20134149.\n[99] E. Mueggler, C. Bartolozzi, and D. Scaramuzza, \u201cFast event-\nbased corner detection,\u201d in Proc. Brit. Mach. Vis. Conf. , 2017,\npp. 1\u201311.\n[100] Y. Zhou, G. Gallego, H. Rebecq, L. Kneip, H. Li, and D. Scara-\nmuzza, \u201cSemi-dense 3D reconstruction with a stereo event\ncamera,\u201d in Proc. Eur. Conf. Comput. Vis. , 2018, pp. 242\u2013258.[101] P. Bardow, A. J. Davison, and S. Leutenegger, \u201cSimultaneous\noptical \ufb02ow and intensity estimation from an event camera,\u201d in\nProc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2016, pp. 884\u2013892.\n[102] L. Wang, I. S. M. Mostafavi, Y.-S. Ho, and K.-J. Yoon, \u201cEvent-\nbased high dynamic range image and very high frame rate video\ngeneration using conditional generative adversarial networks,\u201d\ninProc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2019,\npp. 10073\u201310082.\n[103] A. Z. Zhu, L. Yuan, K. Chaney, and K. Daniilidis,\n\u201cUnsupervised event-based learning of optical \ufb02ow, depth,\nand egomotion,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recog-\nnit., 2019, pp. 989\u2013997.\n[104] H. Rebecq, R. Ranftl, V. Koltun, and D. Scaramuzza, \u201cEvents-to-\nvideo: Bringing modern computer vision to event cameras,\u201d in\nProc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2019, pp. 3852\u20133861.\n[105] Y. Sekikawa, K. Hara, and H. Saito, \u201cEventNet: Asynchronous\nrecursive event processing,\u201d in Proc. IEEE Conf. Comput. Vis. Pat-\ntern Recognit. , 2019, pp. 3882\u20133891.\n[106] M. Litzenberger et al. , \u201cEmbedded vision system for real-time\nobject tracking using an asynchronous transient vision sensor,\u201d\ninProc. Digit. Signal Process. Workshop , 2006, pp. 173\u2013178.\n[107] Z. Ni, A. Bolopion, J. Agnus, R. Benosman, and S. R /C19egnier,\n\u201cAsynchronous event-based visual shape tracking for stable hap-\ntic feedback in microrobotics,\u201d IEEE Trans. Robot. , vol. 28, no. 5,\npp. 1081\u20131089, Oct. 2012.\n[108] Z. Ni, S.-H. Ieng, C. Posch, S. R /C19egnier, and R. Benosman, \u201cVisual\ntracking using neuromorphic asynchronous event-based cam-\neras,\u201d Neural Comput. , vol. 27, no. 4, pp. 925\u2013953, 2015.\n[109] D. Tedaldi, G. Gallego, E. Mueggler, and D. Scaramuzza,\n\u201cFeature detection and tracking with the dynamic and active-\npixel vision sensor (DAVIS),\u201d in Proc. Int. Conf. Event-Based Con-\ntrol Commun. Signal Process. , 2016, pp. 1\u20137.\n[110] B. Kueng, E. Mueggler, G. Gallego, and D. Scaramuzza, \u201cLow-\nlatency visual odometry using event-based feature tracks,\u201d in\nProc. IEEE Int. Conf. Intell. Robots Syst. , 2016, pp. 16\u201323.\n[111] G. Gallego and D. Scaramuzza, \u201cAccurate angular velocity esti-\nmation with an event camera,\u201d IEEE Robot. Autom. Lett. , vol. 2,\nno. 2, pp. 632\u2013639, Apr. 2017.\n[112] G. Gallego, H. Rebecq, and D. Scaramuzza, \u201cA unifying contrast\nmaximization framework for event cameras, with applications to\nmotion, depth, and optical \ufb02ow estimation,\u201d in Proc. IEEE Conf.\nComput. Vis. Pattern Recognit. , 2018, pp. 3867\u20133876.\n[113] H. Rebecq, T. Horstschaefer, and D. Scaramuzza, \u201cReal-time\nvisual-inertial odometry for event cameras using keyframe-\nbased nonlinear optimization,\u201d in Proc. Brit. Mach. Vis. Conf. ,\n2017, pp. 1\u201312.\n[114] A. Z. Zhu, N. Atanasov, and K. Daniilidis, \u201cEvent-based feature\ntracking with probabilistic data association,\u201d in Proc. IEEE Int.\nConf. Robot. Autom. , 2017, pp. 4465\u20134470.\n[115] A. Z. Zhu, N. Atanasov, and K. Daniilidis, \u201cEvent-based visual\ninertial odometry,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recog-\nnit., 2017, pp. 5816\u20135824.\n[116] A. Mitrokhin, C. Fermuller, C. Parameshwara, and Y. Aloimo-\nnos, \u201cEvent-based moving object detection and tracking,\u201d in\nProc. IEEE Int. Conf. Intell. Robots Syst. , 2018, pp. 1\u20139.\n[117] A. Mitrokhin, C. Ye, C. Fermuller, Y. Aloimonos, and T. Delbruck,\n\u201cEV-IMO: Motion segmentation dataset and learning pipeline for\nevent cameras,\u201d in Proc. IEEE Int. Conf. Intell. Robots Syst. , 2019,\npp. 6105\u20136112.\n[118] T. Brosch, S. Tschechne, and H. Neumann, \u201cOn event-based\noptical \ufb02ow detection,\u201d Front. Neurosci. , vol. 9, Apr. 2015,\nArt. no. 137.\n[119] C. Reinbacher, G. Graber, and T. Pock, \u201cReal-time intensity-\nimage reconstruction for event cameras using manifold regu-\nlarisation,\u201d in Proc. Brit. Mach. Vis. Conf. , 2016, pp. 1\u201312.\n[120] H. Akolkar, S. Panzeri, and C. Bartolozzi, \u201cSpike time based unsu-\npervised learning of receptive \ufb01elds for event-driven vision,\u201d in\nProc. IEEE Int. Conf. Robot. Autom. , 2015, pp. 4258\u20134264.\n[121] J. A. P /C19erez-Carrasco et al., \u201cMapping from frame-driven to frame-\nfree event-driven vision systems by low-rate rate coding and coin-\ncidence processing\u2013application to feedforward ConvNets,\u201d IEEE\nTrans. Pattern Anal. Mach. Intell. , vol. 35, no. 11, pp. 2706\u20132719,\nNov. 2013.\n[122] P. O\u2019Connor, D. Neil, S.-C. Liu, T. Delbruck, and M. Pfeiffer,\n\u201cReal-time classi\ufb01cation and sensor fusion with a spiking deep\nbelief network,\u201d Front. Neurosci. , vol. 7, 2013, Art. no. 178.176 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 1, JANUARY 2022",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 22
        }
    },
    {
        "page_content": "[123] P. U. Diehl, D. Neil, J. Binas, M. Cook, S.-C. Liu, and M. Pfeiffer,\n\u201cFast-classifying, high-accuracy spiking deep networks through\nweight and threshold balancing,\u201d in Proc. Int. Joint Conf. Neural\nNetw. , 2015, pp. 2933\u20132940.\n[124] S. K. Esser et al. , \u201cConvolutional networks for fast, energy-\nef\ufb01cient neuromorphic computing,\u201d Proc. Nat. Acad. Sci. USA ,\nvol. 113, no. 41, pp. 11 441\u201311 446, 2016.\n[125] B. Rueckauer, I.-A. Lungu, Y. Hu, M. Pfeiffer, and S.-C. Liu,\n\u201cConversion of continuous-valued deep networks to ef\ufb01cient\nevent-driven networks for image classi\ufb01cation,\u201d Front. Neurosci. ,\nvol. 11, 2017, Art. no. 682.\n[126] S. B. Shrestha and G. Orchard, \u201cSLAYER: Spike layer error reas-\nsignment in time,\u201d in Proc. Int. Conf. Neural Inf. Process. Syst. ,\n2018, pp. 1412\u20131421.\n[127] J. H. Lee, T. Delbruck, and M. Pfeiffer, \u201cTraining deep spiking\nneural networks using backpropagation,\u201d Front. Neurosci. ,v o l .1 0 ,\n2016, Art. no. 508.\n[128] E. Neftci, \u201cData and power ef\ufb01cient intelligence with neuromor-\nphic learning machines,\u201d iScience , vol. 5, pp. 52\u201368, 2018.\n[129] J. Kogler, C. Sulzbachner, M. Humenberger, and F. Eibensteiner,\n\u201cAddress-event based stereo vision with bio-inspired silicon ret-\nina imagers,\u201d in Advances in Theory and Applications of Stereo\nVision . Rijeka, Croatia: InTech, 2011, pp. 165\u2013188.\n[130] H. Li, G. Li, and L. Shi, \u201cClassi\ufb01cation of spatiotemporal events\nbased on random forest,\u201d in Proc. Int. Conf. Brain Inspired Cogn.\nSyst., 2016, pp. 138\u2013148.\n[131] A. Nguyen, T.-T. Do, D. G. Caldwell, and N. G. Tsagarakis,\n\u201cReal-time 6DOF pose relocalization for event cameras with\nstacked spatial LSTM networks,\u201d in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit. Workshops , 2019, pp. 1638\u20131645.\n[132] E. Mueggler, C. Forster, N. Baumli, G. Gallego, and D. Scaramuzza,\n\u201cLifetime estimation of events from dynamic vision sensors,\u201d in\nProc. IEEE Int. Conf. Robots Autom. , 2015, pp. 4874\u20134881.\n[133] S.-H. Ieng, J. Carneiro, M. Osswald, and R. Benosman,\n\u201cNeuromorphic event-based generalized time-based stereovision,\u201d\nFront. Neurosci. , vol. 12, 2018, Art. no. 442.\n[134] D. P. Moeys et al. , \u201cSteering a predator robot using a mixed\nframe/event-driven convolutional neural network,\u201d in Proc. Int.\nConf. Event-Based Control Commun. Signal Process. , 2016, pp. 1\u20138.\n[135] I.-A. Lungu, F. Corradi, and T. Delbruck, \u201cLive demonstration:\nConvolutional neural network driven by dynamic vision sensor\nplaying RoShamBo,\u201d in Proc. IEEE Int. Symp. Circuits Syst. , 2017,\npp. 1\u20131.\n[136] J. Binas, D. Neil, S.-C. Liu, and T. Delbruck, \u201cDDD17: End-to-end\nDAVIS driving dataset,\u201d in Proc. ICML Workshop Mach. Learn.\nAuton. Veh. , 2017, pp. 1\u20139.\n[137] C. Ye, A. Mitrokhin, C. Parameshwara, C. Ferm \u20aculler, J. A. Yorke,\nand Y. Aloimonos, \u201cUnsupervised learning of dense optical\n\ufb02ow, depth and egomotion with event-based sensors,\u201d IEEE/RSJ\nInt. Conf. Intell. Robots and Systems (IROS) , 2020.\n[138] T. Stoffregen and L. Kleeman, \u201cSimultaneous optical \ufb02ow and\nsegmentation (SOFAS) using dynamic vision sensor,\u201d in Proc.\nAustralas. Conf. Robot. Autom. , 2017, pp. 1\u201310.\n[139] T. Stoffregen, G. Gallego, T. Drummond, L. Kleeman, and\nD. Scaramuzza, \u201cEvent-based motion segmentation by motion\ncompensation,\u201d in Proc. Int. Conf. Comput. Vis. , 2019, pp. 7243\u20137252.\n[140] L. A. Camu ~nas-Mesa, T. Serrano-Gotarredona, and B. Linares-\nBarranco, \u201cEvent-driven sensing and processing for high-speed\nrobotic vision,\u201d in Proc. IEEE Biomed. Circuits Syst. Conf. , 2014,\npp. 516\u2013519.\n[141] G. Orchard, R. Benosman, R. Etienne-Cummings, and\nN. V. Thakor, \u201cA spiking neural network architecture for\nvisual motion estimation,\u201d in Proc. IEEE Biomed. Circuits Syst.\nConf. , 2013, pp. 298\u2013301.\n[142] E. Chicca, P. Lichtsteiner, T. Delbruck, G. Indiveri, and\nR. J. Douglas, \u201cModeling orientation selectivity using a neuro-\nmorphic multi-chip system,\u201d in Proc. IEEE Int. Symp. Circuits\nSyst., 2006.\n[143] R. L. D. Valois, N. P. Cottaris, L. E. Mahon, S. D. Elfar, and\nJ. Wilson, \u201cSpatial and temporal receptive \ufb01elds of geniculate\nand cortical cells and directional selectivity,\u201d Vis. Res. ,\nvol. 40, no. 27, pp. 3685\u20133702, 2000.\n[144] F. Rea, G. Metta, and C. Bartolozzi, \u201cEvent-driven visual atten-\ntion for the humanoid robot iCub,\u201d Front. Neurosci. , vol. 7, 2013,\nArt. no. 234.\n[145] L. Itti and C. Koch, \u201cComputational modelling of visual\nattention,\u201d Nat. Rev. Neurosci. , vol. 2, no. 3, pp. 194\u2013203, 2001.[146] D. Marr and T. Poggio, \u201cCooperative computation of stereo dis-\nparity,\u201d Science , vol. 194, no. 4262, pp. 283\u2013287, 1976.\n[147] M. Mahowald, The Silicon Retina . Boston, MA, USA: Springer,\n1994, pp. 4\u201365.\n[148] M. Osswald, S.-H. Ieng, R. Benosman, and G. Indiveri, \u201cA spik-\ning neural network model of 3D perception for event-based neu-\nromorphic stereo vision systems,\u201d Sci. Rep. , vol. 7, no. 1, Jan. 2017,\nArt. no. 40703.\n[149] G. Dikov, M. Firouzi, F. R \u20acohrbein, J. Conradt, and C. Richter,\n\u201cSpiking cooperative stereo-matching at 2ms latency with neuro-\nmorphic hardware,\u201d in Proc. Conf. Biomimetic Biohybrid Syst. ,\n2017, pp. 119\u2013137.\n[150] E. Piatkowska, A. N. Belbachir, and M. Gelautz, \u201cAsynchronous\nstereo vision for event-driven dynamic stereo sensor using an\nadaptive cooperative approach,\u201d in Proc. Int. Conf. Comput. Vis.\nWorkshops , 2013, pp. 45\u201350.\n[151] V. Vasco, A. Glover, Y. Tirupachuri, F. Solari, M. Chessa, and\nC. Bartolozzi, \u201cVergence control with a neuromorphic iCub,\u201d in\nProc. IEEE-RAS Int. Conf. Humanoid Robots , 2016, pp. 732\u2013738.\n[152] M. Riesenhuber and T. Poggio, \u201cHierarchical models of object rec-\nognition in cortex,\u201d Nat. Neurosci. , vol. 2, no. 11, pp. 1019\u20131025,\nNov. 1999.\n[153] H. Akolkar et al. \u201cWhat can neuromorphic event-driven precise\ntiming add to spike-based pattern recognition?\u201d Neural Comput. ,\nvol. 27, no. 3, pp. 561\u2013593, Mar. 2015.\n[154] L. A. Camunas-Mesa, T. Serrano-Gotarredona, S. H. Ieng,\nR. B. Benosman, and B. Linares-Barranco, \u201cOn the use of orienta-\ntion \ufb01lters for 3D reconstruction in event-driven stereo vision,\u201d\nFront. Neurosci. , vol. 8, 2014, Art. no. 48.\n[155] M. B. Milde, D. Neil, A. Aimar, T. Delbr \u20acuck, and G. Indiveri,\n\u201cADaPTION: Toolbox and benchmark for training convolutional\nneural networks with reduced numerical precision weights and\nactivation,\u201d 2017, arXiv:1711.04713 .\n[156] E. Stromatias, M. Soto, T. Serrano-Gotarredona, and B. Linares-\nBarranco, \u201cAn event-driven classi\ufb01er for spiking neural net-\nworks fed with synthetic or dynamic vision sensor data,\u201d Front.\nNeurosci. , vol. 11, Jun. 2017, Art. no. 350.\n[157] E. Neftci, C. Augustine, S. Paul, and G. Detorakis, \u201cEvent-\ndriven random back-propagation: Enabling neuromorphic\ndeep learning machines,\u201d Front. Neurosci. , vol. 11, 2017,\nArt. no. 324.\n[158] D. Drazen, P. Lichtsteiner, P. H \u20aca\ufb02iger, T. Delbr \u20acuck, and A. Jensen,\n\u201cToward real-time particle tracking using an event-based dynamic\nvision sensor,\u201d Experiments Fluids , vol. 51, no. 5, pp. 1465\u20131469,\n2011.\n[159] Z. Ni, C. Pacoret, R. Benosman, S.-H. Ieng, and S. R /C19egnier,\n\u201cAsynchronous event-based high speed vision for microparticle\ntracking,\u201d J. Microscopy , vol. 245, no. 3, pp. 236\u2013244, 2012.\n[160] E. Piatkowska, A. N. Belbachir, S. Schraml, and M. Gelautz,\n\u201cSpatiotemporal multiple persons tracking using dynamic vision\nsensor,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Work-\nshops , 2012, pp. 35\u201340.\n[161] X. Lagorce, C. Meyer, S.-H. Ieng, D. Filliat, and R. Benosman,\n\u201cAsynchronous event-based multikernel algorithm for high-\nspeed visual features tracking,\u201d IEEE Trans. Neural Netw. Learn.\nSyst., vol. 26, no. 8, pp. 1710\u20131720, Aug. 2015.\n[162] A. Glover and C. Bartolozzi, \u201cRobust visual tracking with a\nfreely-moving event camera,\u201d in Proc. IEEE Int. Conf. Intell. Robots\nSyst., 2017, pp. 3769\u20133776.\n[163] D. R. Valeiras, X. Lagorce, X. Clady, C. Bartolozzi, S.-H. Ieng, and\nR. Benosman, \u201cAn asynchronous neuromorphic event-driven\nvisual part-based shape tracking,\u201d IEEE Trans. Neural Netw.\nLearn. Syst. , vol. 26, no. 12, pp. 3045\u20133059, Dec. 2015.\n[164] M. A. Fischler and R. A. Elschlager, \u201cThe representation and\nmatching of pictorial structures,\u201d IEEE Trans. Comput. , vol. C-22,\nno. 1, pp. 67\u201392, Jan. 1973.\n[165] C. Harris and M. Stephens, \u201cA combined corner and edge\ndetector,\u201d in Proc. 4th Alvey Vis. Conf. , 1988, pp. 147\u2013151.\n[166] B. D. Lucas and T. Kanade, \u201cAn iterative image registration tech-\nnique with an application to stereo vision,\u201d in Proc. Int. Joint\nConf. Artif. Intell. , 1981, pp. 674\u2013679.\n[167] X. Clady, S.-H. Ieng, and R. Benosman, \u201cAsynchronous event-\nbased corner detection and matching,\u201d Neural Netw. , vol. 66,\npp. 91\u2013106, 2015.\n[168] E. Rosten and T. Drummond, \u201cMachine learning for high-\nspeed corner detection,\u201d in P r o c .E u r .C o n f .C o m p u t .V i s . , 2006,\npp. 430\u2013443.GALLEGO ET AL.: EVENT-BASED VISION: A SURVEY 177",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 23
        }
    },
    {
        "page_content": "[169] V. Vasco, A. Glover, E. Mueggler, D. Scaramuzza, L. Natale, and\nC. Bartolozzi, \u201cIndependent motion detection with event-driven\ncameras,\u201d in Proc. IEEE Int. Conf. Adv. Robot. , 2017, pp. 530\u2013536.\n[170] Y. Hu, H. Liu, M. Pfeiffer, and T. Delbruck, \u201cDVS benchmark\ndatasets for object tracking, action recognition, and object recog-\nnition,\u201d Front. Neurosci. , vol. 10, 2016, Art. no. 405.\n[171] B. Rueckauer and T. Delbruck, \u201cEvaluation of event-based algo-\nrithms for optical \ufb02ow with ground-truth from inertial measure-\nment sensor,\u201d Front. Neurosci. , vol. 10, 2016, Art. no. 176.\n[172] R. Benosman, S.-H. Ieng, C. Clercq, C. Bartolozzi, and M. Sriniva-\nsan, \u201cAsynchronous frameless event-based optical \ufb02ow,\u201d Neural\nNetw. , vol. 27, pp. 32\u201337, 2012.\n[173] F. Barranco, C. Fermuller, and Y. Aloimonos, \u201cContour motion\nestimation for asynchronous event-driven cameras,\u201d Proc. IEEE ,\nvol. 102, no. 10, pp. 1537\u20131556, Oct. 2014.\n[174] G. Haessig, A. Cassidy, R. Alvarez, R. Benosman, and G. Orchard,\n\u201cSpiking optical \ufb02ow for event-based sensors using IBM\u2019s True-\nNorth neurosynaptic system,\u201d IEEE Trans. Biomed. Circuits Syst. ,\nvol. 12, no. 4, pp. 860\u2013870, Aug. 2018.\n[175] A. Z. Zhu, D. Thakur, T. Ozaslan, B. Pfrommer, V. Kumar, and\nK. Daniilidis, \u201cThe multivehicle stereo event camera dataset: An\nevent camera dataset for 3D perception,\u201d IEEE Robot. Autom.\nLett., vol. 3, no. 3, pp. 2032\u20132039, Jul. 2018.\n[176] R. Hartley and A. Zisserman, Multiple View Geometry in Computer\nVision , 2nd ed., Cambridge, U.K.: Cambridge Univ. Press, 2003.\n[177] S. Schraml, A. N. Belbachir, N. Milosevic, and P. Sch \u20acon,\n\u201cDynamic stereo vision system for real-time tracking,\u201d in Proc.\nIEEE Int. Symp. Circuits Syst. , 2010, pp. 1409\u20131412.\n[178] J. Kogler, M. Humenberger, and C. Sulzbachner, \u201cEvent-based\nstereo matching approaches for frameless address event stereo\ndata,\u201d in Proc. Int. Symp. Adv. Vis. Comput. , 2011, pp. 674\u2013685.\n[179] J. Lee et al., \u201cLive demonstration: Gesture-based remote control\nusing stereo pair of dynamic vision sensors,\u201d in Proc. IEEE Int.\nSymp. Circuits Syst. , 2012, pp. 741\u2013745.\n[180] E. Piatkowska, A. N. Belbachir, and M. Gelautz, \u201cCooperative\nand asynchronous stereo vision for dynamic vision sensors,\u201d\nMeas. Sci. Technol. , vol. 25, no. 5, Apr. 2014, Art. no. 055108.\n[181] R. Benosman, S.-H. Ieng, P. Rogister, and C. Posch,\n\u201cAsynchronous event-based Hebbian epipolar geometry,\u201d IEEE\nTrans. Neural Netw. , vol. 22, no. 11, pp. 1723\u20131734, Nov. 2011.\n[182] J. Carneiro, S.-H. Ieng, C. Posch, and R. Benosman, \u201cEvent-based\n3D reconstruction from neuromorphic retinas,\u201d Neural Netw. ,\nvol. 45, pp. 27\u201338, 2013.\n[183] D. Zou et al., \u201cContext-aware event-driven stereo matching,\u201d in\nProc. IEEE Int. Conf. Image Process. , 2016, pp. 1076\u20131080.\n[184] D. Zou et al., \u201cRobust dense depth map estimation from sparse\nDVS stereos,\u201d in Proc. Brit. Mach. Vis. Conf. , 2017, pp. 1\u201311.\n[185] M. Firouzi and J. Conradt, \u201cAsynchronous event-based coopera-\ntive stereo matching using neuromorphic silicon retinas,\u201d Neural\nProc. Lett. , vol. 43, no. 2, pp. 311\u2013326, 2016.\n[186] J. Kogler, F. Eibensteiner, M. Humenberger, C. Sulzbachner,\nM. Gelautz, and J. Scharinger, \u201cE nhancement of sparse silicon ret-\nina-based stereo matching using belief propagation and two-stage\npost\ufb01ltering,\u201d J. Electron. Imag. , vol. 23, no. 4, pp. 1\u201315, 2014.\n[187] Z. Xie, S. Chen, and G. Orchard, \u201cEvent-based stereo depth esti-\nmation using belief propagation,\u201d Front. Neurosci. , vol. 11, 2017,\nArt. no. 535.\n[188] Z. Xie, J. Zhang, and P. Wang, \u201cEvent-based stereo matching\nusing semiglobal matching,\u201d Int. J. Adv. Robot. Syst. ,v o l .1 5 ,n o .1 ,\npp. 1\u201311, 2018.\n[189] H. Hirschmuller, \u201cStereo processing by semiglobal matching and\nmutual information,\u201d IEEE Trans. Pattern Anal. Mach. Intell. ,v o l .3 0 ,\nno. 2, pp. 328\u2013341, Feb. 2008.\n[190] A. Andreopoulos, H. J. Kashyap, T. K. Nayak, A. Amir, and\nM. D. Flickner, \u201cA low power, high throughput, fully event-\nbased stereo system,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Rec-\nognit. , 2018, pp. 7532\u20137542.\n[191] A. Z. Zhu, Y. Chen, and K. Daniilidis, \u201cRealtime time synchro-\nnized event-based stereo,\u201d in Proc. Eur. Conf. Comput. Vis. , 2018,\npp. 433\u2013447.\n[192] R. Szeliski, Computer Vision: Algorithms and Applications . Berlin,\nGermany: Springer, 2010.\n[193] C. Cadena et al., \u201cPast, present, and future of simultaneous locali-\nzation and mapping: Toward the robust-perception age,\u201d IEEE\nTrans. Robot. , vol. 32, no. 6, pp. 1309\u20131332, Dec. 2016.\n[194] R. T. Collins, \u201cA space-sweep approach to true multi-image\nmatching,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. ,\n1996, pp. 358\u2013363.[195] S. Thrun, W. Burgard, and D. Fox, Probabilistic Robotics . Cam-\nbridge, MA, USA: MIT Press, 2005.\n[196] D. Weikersdorfer, R. Hoffmann, and J. Conradt, \u201cSimultaneous\nlocalization and mapping for event-based vision systems,\u201d in\nProc. Int. Conf. Comput. Vis. Syst. , 2013, pp. 133\u2013142.\n[197] D. Weikersdorfer, D. B. Adrian, D. Cremers, and J. Conradt, \u201cEvent-\nbased 3D SLAM with a depth-augm ented dynamic vision sensor,\u201d\ninProc. IEEE Int. Conf. Robot. Autom. , 2014, pp. 359\u2013364.\n[198] E. Mueggler, B. Huber, and D. Scaramuzza, \u201cEvent-based, 6-DOF\npose tracking for high-speed maneuvers,\u201d in Proc. IEEE Int. Conf.\nIntell. Robots Syst. , 2014, pp. 2761\u20132768.\n[199] S. Barua, Y. Miyatani, and A. Veeraraghavan, \u201cDirect face detec-\ntion and video reconstruction from event cameras,\u201d in Proc. IEEE\nWinter Conf. Appl. Comput. Vis. , 2016, pp. 1\u20139.\n[200] C. Brandli, L. Muller, and T. Delbruck, \u201cReal-time, high-speed\nvideo decompression using a frame- and event-based DAVIS\nsensor,\u201d in Proc. IEEE Int. Symp. Circuits Syst. , 2014, pp. 686\u2013689.\n[201] G. Munda, C. Reinbacher, and T. Pock, \u201cReal-time intensity-image\nreconstruction for event cameras using manifold regularisation,\u201d\nInt. J. Comput. Vis. , vol. 126, no. 12, pp. 1381\u20131393, 2018.\n[202] C. Scheerlinck, H. Rebecq, D. Gehrig, N. Barnes, R. E. Mahony, and\nD. Scaramuzza, \u201cFast image reconstruction with an event camera,\u201d\ninProc. IEEE Winter Conf. Appl. Comput. Vis. , 2020, pp. 156\u2013163.\n[203] A. N. Belbachir, S. Schraml, M. Mayerhofer, and M. Hofst \u20acaetter,\n\u201cA novel HDR depth camera for real-time 3D 360-degree pan-\noramic vision,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.\nWorkshops , 2014, pp. 425\u2013432.\n[204] C. Scheerlinck, H. Rebecq, T. Stoffregen, N. Barnes, R. Mahony,\nand D. Scaramuzza, \u201cCED: Color event camera dataset,\u201d in Proc.\nIEEE Conf. Comput. Vis. Pattern Recognit. Workshops , 2019,\npp. 1684\u20131693.\n[205] G. Wiesmann, S. Schraml, M. Litzenberger, A. N. Belbachir,\nM. Hofstatter, and C. Bartolozzi, \u201cEvent-driven embodied sys-\ntem for feature extraction and object recognition in robotic\napplications,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.\nWorkshops , 2012, pp. 76\u201382.\n[206] D. Gehrig, M. Gehrig, J. Hidalgo-Carri /C19o, and D. Scaramuzza,\n\u201cVideo to events: Recycling video datasets for event cameras,\u201d in\nProc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2020, pp. 3583\u20133592.\n[207] A. Zanardi, A. J. Aumiller, J. Zilly, A. Censi, and E. Frazzoli,\n\u201cCross-modal learning \ufb01lters for RGB-neuromorphic wormhole\nlearning,\u201d in Proc. Robot.: Sci. Syst. , 2019, Art. no. P45.\n[208] G. Orchard, A. Jayawant, G. K. Cohen, and N. Thakor,\n\u201cConverting static image datasets to spiking neuromorphic data-\nsets using saccades,\u201d Front. Neurosci. , vol. 9, 2015, Art. no. 437.\n[209] D. Neil and S.-C. Liu, \u201cEffective sensor fusion with event-based\nsensors and deep network architectures,\u201d in Proc. IEEE Int. Symp.\nCircuits Syst. , 2016, pp. 2282\u20132285.\n[210] Y. Wu, L. Deng, G. Li, J. Zhu, and L. Shi, \u201cSpatio-temporal back-\npropagation for training high-performance spiking neural\nnetworks,\u201d Front. Neurosci. , vol. 12, 2018, Art. no. 331.\n[211] A. Yousefzadeh, G. Orchard, T. Serrano-Gotarredona, and\nB. Linares-Barranco, \u201cActive perception with dynamic vision\nsensors. Minimum saccades with optimum recognition,\u201d IEEE\nTrans. Biomed. Circuits Syst. , vol. 12, no. 4, pp. 927\u2013939, Aug. 2018.\n[212] X. Clady, J.-M. Maro, S. Barr /C19e, and R. B. Benosman, \u201cA motion-\nbased feature for event-based pattern recognition,\u201d Front. Neuro-\nsci., vol. 10, Jan. 2017, Art. no. 594.\n[213] C. A. Sims, \u201cImplications of rational inattention,\u201d J. Monetary\nEcon. , vol. 50, pp. 665\u2013690, 2003.\n[214] M. Miskowicz, Event-Based Control and Signal Processing . Boca\nRaton, FL, USA: CRC Press, 2018.\n[215] W. P. M. H. Heemels, K. H. Johansson, and P. Tabuada, \u201cAn\nintroduction to event-triggered and self-triggered control,\u201d in\nProc. IEEE Conf. Decis. Control , 2012, pp. 3270\u20133285.\n[216] K. J. Astr \u20acom, Event Based Control . Berlin, Germany: Springer,\n2008, pp. 127\u2013147.\n[217] B. Wang and M. Fu, \u201cComparison of periodic and event-based\nsampling for linear state estimation,\u201d IFAC Proc. Vol. , vol. 47,\npp. 5508\u20135513, 2014.\n[218] A. Censi, E. Mueller, E. Frazzoli, and S. Soatto, \u201cA power-perfor-\nmance approach to comparing sensor families, with application to\ncomparing neuromorphic to traditional vision sensors,\u201d in Proc.\nIEEE Int. Conf. Robot. Autom. , 2015, pp. 3319\u20133326.\n[219] E. Mueller, A. Censi, and E. Frazzoli, \u201cLow-latency heading feed-\nback control with neuromorphic vision sensors using ef\ufb01cient\napproximated incremental inference,\u201d in Proc. IEEE Conf. Decis.\nControl , 2015, pp. 992\u2013999.178 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 1, JANUARY 2022",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 24
        }
    },
    {
        "page_content": "[220] P. Singh, S. Z. Yong, J. Gregoire, A. Censi, and E. Frazzoli,\n\u201cStabilization of linear continuous-time systems using neuro-\nmorphic vision sensors,\u201d in Proc. IEEE Conf. Decis. Control , 2016,\npp. 3030\u20133036.\n[221] A. Censi, \u201cEf\ufb01cient neuromorphic optomotor heading regulation,\u201d\ninProc. IEEE Amer. Control Conf. , 2015, pp. 3854\u20133861.\n[222] E. Mueller, A. Censi, and E. Fr azzoli, \u201cEf\ufb01cient high speed\nsignal estimation with neuromorphic vision sensors,\u201d in Proc.\nInt. Conf. Event-Based Control Commun. Signal Process. , 2015,\npp. 1\u20138.\n[223] D. Kahneman, Thinking, Fast and Slow . New York, NY, USA: Far-\nrar, Straus, 2011.\n[224] S. B. Furber et al. , \u201cOverview of the SpiNNaker system\narchitecture,\u201d IEEE Trans. Comput. , vol. 62, no. 12, pp. 2454\u20132467,\nDec. 2013.\n[225] F. Akopyan et al., \u201cTrueNorth: Design and tool \ufb02ow of a 65 mW 1\nmillion neuron programmable neurosynaptic chip,\u201d IEEE Trans.\nComput.-Aided Design Integr. Circuits Syst. , vol. 34, no. 10,\npp. 1537\u20131557, Oct. 2015.\n[226] M. Davies et al. , \u201cLoihi: A neuromorphic manycore processor\nwith on-chip learning,\u201d IEEE Micro , vol. 38, no. 1, pp. 82\u201399,\nJan./Feb. 2018.\n[227] S. Moradi, N. Qiao, F. Stefanini, and G. Indiveri, \u201cA scalable mul-\nticore architecture with heterogeneous memory structures for\ndynamic neuromorphic asynchronous processors (DYNAPs),\u201d\nIEEE Trans. Biomed. Circuits Syst. , vol. 12, no. 1, pp. 106\u2013122, Feb.\n2018.\n[228] A. S. Neckar, \u201cBraindrop: A mixed signal neuromorphic archi-\ntecture with a dynamical systems-based programming model,\u201d\nPhD dissertation, Dept. Elec. Eng., Stanford Univ., Stanford, CA,\nUSA, Jun. 2018.\n[229] L. A. Camunas-Mesa, B. Linares-Barranco, and T. Serrano-Gotar-\nredona, \u201cNeuromorphic spiking neural networks and their mem-\nristor-CMOS hardware implementations,\u201d Materials , vol. 12,\nno. 17, Aug. 2019, Art. no. 2745.\n[230] B. Rajendran, A. Sebastian, M. Schmuker, N. Srinivasa, and\nE. Eleftheriou, \u201cLow-power neuromorphic hardware for signal\nprocessing applications: A review of architectural and system-\nlevel design approaches,\u201d IEEE Signal Process. Mag. , vol. 36,\nno. 6, pp. 97\u2013110, Nov. 2019.\n[231] S. B. Furber, F. Galluppi, S. Temple, and L. A. Plana, \u201cThe SpiN-\nNaker project,\u201d Proc. IEEE , vol. 102, no. 5, pp. 652\u2013665, May 2014.\n[232] G. Haessig, F. Galluppi, X. Lagorce, and R. Benosman,\n\u201cNeuromorphic networks on the SpiNNaker platform,\u201d in Proc.\nIEEE Int. Conf. Artif. Intell. Circuits Syst. , 2019, pp. 86\u201391.\n[233] C. Richter, F. R \u20acohrbein, and J. Conradt, \u201cBio inspired optic \ufb02ow\ndetection using neuromorphic hardware,\u201d in Proc. Bernstein\nConf. , 2014, pp. 1\u20131.\n[234] A. Glover, A. B. Stokes, S. Furber, and C. Bartolozzi, \u201cATIS + SpiN-\nNaker: A fully event-based visual tracking demonstration,\u201d in Proc.\nIEEE/RSJ Int. Conf. Intell. Robot. Syst. Workshops , 2018, pp. 1\u20132.\n[235] T. Serrano-Gotarredona, B. Linares-Barranco, F. Galluppi,\nL. Plana, and S. Furber, \u201cConvNets experiments on SpiNNaker,\u201d\ninProc. IEEE Int. Symp. Circuits Syst. , 2015, pp. 2405\u20132408.\n[236] P. A. Merolla et al., \u201cA million spiking-neuron integrated circuit\nwith a scalable communication network and interface,\u201d Science ,\nvol. 345, no. 6197, pp. 668\u2013673, 2014.\n[237] P. Blouw, X. Choo, E. Hunsberger, and C. Eliasmith,\n\u201cBenchmarking keyword spotting ef\ufb01ciency on neuromorphic\nhardware,\u201d 2018, arXiv: 1812.01739 .\n[238] T. Bekolay et al. , \u201cNengo: A Python tool for building large-\nscale functional brain models,\u201d Front. Neuroinf. , vol. 7, 2014,\nArt. no. 48.\n[239] C. Eliasmith et al., \u201cA large-scale model of the functioning brain,\u201d\nScience , vol. 338, no. 6111, pp. 1202\u20131205, 2012.\n[240] N. Waniek, J. Biedermann, and J. Conradt, \u201cCooperative SLAM\non small mobile robots,\u201d in Proc. IEEE Int. Conf. Robot. Biomimet-\nics, 2015, pp. 1810\u20131815.\n[241] B. J. P. Hordijk, K. Y. Scheper, and G. C. D. Croon, \u201cVertical land-\ning for micro air vehicles using event-based optical \ufb02ow,\u201d J. Field\nRobot. , vol. 35, no. 1, pp. 69\u201390, Jan. 2017.\n[242] S. J. Carey, A. Lopich, D. R. Barr, B. Wang, and P. Dudek, \u201cA\n100,000 fps vision sensor with embedded 535 GOPS/W 256x256\nSIMD processor array,\u201d in Proc. VLSI Circuits Symp. , 2013,\npp. 182\u2013183.\nGuillermo Gallego (Senior Member , IEEE)\nreceived the PhD degree in electrical and com-\nputer engineering from the Georgia Institute of\nTechnology , Atlanta, Georgia, in 2011. He is cur-\nrently an associate professor at the Department\nof Electrical Engineering and Computer Science,\nTechnische Universit \u20acat Berlin, Berlin, Germany .\nFrom 2011 to 2014, he was a Marie Curie\nresearcher with the Universidad Politecnica de\nMadrid, Spain, and from 2014 to 2019 he was a\npostdoctoral researcher with the University of\nZurich, Switzerland.\nTobi Delbr \u20acuck (Fellow, IEEE) received the BSc\ndegree in physics from the UC San Diego, San\nDiego, California, in 1986, and the PhD degree\nfrom the Caltech, Pasadena, California, in 1993.\nHe is a professor of physics and electrical engi-\nneering with the Institute of Neuroinformatics,\nETH Zurich, Zurich, Switzerland, where he has\nbeen since 1998. His group with S.-C. Liu focuses\non neuromorphic sensory processing and ef\ufb01-\ncient deep learning.\nGarrick Orchard received the PhD degree in\nelectrical and computer engineering from Johns\nHopkins University, Baltimore, Maryland, in 2012.\nHe is currently a researcher with the Neuromor-\nphic Computing Laboratory, Intel Labs, Santa\nClara, California. From 2012 to 2019, he was\nsenior research scientist with Temasek Laborato-\nries and Singapore Institute for Neurotechnology ,\nNational University of Singapore.\nChiara Bartolozzi (Member , IEEE) received the\ndegree in engineering from the University of Gen-\nova, Genoa, Italy, and the PhD degree in neuroin-\nformatics from ETH Zurich, Zurich, Switzerland,\ndeveloping analog subthreshold circuits for emu-\nlating biophysical neuronal properties onto silicon\nand modelling selective attention on hierarchical\nmulti-chip systems. She is currently a researcher\nwith the Istituto Italiano di Tecnologia (IIT), Italy .\nShe leads the Neuromorphic Systems and Inter-\nfaces Group, IIT , with the aim of applying neuro-\nmorphic engineering to design autonomous\nrobotic machines.\nBrian Taba received the BS degree in electrical\nengineering from the California Institute of Tech-\nnology, Pasadena, California, in 1999, and the\nPhD degree in bioengineering from the University\nof Pennsylvania, Philadelphia, Pennsylvania. He\nis currently a researcher with IBM, within the\nSyNAPSE Project.\nAndrea Censi received the PhD degree in con-\ntrol & dynamical systems from the California Insti-\ntute of Technology, Pasadena, California, in 2012.\nHe is currently a deputy director for the Chair of\nDynamic Systems and Control (Prof. Frazzoli) at\nthe Institute for Dynamic Systems and Control,\nDepartment of Mechanical and Process Engi-\nneering, ETH Z \u20acurich. From 2013 to 2017, he was\na postdoctoral researcher with the Laboratory for\nInformation and Decision Systems, Massachu-\nsetts Institute of Technology , Cambridge, Massa-\nchusetts.GALLEGO ET AL.: EVENT-BASED VISION: A SURVEY 179",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 25
        }
    },
    {
        "page_content": "Stefan Leutenegger received the PhD degree in\nmechanical engineering from Autonomous Sys-\ntems Lab, ETH Zurich, Zurich, Switzerland, in\n2014. He is currently a senior lecturer in robotics\nat the Department of Computing, Imperial Col-\nlege London, U.K. 2014, Since 2014, he leads the\nSmart Robotics Lab, Imperial College London\nand co-leads research with the Dyson Robotics\nLab together with Prof. A. Davison. He is co-\nfounder of the startup SLAMcore.\nAndrew J. Davison is currently a professor of\nrobot vision and director of the Dyson Robotics\nLaboratory, Imperial College London. His\nresearch focus is on SLAM and its evolution\ntowards general \u201cSpatial AI.\u201d He has also had\nstrong involvement in taking this technology into\nreal applications, in particular through his work\nwith Dyson and as co-founder of SLAMcore. He\nwas elected fellow of the Royal Academy of Engi-\nneering, in 2017.\nJ\u20acorg Conradt (Senior Member , IEEE) received\nthe PhD degree in physics/neuroscience from\nETH Zurich, Zurich, Switzerland. He is currently\nan associate professor at the School of Electrical\nEngineering and Computer Science, KTH, Stock-\nholm, Sweden. Before joining KTH, he was W1\nprofessor with the Technische Universit \u20acat\nM\u20acunchen, Germany . He was the founding director\nof the Elite Master Program NeuroEngineering,\nTechnische Universit \u20acat M \u20acunchen.\nKostas Daniilidis (Fellow, IEEE) received the\nPhD degree in computer science from the Univer-\nsity of Karlsruhe, Karlsruhe, Germany, in 1992.\nHe is currently the currently Ruth Yalom Stone\nprofessor of computer and information science\nwith the University of Pennsylvania where he has\nbeen faculty since 1998. He was the director of\nthe interdisciplinary GRASP Laboratory from\n2008 to 2013, associate dean for graduate edu-\ncation from 2012-2016, and director of online\nlearning since 2016. His main interest include in\ndeep learning of 3D representations, data association, event-based\ncameras, semantic localization and mapping, and vision based manipu-\nlation.\nDavide Scaramuzza (Senior Member , IEEE)\nreceived the PhD degree in robotics and com-\nputer vision from ETH Z \u20acurich, Z \u20acurich, Switzer-\nland, in 2008. He is currently an associate\nprofessor of robotics and perception at the Uni-\nversity of Z \u20acurich, Switzerland, where he does\nresearch on autonomous, vision-based naviga-\ntion of mini drones and event cameras. For his\nresearch contributions, he received a European\nResearch Council (ERC) Grant, the IEEE Robot-\nics and Automation Early Career Award, and sev-\neral industry and paper awards.\n\"For more information on this or any other computing topic,\nplease visit our Digital Library at www.computer .org/csdl.180 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 1, JANUARY 2022",
        "metadata": {
            "source": "pdf/180.pdf",
            "page": 26
        }
    }
]