{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RAG import run_maven, load_document_batch, get_rag_response\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Ur key\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n",
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='THESIS FOR THE DEGREE OF DOCTOR OF PHILOSOPHY\\n\\nFrom Industry to Practice: Can Users Tackle Domain Tasks with Augmented Reality?\\n\\nYUCHONG ZHANG\\n\\nDepartment of Computer Science and Engineering\\n\\nDivision of Interaction Design and Software Engineering\\n\\nCHALMERS UNIVERSITY OF TECHNOLOGY\\n\\nGöteborg, Sweden 2023\\n\\nFrom Industry to Practice: Can Users Tackle Domain Tasks with Augmented Reality?\\n\\nYUCHONG ZHANG Göteborg, Sweden 2023 ISBN 978-91-7905-784-8\\n\\nCOPYRIGHT © YUCHONG ZHANG, 2023\\n\\nDoktorsavhandlingar vid Chalmers tekniska högskola Ny serie Nr 5250 ISSN 0346-718X\\n\\nISSN 1652-0769 Technical Report 230D Division of Interaction Design and Software Engineering Department of Computer Science and Engineering Chalmers University of Technology SE-412 96 Göteborg, Sweden Telephone: +46 (0)31-772 1000\\n\\nPrinted by Chalmers Reproservice Göteborg, Sweden 2023\\n\\nFrom Industry to Practice: Can Users Tackle Domain Tasks with Augmented Reality? YUCHONG ZHANG Division of Interaction Design and Software Engineering Department of Computer Science and Engineering Chalmers University of Technology\\n\\nAbstract\\n\\nAugmented Reality (AR) is a cutting-edge interactive technology. While Vir- tual Reality (VR) is based on completely virtual and immersive environments, AR superimposes virtual objects onto the real world. The value of AR has been demonstrated and applied within numerous industrial application areas due to its capability of providing interactive interfaces of visualized digital content. AR can provide functional tools that support users in undertaking domain-related tasks, especially facilitating them in data visualization and interaction by jointly augmenting physical space and user perception. Making effective use of the ad- vantages of AR, especially the ability which augment human vision to help users perform different domain-related tasks is the central part of my PhD research.\\n\\nIndustrial process tomography (IPT), as a non-intrusive and commonly-used imaging technique, has been effectively harnessed in many manufacturing com- ponents for inspections, monitoring, product quality control, and safety issues. IPT underpins and facilitates the extraction of qualitative and quantitative data regarding the related industrial processes, which is usually visualized in various ways for users to understand its nature, measure the critical process characteris- tics, and implement process control in a complete feedback network. The adoption of AR in benefiting IPT and its related fields is currently still scarce, resulting in a gap between AR technique and industrial applications. This thesis establishes a bridge between AR practitioners and IPT users by accomplishing four stages. First of these is a need-finding study of how IPT users can harness AR tech- nique was developed. Second, a conceptualized AR framework, together with the implemented mobile AR application developed in an optical see-through (OST) head-mounted display (HMD) was proposed. Third, the complete approach for IPT users interacting with tomographic visualizations as well as the user study was investigated.\\n\\nBased on the shared technologies from industry, we propose and examine an AR approach for visual search tasks providing visual hints, audio hints, and gaze- assisted instant post-task feedback as the fourth stage. The target case was a book-searching task, in which we aimed to explore the effect of the hints and\\n\\niii\\n\\nthe feedback with two hypotheses: that both visual and audio hints can posi- tively affect AR search tasks whilst the combination outperforms the individuals; that instant post-task feedback can positively affect AR search tasks. The proof- of-concept was demonstrated by an AR app in an HMD with a two-stage user evaluation. The first one was a pilot study (n=8) where the impact of the vi- sual hint in benefiting search task performance was identified. The second was a comprehensive user study (n=96) consisting of two sub-studies, Study I (n=48) and Study II (n=48). Following quantitative and qualitative analysis, our results partially verified the first hypothesis and completely verified the second, enabling us to conclude that the synthesis of visual and audio hints conditionally improves AR search task efficiency when coupled with task feedback.\\n\\nKeywords: Industrial Process Tomography, Augmented Reality, Human-centered Design, User Study, Qualitative and Quantitative Analysis\\n\\niv\\n\\nAcknowledgements\\n\\nI consider my entire PhD journey as an evolution from ’nearly knowing nothing’ to ’knowing something’. I would not have accomplished it without the support of many people. First and foremost, I would like to thank my parents who firmly stand behind me me all the time. I’ve been studying and working abroad alone for eight years, it is unimaginable for me to become who I am if I don’t have their constant support. I also want to express my gratitude to all of my family members caring about me, they are my strongest backing.\\n\\nI would like to express my sincere appreciation to my supervisor Prof. Morten Fjeld for choosing me as one of his PhD students at Chalmers. 5 years ago, I got the chance to become a PhD researcher and write the new chapter of my life. He is not only a supervisor but more a good friend of mine, giving me full support and recognition unconditionally at all time. Then, I would like also to thank my co-supervisors Dr Alan Said. He is really a kind person with strong academic skills, which helped my learning and growth a lot in my PhD study. Also, I would like to thank my previous co-supervisor Prof. Marco Fratarcangeli, he is a perfect role model for academic researchers.\\n\\nMy sincere thanks to t2i lab and my former and current colleagues. Special thanks to Tomasz Kosiński, Mads Rønnow for both of their colleagueship but more importantly the long-term friendship. Thanks to Ziming Wang, Yemao Man for being good lab mates/division mates, it is lucky for me to have some people to communicate with in my mother tongue. Many thanks to everyone in Interaction Design, IDSE, CSE, Chalmers, a cohesive and friendly unit which makes my PhD journey more wonderful.\\n\\nI would like to thank more people who helped me in some forms during my PhD. They are: Adam Nowak, Andrzej Romanowski, Paweł Woźniak, Adel Om- rani, Rahul Yadav, Guruprasad Rao, Barbara Stuckey, Shengdong Zhao, Philippa Beckman.\\n\\nFinally, I would like to give my gratitude to the project I was involved in which constituted most of my PhD – TOMOCON (https://www.tomocon.eu/). This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No. 764902.\\n\\nv\\n\\nList of main publications\\n\\nThis thesis presents an introduction, summary and extension to the following appended main publications: papers I to V\\n\\n[I] Y. Zhang, A. Nowak, G. Rao, A. Romanowski, and M. Fjeld, “Is Industrial Tomography Ready for Augmented Reality? A Need-finding Study of How Augmented Reality Can Be Adopted by Industrial Tomography Experts”, in International Conference on Human-Computer Interaction (Springer, 2023, in press).\\n\\n[II] Y. Zhang, R. Yadav, A. Omrani, and M. Fjeld, “A Novel Augmented Reality System To Support Volumetric Visualization in Industrial Process Tomog- raphy”, in Proceedings of the 2021 Conference on Interfaces and Human Computer Interaction (2021), pp. 3–9.\\n\\n[III] Y. Zhang, A. Omrani, R. Yadav, and M. Fjeld, “Supporting Visualization Analysis in Industrial Process Tomography by Using Augmented Reality–A Case Study of an Industrial Microwave Drying System”, Sensors 21, 6515 (2021).\\n\\n[IV] Y. Zhang, Y. Xuan, A. Omrani, R. Yadav, and M. Fjeld, “Playing with Data: An Augmented Reality Approach to Immersively Interact with Vi- sualizations of Industrial Process Tomography”, in IFIP Conference on Human-Computer Interaction (Springer, 2023, Under review).\\n\\n[V] Y. Zhang, A. Nowak, X. Yueming, R. Andrzej, and F. Morten, “See or Hear? Exploring the Effect of Visual and Audio Hints and Gaze-assisted Task Feedback for Visual Search Tasks in Augmented Reality”, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 2023 (Under review).\\n\\nWe always refer to these publications in the thesis as papers I, II, III, IV, and V, according to the labeling in the list above.\\n\\nvii\\n\\nSpecification of my contributions to the appended main publications – papers I to V included in the thesis\\n\\nI Led and directed the work included in this paper. Finished the survey study with data collection and analysis. Have written most of the paper with feedback from my co-authors.\\n\\nII Led and directed the work included in this paper. Finished the conceptual- ization, mobile app development, and data analysis. Have written most of the paper with feedback from my co-authors.\\n\\nIII Led and directed the work included in this paper. Finished the conceptual- ization, mobile app development, and data analysis. Have written at least 70% of the paper together with my co-authors.\\n\\nIV Led and directed the work included in this paper. Finished the conceptu- alization, reasoning, experimental design, user study implementation, and data analysis. Have written at least 90% of the paper together with my co-authors.\\n\\nV Led and directed the work included in this paper. Finished the conceptu- alization, reasoning, experimental design, user study implementation, and data analysis. Have written at least 90% of the paper together with my co-authors.\\n\\nviii\\n\\nList of other publications\\n\\n[1] Y. Zhang, M. Fjeld, M. Fratarcangeli, A. Said, and S. Zhao, “Affective Colormap Design for Accurate Visual Comprehension in Industrial Tomog- raphy”, Sensors 21, 4766 (2021).\\n\\n[2] A. Nowak, Y. Zhang, A. Romanowski, and M. Fjeld, “Augmented Reality with Industrial Process Tomography: To Support Complex Data Analysis in 3D Space”, in Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers (2021).\\n\\n[3] Y. Zhang, A. Nowak, A. Romanowski, and M. Fjeld, “An Initial Explo- ration of Visual Cues in Head-mounted Display Augmented Reality for Book Searching”, in Proceedings of the 21st International Conference on Mobile and Ubiquitous Multimedia (2022).\\n\\n[4] Y. Zhang, A. Nowak, A. Romanowski, and M. Fjeld, “On-site or Remote Working?: An Initial Solution on How COVID-19 Pandemic May Impact Augmented Reality Users”, in Proceedings of the 2022 International Con- ference on Advanced Visual Interfaces (2022), pp. 1–3.\\n\\nix\\n\\nList of figures\\n\\n1.1 Overview of this thesis. . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n2.1 The example scene of an IPT laboratory located at Lodz, Poland. 2.2 The three AR modalities presented to the participants. A: Head Mounted\\n\\nDisplay: These devices can be further categorized into optical see- through and video see-through [20], displaying information inside the device while allowing users to see the surrounding real world; B: Hand Held Display [30]: These are mobile screens, smartphones, or tablets with embedded camera and screen. The device fits within the user’s hands and can display virtual objects; C: Spatial Aug- mented Reality [31]: These systems utilize digital projectors to dis- play virtual information onto the physical objects. A clear descrip- . . . . . . tion of each modality was provided for our participants. 2.3 Conceptualization of our proposed AR approach for IPT visualization analysis. a): The specialized IPT controlled industrial process is implemented in a confined environment. b): The IPT data visual- izations originate from different processes. c): Users engage with relevant data analysis. d): User equipped with proposed AR en- vironment by an OST HMD interacting with the visualizations for . . . . . . . . . . . . . . . . . . . . . . . . . . . . further analysis.\\n\\n4.1 The answers of how domain experts responded to the current AR sta- tus. A: Q: Comfort: Which AR scenario shown above looks most comfortable for you? B: Q: Usability: Which AR scenario shown above looks most usable for you (might help with your work)? C: Q: Are you aware of any ongoing AR applications in industrial to- mography? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 The conceptualization framework of the proposed AR system is: the MWT-controlled industrial microwave drying process block, which- covers the whole running procedure of the process and provides data and elements used for analysis; users’ block, representing the peo- ple involved in this context, such as operators who control and run the process and researchers who observe and analyze the process; the AR app block, which entails the AR implementation of a mobile AR app developed on the iOS/Android platform at the initial stage. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . [83, 111].\\n\\n4\\n\\n7\\n\\n9\\n\\n16\\n\\n18\\n\\n4.3 The realization of the proposed AR system. Users hold the handheld smartphones/tablets to run the AR app alongside the industrial setting. After the app activation, users can then observe the vi- sualizations displayed on the AR interfaces. At present, the app is supported via iOS/Android mobile devices. Users are empow- ered to switch to different visualizations regarding different phases of the process. The visualizations shown on the interfaces are mul- timodal, including the infrared image showing the condition of the process and the 3D/2D reconstruction of the MWT images (the upper left small figure on the floating interface in the upper right subfigure); the reconstructed MWT images with their segmented re- sults to show the high moisture areas (the middle right subfigure); and the volumetric visualization (the bottom right subfigure) [111]. 4.4 Flowchart of our user study with counterbalancing design. . . . . . . 4.5 Statistical results between the AR approach and the conventional set- ting. a: The distribution of the understandability levels (1 to 7) rated by participants between the AR approach and the conven- tional setting. b: The distribution of task completion time (s). c: The distribution of the error rate (0.00 to 1.00). d: The distribu- tion of the SUS scores representing usability (0 to 100). e: The . . . . . . . . . distribution of the recommendation levels (1 to 7). 4.6 Study I (a) and Study II (b): The mean TCT (s) used per task by the . . . . four groups. Error bars show mean ± standard error (SE). 4.7 Study I: NASA TLX for the first task by the four groups. Error bars show mean ± standard error (SE). . . . . . . . . . . . . . . . . . . 4.8 Study I: NASA TLX for the second task by the four groups. Error bars show mean ± standard error (SE). . . . . . . . . . . . . . . . . . . 4.9 Study II: NASA TLX for the first task by the four groups. Error bars show mean ± standard error (SE). . . . . . . . . . . . . . . . . . . 4.10 Study II: NASA TLX for the second task by the four groups. Error . . . . . . . . . . . . . . .\\n\\nbars show mean ± standard error (SE).\\n\\n20 23\\n\\n24\\n\\n25\\n\\n26\\n\\n26\\n\\n27\\n\\n27\\n\\nContents\\n\\nAbstract\\n\\nAbstract\\n\\n2.1 Need-finding: Is AR Ready for IPT? . . . . . . . . . . . . . . . . . 2.2 AR Framework and Mobile Realization for IPT . . . . . . . . . . . 2.3 The Complete AR Approach with Evaluation for IPT . . . . . . . 2.4 Supporting AR Visual Search Tasks . . . . . . . . . . . . . . . . . 3.1 AR Visualization with IPT . . . . . . . . . . . . . . . . . . . . . . 3.1.1 Industrial AR . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.2 Visualization for IPT . . . . . . . . . . . . . . . . . . . . . 3.2 AR for Visual Search Tasks . . . . . . . . . . . . . . . . . . . . . . 3.2.1 AR with gaze assistance . . . . . . . . . . . . . . . . . . . . 3.2.2 AR with hints and feedback . . . . . . . . . . . . . . . . . . 4.1 Need-finding: Is AR Ready for IPT? . . . . . . . . . . . . . . . . . 4.1.1 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.2 Results 4.2 AR Framework and Mobile AR Development . . . . . . . . . . . . 4.2.1 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.2 Results 4.3 The Complete AR Approach . . . . . . . . . . . . . . . . . . . . . 4.3.1 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3.2 Results v vii ix xi xiii 1 5 5 7 8 9 11 11 11 11 12 13 13 15 15 15 16 17 18 19 21 21 23\\n\\nAcknowledgements\\n\\nList of main publications\\n\\nList of other publications\\n\\nList of figures\\n\\nContents\\n\\n1 Introduction\\n\\n2 Background\\n\\n3 Related Work\\n\\n4 Problem Solving\\n\\nxiii\\n\\niii\\n\\n4.4 Practical Visual Search Tasks . . . . . . . . . . . . . . . . . . . . . 4.4.1 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4.2 Results\\n\\n5 Discussions 5.1 Need-finding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 AR Framework and Mobile Implementation . . . . . . . . . . . . . 5.3 The Complete AR Approach with Evaluation . . . . . . . . . . . . 5.4 Practical Visual Search Tasks . . . . . . . . . . . . . . . . . . . . . 6 Conclusion References Appended Papers: Main Publications Paper I Paper II Paper III Paper IV Paper V Appended Papers: Other Publications Paper 1 Paper 2 Paper 3 Paper 4 29 29 29 30 31 33 35 47 49 63 73 91 101 121 123 145 151 157\\n\\nxiv\\n\\n24 24 26\\n\\n1 Introduction\\n\\nThe main work presented in this thesis first focuses on investigating how aug- mented reality (AR) can help the domain users in specific industrial processes monitored by industrial process tomography (IPT). Second, by sharing the same technique, it also identifies the capability of AR in improving practical visual search task performance with gaze assistance. This thesis illustrates the research done in the appended papers I to V, regarding the initial adoption of AR tech- niques for IPT and practical task performing with respect to four stages: need- finding of how AR can be used by IPT experts; AR framework conceptualization and mobile realization; the complete AR approach with evaluation; and the prac- tical search task performing exploration.\\n\\nIn each paper, the research questions and the contributions are elicited and generated. Tables 1.1 and 1.2 give an overview of the specification of the papers I to V which constitute the main content of this thesis.\\n\\nThis thesis is summarized as follows and shown in Figure 1.1. First, Chapter 1 gives a preliminary overview of the research as well as the structure presented in the thesis. Chapter 2 introduces the general background of each of the four stages. The related work and contemporary research regarding each specific field is elab- orated in Chapter 3. Chapter 4 illustrates the methodologies used and systems developed for each stage, as well as the results obtained correspondingly. Finally, discussions and conclusions with future work recommendations are presented in Chapters 5 and 6.\\n\\n1\\n\\n2\\n\\nChapter 1. Introduction\\n\\nTable 1.1: The research questions and research contributions listed in the papers I to V. Paper\\n\\nResearch Questions\\n\\nResearch Contribu- tions 1. Give an overview of the present status of us- ing AR by IPT experts.\\n\\nI Could AR be used by IPT experts to effec- tively support process monitoring ? II III IV How can mobile AR sup- port IPT users with vol- umetric visualization? How can mobile AR sup- port IPT users for onsite analysis? Can OST HMD AR fa- cilitate IPT users for in- teraction and visualiza- tion analysis?\\n\\n2. Identify the need of using AR by IPT ex- perts to effectively sup- port process monitoring. 3. Present the poten- tial challenges in deploy- ing AR in IPT 1. Propose an AR sys- tem in supporting col- laborative analysis with interactivity and mobil- ity for IPT. 2. Involve volumetric vi- sualization regarding the IPT processes to support mutual collaboration of users. 1. Propose an entire data processing and vi- sualizing workflow of the IPT controlled industrial process. 2. Integrate AR tech- nique to support IPT data visualization and on-site analysis for users. 1. Pioneer a complete AR approach to help IPT users for contextual data visualization analy- sis by OST HMDs.\\n\\nPaper\\n\\nV\\n\\nTable 1.2: Cont.\\n\\nResearch Questions\\n\\nResearch Contribu- tions 2. Provide immersive ex- perience and interactive visualizations for users to interact and commu- nicate with the IPT vi- sualizations for better task understandability, performance and user experience. Design and im- 3. plement a comparative study to prove the effec- tiveness of the proposed AR approach compared to conventional methods and acquire early-stage feedback. 1. Propose an AR ap- proach supporting visual and audio hints, as well as gaze-assisted instant post-task feedback for visual search tasks. 2. Explore the effect of visual hints, audio combined hints, hints, and instant post-task feedback through gaze assistance on contextual AR searching. 3. Explore the effect of combining hints and task feedback in the same AR context.\\n\\nHow can hints and task feedback affect AR vi- sual search task perfor- mance?\\n\\n3\\n\\n4\\n\\nChapter 1. Introduction\\n\\nChapter 1 > Chapter 2 Background Need-finding a AR framework study: AR for and mobile IPT? realization Related work AR visualization AR for visual with IPT search tasks Industrial Visualization AR with gaze . AR for IPT assistance Problem solving Chapter 4 AR framework The Penne A . complete AR Practical visual Needfinding for IPT and mobile approach for IPT search tasks development a Methods and results approach for IPT search task The complete mH For AR visual | AR with hints and feedback Discussions AR Framework ; E A ‘ The complete AR Practical visual Needfinding for IPT ee approach for IPT search tasks Conclusion\\n\\nFigure 1.1: Overview of this thesis.\\n\\n2 Background\\n\\nThis chapter gives a detailed overview of the relevant background within the con- text of this thesis, including the four stages mentioned above in the introduction.\\n\\n2.1 Need-finding: Is AR Ready for IPT?\\n\\n\\n\\nFigure 2.1: The example scene of an IPT laboratory located at Lodz, Poland.\\n\\nIPT is a dedicated and non-invasive imaging technique which is pervasively used in manufacturing scenarios for process monitoring or integrated control[1–6]. It serves as an effective mechanism to extract complex data which is visualized in various representations and interprets it for domain users to comprehend the essence of the industrial processes [7–12]. An example view of an IPT working environment is shown in Figure 2.1. Due to the speciality and complexity of IPT, some rising technologies are therefore harnessed and made more accessible for all users - expert and lay - to perform complex tomographic data analysis to im- prove their productivity and efficiency. Some typical representatives of IPT, such as microwave tomography (MWT) [11, 13, 14], electrical resistance tomography (ERT) [15], and electrical capacitance tomography (ECT) [16] are widely used\\n\\n5\\n\\n6\\n\\n2 Background\\n\\nfor industrial purposes such as moisture detection [17, 18], crack detection and powder flow in pipes, and flow pattern detection of granules. This thesis specifi- cally concentrates on a unique industrial microwave drying process [10] which uses precise drying and heating equipment for polymer foams with the aid of MWT. The targets of this drying process are firstly heating the object foam placed on the conveyor belt and then detecting the post-drying moisture distribution, by adopting the MWT technique.\\n\\nWhile having some common points with Virtual Reality (VR), augmented real- ity/mixed reality (AR/MR) is an interactive technique where computer-generated perceptual information is visually superimposed on the real world [19–26]. Whereas in VR, a user interacts with virtual objects in a fully artificial three-dimensional world, in AR, a user’s view of the real environment is enhanced with virtual ob- jects shown at the right time and position from the user’s perspective [27]. AR technology has been effectively deployed in various applications within industry due to its high mobility and interactivity. For example, AR can provide func- tional tools that support users when undertaking domain-related tasks such as data visualization and interaction because of its ability to jointly augment the physical space and the user’s perception [28, 29]. Taking devices and hardware as key parameters [19, 28], AR can be classified into three categories:\\n\\n• Head-Mounted Displays (HMD) and wearable hardware: These devices can be further categorized into optical see-through and video see- through [20], displaying information inside the device while allowing users to see the surrounding real world.\\n\\n• Hand-Held Displays (HHD): These are mobile screens, smartphones, or tablets with embedded camera and screen. The device fits within the user’s hands and can display virtual objects.\\n\\n• Spatial Augmented Reality (SAR): These systems utilize digital projec- tors to display virtual information onto physical objects.\\n\\nAR has successfully provided accessible and innovative solutions for various industrial application domains due to its capacity for interactive visualization through intelligent user interfaces[32]. Given its proven success within the auto- motive industry [33], shipbuilding [28], and robotics [29], our driving question is whether AR could also be useful in the domain of industrial tomography. The re- search question is thus elicited: Could AR effectively support process monitoring in industrial tomography? Both to capture the current status and to examine the potential of AR in this domain, we conducted a need-finding study in the form of a systematic survey to model the involved 14 participants. All were industrial tomography experts with at least three years of hands-on experience, whom we considered eligible to offer sufficiently in-depth insights and opinions.\\n\\n\\n\\nFigure 2.2: The three AR modalities presented to the participants. A: Head Mounted Display: These devices can be further categorized into optical see-through and video see-through [20], displaying information inside the device while allowing users to see the surrounding real world; B: Hand Held Display [30]: These are mobile screens, smartphones, or tablets with embedded camera and screen. The device fits within the user’s hands and can display virtual objects; C: Spatial Augmented Reality [31]: These systems utilize digital projectors to display virtual information onto the physical objects. A clear description of each modality was provided for our participants.\\n\\n2.2 AR Framework and Mobile Realization for IPT\\n\\nAfter discovering the high potential of deploying AR/MR into the IPT domain from the need-finding process, we aimed to develop a realistic AR method for benefiting the relevant users. We identified that even though AR has been proved and applied in numerous fields such as medicine and education [34], due to defi- ciencies such as hardware limitation, poor computation power and low knowledge convertibility, it is still not a prevalent tool in most industrial application domains [19, 35]. Given that AR is capable of providing an interactive interface displaying digital content and offering ways to interact with information in a multimodal and collaborative manner [36], it has great potential to fill the gap between de- velopers and IPT domain users by inciting novel mechanisms for onsite analysis. Hence, how to develop effective AR tools or systems which provide users with straightforward and comprehensible visualizations pertaining to the IPT process data becomes the central topic.\\n\\nVisualization plays a dominant role in IPT-controlled industrial processes in that it can display the relevant data in an observation-friendly manner, which is then provided to domain users. The information related to the designated process, for example, a 2D graph of the tomographic spectrum or a 3D reconstruction of the voxel data, if collected and visualized instantly in a reasonable way, can help in better understanding of the process among the users [37]. Sophisticated data analysis always demands efficient reproduction of the measurement regarding the imaging process by IPT to uncover the material distribution inside the closed containers where the industrial process is executed [38]. As mentioned before, AR can provide functional tools that support users undertaking domain-related tasks, especially facilitating them in data visualization and interaction because\\n\\n7\\n\\n8\\n\\n2 Background\\n\\nof its ability to jointly augment the physical space and the user’s perception [28, 37]. Based on the interactivity and accessibility which AR can supply, we aim to explore the deeper usage of this technique in the IPT domain to support process data visualization and analysis.\\n\\nIn this stage, we introduce a novel and preliminary AR framework as well as the mobile realization to support interactive and collaborative onsite analysis regarding volumetric visualization in IPT. In our AR system, we are capable of providing users with an interactive way with high mobility to conduct visualization analysis alongside the ongoing industrial process. The necessary information is visualized in our mobile AR App which was built on iOS devices as an early-stage setup. Moreover, the proposed AR system incorporates multimodal visualizations used for characterizing the MWT processes, which opens the horizon of in-situ mutual collaboration of IPT related users.\\n\\n2.3 The Complete AR Approach with Evaluation for IPT\\n\\nAs a representative human computer interaction technology, AR consolidates the interplay between digital objects and human subjects with much immersion based on a powerful and interactive AR visualization modality [39–41]. Such usage has still not become mainstream in most industries but a moderate number of re- search projects have successfully demonstrated its value in some cases [19]. With the rapid development of hardware technology, head-mounted display (HMD) AR has become widespread in many contexts since it enables interaction between dig- ital information and human hands [42]. Under this circumstance, manipulation tasks that require interaction with the virtual objects can be achieved more ac- curately and quickly than when using conventional tangible interfaces [43, 44]. Optical see-through (OST) HMDs occupy the majority of contemporary AR con- texts due to their ability to use optical elements to superimpose the real world with virtual augmentation [45]. This differs from video see-though AR where OST AR does not bear a component to process captured camera imagery [46]. With the ability of displaying manifold data in a more perceivable and interactive way [47, 48], applying AR to generate interactive visualizations for user-oriented tasks is often the preferred choice, and not only for industrial applications [49–53]. More specifically, it has been proven that OST AR can insert the virtual information into the physical environment with visual enhancement for human observers to control, thus improving the performance of information-dense domain tasks and further adding to its appeal to industrial practitioners [54, 55].\\n\\nAfter the framework conceptualization and initial mobile AR realization, we propose the complete AR approach with practical user evaluation which (Figure 2.3.b and 2.3.c) allows users to easily observe and manipulate the IPT visualiza-\\n\\n(c) THE (a) USER INDUSTRIAL or AR wail PROCESS DATA OST HMD TOMOGRAPHY VISUALIZATION\\n\\nFigure 2.3: Conceptualization of our proposed AR approach for IPT visualization analysis. a): The specialized IPT controlled industrial process is implemented in b): The IPT data visualizations originate from different a confined environment. processes. c): Users engage with relevant data analysis. d): User equipped with proposed AR environment by an OST HMD interacting with the visualizations for further analysis.\\n\\ntions with high immersion [56]. We propose deploying AR/MR applications to tackle the practical problems stemming from the specific area (interactive IPT visualization analysis). The main advantage of the proposed methodology is that it initiates, to our knowledge, the first mechanism for furnishing IPT users with immersive OST HMD AR with comparative evaluation to communicate with in- formative visualizations, using their hands for better comprehension of the des- ignated industrial processes. The AR system employs Microsoft HoloLens 2, one of the representative OST HMDs (Figure 2.3.d), as the fundamental supplying equipment to create the AR/MR environments. The source data derived from the IPT supported industrial processes is always formidable to understand, so it needs straightforward and precise patterns to be visualized and interpreted. Our proposed approach provides a systematic framework which adopts accurate 2D/3D IPT data visualizations that are further used for interactive analysis to provide better understandability for domain users. We carried out a comparative study to demonstrate the superiority of our AR approach on bringing interactive visualization surroundings as well as eliciting better contextual awareness. We en- vision our AR approach to benefit areas where users deal with IPT visualization analysis.\\n\\n2.4 Supporting AR Visual Search Tasks\\n\\nApart from the industrial domain, especially IPT, AR is endorsed and applied in facilitating task performance due to its capability of providing real-time interac- tion, as well as generating interactive interfaces of visualized digital content [57,\\n\\n9\\n\\n10\\n\\n2 Background\\n\\n58]. Gaze assistance, in particular, has a promising role in AR applications ow- ing to its easy, natural, and fast way of involving people in virtual environments [59]. Gaze-assisted techniques and implementations have been demonstrated to efficiently support AR applications by providing better user experiences (UX) [60] and more accurate target selection techniques [61]. People’s eye movements are easy to track, and gaze implicitly conveys what people are interested in. Thus, eye-tracking approaches are becoming more pervasive in interactive AR devices. The search task in AR is a widespread research area which has received consid- erable attention, while its representations can be diverse, including visual element - [62], real world object - [63], or general searching [64, 65]. As a means to make the search process more accessible, it is even possible that HMD devices with the most advanced AR technology could soon become as ubiquitous as smartphones [66].\\n\\nWhen performing particular tasks in the context of AR, hints can improve the ultimate task performance [67]. The type of hint, acting as a central and core tool, can vary. A widely-used tool is the visual hint, which is a mixture of graph- ical representations in AR with user interface (UI) actions associated with the physical world [68]. This effectively provides spatial and temporal guidance in AR. Another well-adopted hint is the audio hint, which presented as voice com- mands or directives can swiftly and clearly aid the user. A number of researchers have exploited audio hints for the purpose of informing, guiding, and directing within their designs [69, 70]. However, even though there is a substantial body of research into how visual and audio hints work in AR tasks, little has focused on search tasks other than exploring the combination of visual and audio hints in one AR framework.\\n\\nTask feedback also matters. At present, there is almost no substantial research into this area. Task feedback is crucial for domain users to obtain better task performance in many situations related to AR [71, 72]. Here, we explore the effect of task feedback which is embodied as the playback of the human eye gaze recorded during the searching process. A vision-friendly colored dot smoothly following the path is used to display the eye trajectory (Figure 2.3.b). According to Vieira et [73], feedback has the potential to provide more specific information needed al. by the users in AR, which encouraged us to explore its potential influence in the same context.\\n\\nIn this stage, a complete AR approach for search tasks is proposed, examining the independent and the combined impact of visual and audio hints, and studying instant post-task feedback through gaze assistance [74]. We specifically concen- trated on a book-searching task case, which includes all the essential components of a visual search procedure: visual environment, a particular object (the target book), and distractors (irrelevant books) [75].\\n\\n3 Related Work\\n\\nThis chapter describes the relevant research related to the main work of this thesis. Specifically, it emphasizes the research status of harnessing AR/MR into IPT and how AR can affect practical search task performance.\\n\\n3.1 AR Visualization with IPT\\n\\n3.1.1 Industrial AR\\n\\nThere is a body of research to reflect the growing interest of the deployment of AR in various industrial applications. Cardoso et al. [19] wrote a systematic survey paper revealing the present status of industrial AR, where they concluded that most AR applications in the industry domain are incremental as they of- fer a different and more efficient way to convey information needed. Bruno et al. presented an AR application where industrial engineering data was visually superimposed on the real object that represents a spatial reference, where the exploration is more natural compared to traditional visualization software [76]. A recently published paper by Noghabaei et al. [77] indicated that the utilization of AR in architecture, engineering, and the construction industry had a significant increase from 2017 to 2018. More specifically, according to the powerful visualiza- tion functionality provided by AR, Lamas et al. [28] pointed out that AR could be used to visualize the 2D location of products and tools as well as hidden instal- lation areas in the shipyard industry. Additionally, DePace et al. [29] found that AR can enhance a user’s ability to understand the movement of mobile robots.\\n\\n3.1.2 Visualization for IPT\\n\\nOver a decade ago, Bruno et al. [76] developed an AR application named VTK4AR, featuring that functionality which uses AR to scientifically visualize industrial en- gineering data for potential manipulation of the dataset. The many requirements of applying AR within industrial applications have been summarized by Lorenz et al. [78] as they enumerated the user, technical, and environmental requirements. Mourtzis et al. [52] proposed a methodology to visualize and display industrial production scheduling and data monitoring by using AR, empowering people to supervise and interact with the incorporated components. In industrial manufac- [53], demonstrated that turing, a comparative study conducted by Alves et al.\\n\\n11\\n\\n12\\n\\n3 Related Work\\n\\nusers who replied on AR based visualizations obtained better results in physi- cal and mental demand in production assembly procedures. Even more specif- [79] identified that using in-situ projection based spatial ically, Büttner et al. AR resulted in a significantly shorter task completion time in industrial manual assembly. They designed two assisting systems and proved that the projection based AR outperformed HMD based AR in ease of use and helpfulness in their [80] proposed an adaptive AR system to visualize the in- context. Avalle et al. dustrial robotic faults through the HMD devices. They developed an adaptive modality where virtual metaphors were used for evoking robot faults, obtaining high effectiveness in empowering users to recognize contextual faults in less time. Satkowski et al. [81] investigated the influence of the physical environments on the 2D visualizations in AR, suggesting that the real world does not have a noticeable impact on the perception of AR visualizations.\\n\\nEven though the volume of intersection research between AR and IPT is not substantial, other researchers have been investigating diverse pipelines of applying AR to generate interactive and effective visualizations for complex tomographic process data. Dating back to 2001, Mann et al. [82] exploited an AR application to visualize the solid-fluid mixture in a 5D way in stirred chemical reactors operated by one breed of IPT–electrical resistivity tomography (ERT). Recently, a new AR solution to visualize IPT data and further lead to collaborative analysis in remote locations was proposed by Nowak et al. [16]. More interestingly, their team also explored a more in-depth AR system with a more advanced prototype which created an entire 3D environment for users to interact with the information visualizations characterizing the workflow of IPT with high immersion [58]. The OST HMD AR was satisfactorily adopted in every experiment they conducted. Furthermore, Zhang et al. formulated a new system to generalize IPT within the context of AR, and directed a study where a novel AR framework was developed to support volumetric visualization of IPT, especially to yield high mobility for users [83].\\n\\n3.2 AR for Visual Search Tasks\\n\\nSearch tasks are becoming more common in using AR because of the capability of projecting additional virtual information onto the physical environment. Con- treras et al. [63] presented a mobile application with AR encapsulated to enable users to search for desired places, people, or events on a university campus. The superiority of AR lay in the fact that it offered certain visual elements that helped users to better locate the required result. Rafiq and his colleagues [84] proposed a dynamic AR framework to support an online book-searching task by using mobile augmented data. This framework also introduced a security layer which ensured [85] utilized gaze move- the protection of sensitive cloud data. Gebhardt et al. ment data to observe the MR object’s label in a visual searching process through\\n\\na reinforcement learning method. Trepkowski et al. [86] presented a series of simulating experiments to investigate how visual search performance is affected by the field of view and information density in AR, indicating that the significant effect was aroused by these two factors. Van Dam et al. [87] studied the cues in a drone-based AR signal detection task but found no significant differences across AR cue types. Nevertheless, the effect of hints and task feedback continues to be more valued in AR visual searching.\\n\\n3.2.1 AR with gaze assistance\\n\\nSome research has already pioneered gaze-assisted UI to access more contextual information [88–91]. It has been shown that the optical see-through HMD which harnesses human gaze with eye-tracking as the interaction metaphor can con- tribute to efficient results [92]. Interestingly, there are some studies exploring eye- tracking to present menus to aircraft pilots, and to adjust their contents based [94] proposed a on what the pilot is looking at [93]. novel conceptualized system adding eye-tracking capabilities to a Head-Mounted Projection Display (HMPD), which was satisfyingly performed from a low-level optical configuration. Three years later, Park et al. [59] pioneered a system which includes a Wearable Augmented Reality System (WARS) to examine their propo- sition on an experiment in which they selected desirable items in an AR gallery with content mobility. Rivu et al. [66] successfully demonstrated the superiority of eye-tracking, showing that users are more positive in concentrating on their ongoing conversations when there is more gaze interactivity within AR environ- ments. All the relevant research proved that the gaze assistance in AR activates improved perception.\\n\\n3.2.2 AR with hints and feedback\\n\\nNumerous researchers have endeavored to bring different modalities of hints into AR/XR systems as auxiliary tools for reaching desired results. Of these, visual hints [68, 95–98] and audio hints [69, 97, 99–101] are the two most common repre- sentations used. Arboleda et al. [102] presented augmented visual hints in a robot- involved AR system which aimed to enhance the visual space of the robot operator about the position of the robot gripper in the workspace, where the visual hints were used to improve distance perception and then the manipulation and grasping task performance. For tangible AR, White et al. [68] examined visual hints to enable discovery, learning, and completion of gestural interaction in a tangible environment. Seven visual hint types were generated: text, diagram, ghost, ani- mation, ghost+animation, ghost+text, and ghost+text+animation. Two decades [70] harnessed audio information to keep users of wearable ago, Sawhney et al. devices updated with incoming messages and events. Lyons et al. [69] developed\\n\\n13\\n\\n14\\n\\n3 Related Work\\n\\nan AR game system named \"Guided by Voices\", which equipped the user with a narrative sound clip that indicated the scenarios encountered and the correct steps to be taken for proceeding. Interestingly and more recently, Mulloni et al. [103] found that AR can be integrated with a mobile navigation system, while audio hints can be advisable for eye-free usage. Cidota et al. [104] compared the effects of automatic visual and audio notifications regarding workspace awareness in AR remote collaboration.\\n\\nAlthough task feedback evaluation in AR has received little attention [105, 106] in the past years, a recent study carried out by Liu et al. [107] evaluated the influence of real-time task feedback in mobile handheld AR, which suggested that significant benefits will emerge if task feedback is engaged during the AR task. Zahiri et al. [108] studied real-time feedback in AR for supporting a surgical train- ing, finding that most of the users preferred receiving the feedback while the task was being performed. Murakami et al. [109] found that haptic feedback as task evaluation can help users perform more effectively in a wearable AR system for virtual assembly tasks. Clemente et al. [72] demonstrated that continuous visual AR feedback can deliver effective information for the users in their sensorimotor control with a robotic hand, which has implications for amputee assistance in a clinical scenario. Even though some researchers have identified the effects of in- stant post-task feedback in gaze-interaction embodied virtual reality (VR) [110], its combination with AR is still scarce.\\n\\n4 Problem Solving\\n\\nThis chapter gives a detailed description of the work done originated from the annexed papers, including the methodologies, experiment designs, and results obtained. As mentioned before, the main contribution of this thesis is first to bring the AR technique to the specialized IPT-controlled processes from zero to one, and then investigate the impact of AR in practical search task performing. For need-finding, we designed a survey study and conducted a qualitative analysis of the results. During the latter AR framework development and mobile AR implementation, the advantages of the AR method were identified and followed by the complete AR approach by OST HMD implementation with a within-subject user evaluation. For the practical search task, two modalities of AR hints and gaze-assisted task feedback were deployed within the OST HMD.\\n\\n4.1 Need-finding: Is AR Ready for IPT?\\n\\nPaper I aimed to identify the gap between AR and its successful deployment. More specifically, we sought to obtain the link between interactive visualization of AR and the effective working performance of the experts.\\n\\n4.1.1 Method\\n\\nTo implement this proposition, we designed a survey study which included a number of objective questions and several subjective narration parts (25 questions in total), divided into three sections: a general demographic section which included five questions (age, gender, country of residence, place of work - industry or academia, and size of organization); a visualization in tomography section with twelve questions, (topics including years of experience, modalities of IPT and visualization techniques used in work along with their relevance and importance, and problems regarding visualization during daily work); and a final part regarding AR with eight questions, (topics including knowledge and experience with AR, recently involved projects, and possible usage with potential advantages and drawbacks of AR in the industrial tomography domain). In the final section, the three AR settings: HMD, HHD, and SAR were presented to the participants. All collected responses were anonymous.\\n\\nWe invited 14 participants from our Europe-wide project (https://www.tomocon. eu/) network to complete the survey. The working statuses of the participants\\n\\n15\\n\\n16\\n\\n4 Problem Solving\\n\\nSAR 7.14%, 7.14%, Not aware at all 7.14% SAR 7.14%. Very aware 14.29% as 14.29% ‘ae HMD 35.71% HHD 42.86% N ‘Neutral 21.43% HHD 50.00%\" ‘ Not aware 42.86%\\n\\nFigure 4.1: The answers of how domain experts responded to the current AR status. A: Q: Comfort: Which AR scenario shown above looks most comfortable for you? B: Q: Usability: Which AR scenario shown above looks most usable for you (might help with your work)? C: Q: Are you aware of any ongoing AR applications in industrial tomography?\\n\\nwere categorized into working in academia with tight collaboration of industry, working in industry, and working in both academia and industry. All were ei- ther experienced researchers or industrial practitioners distributed across Europe. Furthermore, all had distinguished skills in industrial tomography for at least four years while some were outstanding researchers in this domain. To mitigate un- desired biases as well as to make our study more robust, we carried out a small pilot study followed by the formal study.\\n\\n4.1.2 Results\\n\\nRegarding the potential use of AR, even though most domain experts engaged in our study did not have much hands-on experience of working with AR, they still held a positive attitude regarding its potential usage in the context of industrial tomography after being exposed to the three AR modalities. They described the envisioned applications of AR from their own perspectives. We summarized their answers according to three aspects.\\n\\nHigh accessibility. The majority of participants acknowledged the superior accessibility of AR for aiding domain experts. According to one expert involved in the study, AR, as a mobile tool, can make people concentrate on the targets more directly: \"It is very useful to have relevant information directly at the object of interest.\". The accessible AR supports experts in different manners.\\n\\nSee-through capability. Simultaneously, these experts spotted the huge po- tential of utilizing AR within industrial tomographic processes. AR might become one effective tool to support task resolution. One expert came up with a comment regarding specific application cases: \"Especially if you deal with the case where there are opaque fluids which you cannot see, AR could be used as a powerful tool for interactive visualization.\".\\n\\n3D visualization ability. Another significant potential of adopting AR is its 3D visualization ability based on a few participants. The interactive 3D visu-\\n\\nalization was seen to help with more precise task analytics.\\n\\nFrom the responses obtained from these experienced domain experts, we looked into deploying the capability of AR in industrial tomography. The participants who were in favor of AR-tomography interplay claimed specific applications in terms of their hands-on projects. Most of them believed that AR could be unex- pectedly beneficial for domain tasks pertaining to interactive visualization.\\n\\nFor the challenges to apply AR in industrial tomography, although most partic- ipants gave positive responses regarding its potential, we have to realize that AR is still not prevalent in most industries [19]. To uncover how to apply AR appro- priately in industrial tomography as well as to understand its usability, we probed the latent challenges according to the participants’ descriptions. These findings should be fully explored to situate AR properly within industrial tomography.\\n\\nEase of use. The most common challenge they posed was ease of use. Through the survey, participants gained a fundamental conception of AR but mostly questioned its ease of use. For example, some experts were reluctant to wear HMD AR for long-time inspection of a specialized process which would cre- ate unacceptable strain on human bodies: \"Yes, I think the headset is too difficult to wear for the industrial processes which have longer duration.\"; In many large reactors, the projection is possible. But for AR using head mounted display or other devices, it is difficult for hands-on operations and long time use.\".\\n\\nSpeed and accuracy. Participants suggested that the speed and accuracy of AR may become a core problem in many application cases. One expert posed a more concrete appraisal: \"Industrial tomographic processes are often faster than human interaction and need automated control.\". It is challenging for AR to make human interaction achieve the pace of industrial processes or to properly visualize vital information in the form of virtual elements.\\n\\nConvertibility. AR is still rarely used in industrial tomography due to its unestablished reputation for suitably converting domain-related data into desir- able visualizations. One researcher dealing with massive hands-on tomographic tasks remarked: \"From my point of view, converting the output from the tomo- graphic sensor to properly represent output (i.e., moisture distribution) by AR can be challenging.\". We have to realize that most domain experts lack the special- ized knowledge of using AR devices, which is indispensable when adopting AR for visualization.\\n\\n4.2 AR Framework and Mobile AR Development\\n\\nPaper II and paper III concentrated on the theoretical AR framework conceptu- alization and actual mobile AR implementation based on the insights obtained from the need-finding study. Given that the IPT-controlled process in these two papers was an MWT for microwave drying which includes three phases–time- reversal imaging, post-imaging segmentation, and volumetric visualization, our\\n\\n17\\n\\n18\\n\\n4 Problem Solving\\n\\nFeedback Observation\\n\\nFigure 4.2: The conceptualization framework of the proposed AR system is: the MWT-controlled industrial microwave drying process block, whichcovers the whole running procedure of the process and provides data and elements used for analysis; users’ block, representing the people involved in this context, such as operators who control and run the process and researchers who observe and analyze the process; the AR app block, which entails the AR implementation of a mobile AR app developed on the iOS/Android platform at the initial stage. [83, 111].\\n\\nmobile AR app has incorporated the data from all of the three phases.\\n\\n4.2.1 Method\\n\\nTo better support the onsite analysis related to the informative data processing and visualizing workflow, we present an AR framework which enhances the in- teractivity and immediacy of IPT data analysis. The proposed system has three main components, as displayed in Figure 4.2.\\n\\n• MWT-controlled industrial microwave drying process: A unique heating and drying process operated in a confined chamber with sophisticated industrial settings. In our study, the target is a microwave drying process for polymer foams undergone by the precise HEPHAISTOS [17] microwave oven system.\\n\\n• Users: Operators who control and run the drying equipment or researchers who take onsite observations and collect data for further analysis.\\n\\n• AR app: The core part of our proposed system. As a preliminary stage, the application is manifested as a mobile app run in iOS/Android mobile devices and used for interactive and collaborative volumetric visualization and analysis. A multiple floating interface provided and overlaid in a real environment and containing information from our proposed data workflow is the main component of the application.\\n\\nThe principal part of the initiated AR system is a mobile app developed to visu- alize the necessary information revealed from the aforementioned data processing and visualizing pipeline, while it supports interactive and collaborative onsite data analysis. This app serves as an intermediary tool between the users and the precise experimental equipment used for displacing the visualized information regarding the ongoing process. The users, for example, operators, can interact with the visualizations projected on the AR interface, with the mobility of merely holding a mobile device alongside the bulky equipment. As Figure 4.3 illustrates, people from different communities, such as practical operators or academic researchers, have the capacity to interactively and instantly observe the information related to the process by simply activating the app without initiating other facilities. The app was developed via Unity and Vuforia engines, aiming at the present stage for iOS/Android portable devices such as smartphones and tablets. The app it- self provides on-site multiple interfaces containing various visualizations obtained from the data pipeline. Users are able to switch to the visualizations they require for the desired analyses. As shown in Figure 4.3, some different floating interfaces incorporating different modalities of visualizations are presented and supported by switching after activating the app. The mobility of the AR interface provides users with ongoing visualizations as they move around the equipment that runs the designated process.\\n\\n4.2.2 Results\\n\\nThe first concern was how to present a complete data processing pipeline with informative visualizations where these modalities are incorporated by the second focus – a preliminary AR framework to facilitate interactive and collaborative onsite data analysis regarding volumetric visualization in IPT systems. As a pioneer study in this related context, our proposed AR system opens a new horizon of deploying this interactive technique within IPT, boosting the complicated data processing and visualizing with an MWT-controlled microwave drying process. Based on our study, we present the following findings:\\n\\nInteractivity. The most prominent advantage of this system is that the in- formation visualization by the AR interface is completely interactive. By using\\n\\n19\\n\\n20\\n\\n4 Problem Solving\\n\\n\\n\\nFigure 4.3: The realization of the proposed AR system. Users hold the handheld smartphones/tablets to run the AR app alongside the industrial setting. After the app activation, users can then observe the visualizations displayed on the AR interfaces. At present, the app is supported via iOS/Android mobile devices. Users are empowered to switch to different visualizations regarding different phases of the process. The visualizations shown on the interfaces are multimodal, including the infrared image showing the condition of the process and the 3D/2D reconstruction of the MWT images (the upper left small figure on the floating interface in the upper right subfigure); the reconstructed MWT images with their segmented results to show the high moisture areas (the middle right subfigure); and the volumetric visualization (the bottom right subfigure) [111].\\n\\nthe AR app after activation, a user can easily get instant access to the visualized data by observing the interface which incorporates the information needed for decision-making. Our AR system provides a new perspective of interactive data analysis to benefit IPT related users.\\n\\nMobility. Another strength of this system is its mobility. Generally, control- ling and supervising an IPT related process requires implementation in specific and precise equipment and scientific laboratories. Researchers who observe the data and analyze the ongoing process invariably conduct this work in dedicated computers located at different places. However, with this system they are empow- ered to hold a portable mobile device (a smartphone or tablet) with a running\\n\\napp to closely implement the data analysis while standing next to the precise equipment.\\n\\nInformation richness. The AR interface is used for displaying the most sig- nificant information needed for boosting the decision-making of the industrial process. We report the first AR application for bringing a complete data pro- cessing and visualizing workflow related to complex IPT process which contains the key information used for further analysis. For instance, a generated MWT image using a time-reversal technique is able to show the precise moisture distri- bution, while the up-to-date volumetric visualization has the capability to reveal the parameters of the drying material.\\n\\nMutual collaboration. Our system enables in-situ collaboration by provid- ing a number of virtual AR interfaces comprising different information, includ- ing all the sophisticated visualizations emerging from the workflow. Under this setting, users from different communities such as hands-on operators, academic researchers, and engineers, can independently conduct visualization analysis by observing the corresponding visualizations, and interact with each other on com- mon problems as well.\\n\\n4.3 The Complete AR Approach\\n\\nIt is critical for users who intend to deal with IPT related analysis, regardless of their expertise, to fully understand the contextual information for in-depth visualization analysis. Paper IV advanced the whole AR pipeline and proposed a complete AR approach by the OST HMD for interacting with IPT data and tackling visualization analysis. A comparative user study was also implemented for evaluation.\\n\\n4.3.1 Method\\n\\nThe contextual data used in this study was acquired from another microwave drying process for polymer materials monitored by MWT. This imaging modal- ity was applied to detect the moisture levels and distribution of the polymer material through specific tomography imaging algorithms [112]. Three different visualizations were employed in this study from dissimilar MWT drying processes, including two in 3D and one in 2D; as 3D figures incorporate more information regarding the polymer materials used in the process. Different moisture levels were rendered with distinct colors and marked with different letters, denoting low moisture area, moderate moisture, the dry part, and the high moisture area re- spectively. The annotations were created based on the physical understanding by a domain expert.\\n\\nA user study was conducted to find out whether our proposed AR approach, offering interactive and immersive experience for communicating with IPT visual-\\n\\n21\\n\\n22\\n\\n4 Problem Solving\\n\\nizations, was superior to customary in-situ data analysis pertaining to supporting users’ understandability and user experience. The main realization of this study was two different scenarios where the engaged participants encountered two dif- ferent environments; our proposed AR approach to interact with the data rep- resentations with the aid of OST HMD; and the conventional setting using an ordinary computer with 2D screen and Matlab (only the visualization window was utilized for the experiment). Three tasks were designed towards IPT con- textual visualization analysis for the participants to implement. We selected the within-subjects principle with the counterbalancing design principle [113]. The following hypotheses were made:\\n\\n• H1: Our proposed AR approach can obtain better understandability for users compared to using the conventional setting.\\n\\n• H2: Our proposed AR approach can contribute to a lower task completion time and fewer task errors.\\n\\n• H3: Our proposed AR approach has better usability for users to interact with IPT data visualizations.\\n\\n• H4: Our proposed AR approach has a greater recommendation level than the conventional setting.\\n\\nTwenty participants (12 male, 8 female) aged between 24 and 41 (M = 30.1, SD = 4.78) were recruited by e-mail and personal invitations at a local university. All participants signed an informed consent at the beginning of each session, being aware of that there would be no personal information collected and that they could quit the study at any time. They were also told that the sessions would be audio- recorded, but all recordings would be treated confidentially and references would only be made in a purely anonymized form for scientific analysis. The participants had sufficient time to read the consent form and ask questions before signing it. Due to the complexity of IPT, the data generated is usually difficult to interpret by outsiders, even some of the experts. Hence, we started our study with a concise but detailed introduction about the study background, including the fundamental schematic of IPT, the source of the data visualizations, and the basic information about the visualizations used in this study. Before we began with the actual study sessions, we calibrated the Microsoft HoloLens glasses for each participant by helping them follow the instructions from the built-in calibration functional module. A short pre-training session was implemented to get the participants familiarized with and adapted to the HMD and the AR application used. The entire procedure of the user study is displayed in Figure 4.4.\\n\\nRandomized baseline group (n=10) \\\\ \\\\| Fill in the igil A short Eligible ; c participants J post-study |_| interview (n=20) h questionnaires session / (n=20) (n=20) Randomized AR group (n=10) / a pew\\n\\nFigure 4.4: Flowchart of our user study with counterbalancing design.\\n\\n4.3.2 Results\\n\\nQualitative Results: We compiled the results collected from the post inter- view sessions and real-time feedback of the participants during the study. The qualitative measurements generalized by a thematic analysis are presented in this section. The real-time feedback was conveyed spontaneously by the participants and recorded by the authors. For the post study interview, we asked the partic- ipants several subjective questions regarding what they liked and disliked about the tasks regarding our AR approach as well as the comparison between the two tested environmental settings. Concerning privacy and anonymity, we denoted the participants as P1 to P16 when recording the real-time feedback and encod- ing the interview answers. Based on the codes, we derived three themes: in-task understandability, interaction-based usability, and user experience.\\n\\nQuantitative Results: In the following we present our quantitative measure- ments in relation to our hypotheses. The independent variables were identified as the two different environmental settings. From the study process and post- study questionnaire, five metrics – TCT, understandablity, error rate, usability, and recommendation level – were employed as dependent variables. The TCT was merely measured from Understand-n-Select since this task is representative for practical IPT visualization analysis. To test the understandability, a 7-point Likert scale was adopted for participants to rate the level of the two environmental settings helping with understanding complex IPT data. The error was exclusively collected from the third task when users were supposed to select the designated areas. The largest number of errors was four in each environmental setting. Every error, if one occurred, was noted during the study and the error rate was thereby calculated by the authors. The system usability scale (SUS) [114] was harnessed to quantify the usability. We uniquely investigated the recommendation level by asking the participants the extent (the 7-point Likert scale) of recommending the two environmental settings. We statistically analyzed the collected results to iden- tify any significance among the four metrics evaluated. Normality was checked as a prerequisite.\\n\\n23\\n\\n24\\n\\n4 Problem Solving\\n\\np= 0022 p<0.001 Understandability Usability Fs £ 7 Recommendation Level & oo 09 GAR Approach Conventions Setting I Appreech _ Conventoal Sting AR Approach Convention uscsing — d) AR Approach Conventions Seting AR Approach Coaveatons Setting\\n\\nFigure 4.5: Statistical results between the AR approach and the conventional set- ting. a: The distribution of the understandability levels (1 to 7) rated by participants between the AR approach and the conventional setting. b: The distribution of task completion time (s). c: The distribution of the error rate (0.00 to 1.00). d: The distribution of the SUS scores representing usability (0 to 100). e: The distribution of the recommendation levels (1 to 7).\\n\\nWe conducted corresponding statistical analyses for those five metrics and sig- nificant differences were found in each of them as shown in Figure 4.5. Thus, we are confident to conclude that the four hypotheses have been well verified by the results.\\n\\n4.4 Practical Visual Search Tasks\\n\\nIn paper V, to investigate how visaul/audio hints and task feedback of AR can make a difference in facilitating the task performing for visual search tasks, we hypothesized:\\n\\n• H5: Both visual and audio hints have a positive effect in facilitating AR searching performance and decreasing cognitive workload in AR, while the combination of these two hints has a greater effect than either does individ- ually.\\n\\n• H6: Instant post-task feedback has a positive effect for task performance and cognitive workload reduction in AR search tasks.\\n\\n4.4.1 Method\\n\\nOur proposed system, featuring an AR app used in OST HMD, is an interactive tool for users carrying out a book-searching task. The equipment kit consists of an OST HMD with built-in eye tracker which is responsible for recording the user’s gaze during the task being conducted. There are two inter-correlated modules in our proposed system: the hints module and the instant post-task feedback module. The first module was designed to address H5, while the second targeted H6. The components – \"User\", \"The AR app\", and \"Task completion\" are commonly owned\\n\\n120.00- ———— = = 120.00 —————] a ———_ [p<0.007 100.00- — 100.00 p<0.001 | p<0.001 ae Se ! | : a.001 80.00 Beto leereacat 80.00 p< ihe <0.001 <0.001 p<0.001 p< ee! P ps0 60.00 p=0.006 40.00 | 40.00- P= 9.006 0.00- 0.00 First Task Second Task First Task Second Task a) D Hil contoi Group [| Aucio Group [lll Visual Group fl Combined Group Hi contot Group [il Ausio Group fll Visual Group [lll Combined Group\\n\\nFigure 4.6: Study I (a) and Study II (b): The mean TCT (s) used per task by the four groups. Error bars show mean ± standard error (SE).\\n\\nby the two modules. The AR app is the core of the system which bears the necessary information needed including hints and task feedback.\\n\\nThe book-searching case was chosen and implemented with the aid of the pro- posed AR approach. The AR app encoded a number of book titles which were used as the stimulus in the study. The user with an OST HMD with a real-time built in eye-tracker stood in front of the bookcase, where the task was to find a specific but randomly determined book (Figure 2.3.a) after activating the app. We chose arrows as the indicators, but showed them in an easily-recognizable color and in a static state. Since all virtual artifacts were displayed in the world space, users’ movements did not affect an object’s position. As shown in Figure 2.3.c, the bright-purple arrow visually helped the user to locate the target book. For audio hints, users received an audio instruction about the approximate lo- cation of the targeted book. The auditory information was presented as a clear and simple instructive voice message, allowing the users to finish the task with higher efficiency. The instant post-task feedback was provided as a visible play- back of gaze recording. As shown in Figure 2.3.b, the user’s gaze was denoted as a brightly colored dot following the trajectory. During the book-searching phase, gaze trajectories were marked and recorded. After one search task, users were able to watch their gaze playback of the trajectory, after which they proceeded to the next task.\\n\\nA comprehensive comparative user study based on a between-subject with cou- pling within-subject factors was then designed. The independent variables were the tasks (within-subject) and the groups (between-subject). The study was com- prised of two sub-studies (Study I and II) where the participants from Study II received the task feedback while those from Study I did not. Before starting the experiment, we conducted a pre-testing session where several people were invited to test the desired functionalities of the AR app. Some minor adjustments were then made to improve the visual and locational clarity, including the visual hint made into a more vision-friendly arrow and our testing environment moved to a more spacious and bright function room. The participants (n=96) were randomly sorted into four different groups in both Study I and II (n=48 in each): control,\\n\\n25\\n\\n26\\n\\n4 Problem Solving\\n\\n10000 90.00 80.00 , 70.00 60.00 50,00. 30.00: 20.00 10.00 0.09: Mental Demand Temporal Demand Performance Frustration Total Works I Control Group Ill Audio Group ll Visual Group Ill Combined Group\\n\\nFigure 4.7: Study I: NASA TLX for the first task by the four groups. Error bars show mean ± standard error (SE).\\n\\n100.00 90.00 80.00: 70.00 60.00: 50.00 : hil Mental Demand Pedormance Etfort Total Workload I Control Group Ill Ausio Group Ill Visual Group Ill Combined Group\\n\\nFigure 4.8: Study I: NASA TLX for the second task by the four groups. Error bars show mean ± standard error (SE).\\n\\naudio, visual, and the combined groups (n=12 in each).\\n\\n4.4.2 Results\\n\\nThe two metrics (dependent variables) employed for assessing the tasks were task completion time (TCT) and NASA Task Load Index (TLX) in both studies.\\n\\nTCT in Study I: To identify the statistically significant differences of the results, a 2(task)*4(group) mixed analysis of variance (ANOVA)(p = 0.05) was performed. This analysis was performed using IBM SPSS Statistics, as were the following statistical measurements. Bonferroni-corrected post hoc tests were employed to determine if the pairwise groups were significantly different. We found that all participants did not have statistically significant shorter TCT in the second task compared to the first, but there were significant main effects of the four groups on the TCT measured. No significant interaction was found between the tasks and the groups. The Bonferroni post hoc tests showed statistical significance between every pairwise comparison of the control group with all the other groups in both tasks.\\n\\nNASA TLX in Study I: The study showed that all participants did not have a significantly different cognitive workload in the second task compared to\\n\\n100.00 20.00 20.00 70.00 60.00 Mental Demand sical Demani femporai Demand == éerformance Total Workload Control Group I Aucio Group Il Visual Group Ill Combines Group\\n\\nFigure 4.9: Study II: NASA TLX for the first task by the four groups. Error bars show mean ± standard error (SE).\\n\\n90,00. 80,00 70,00. 60,00: = Man I. lis Mental Demand wand Temporal Dem I Conto! Group Ill Ausio Group ll Visual Group Ill Combined Group\\n\\nFigure 4.10: Study II: NASA TLX for the second task by the four groups. Error bars show mean ± standard error (SE).\\n\\nthe first. The post hoc test showed all three groups were significantly different In the second task, the when compared to the control group in the first task. control group had statistical significance between the other three groups. Again, no significant differences between every pairwise comparison of audio, visual, and combined groups were found in both tasks in Study I.\\n\\nTCT in Study II: All participants had statistically significantly shorter TCTs in the second task compared to the first. The results also showed significant main effects of the four groups on the measured TCT. In addition, there was a significant interaction between the tasks and the groups. The Bonferroni post hoc tests showed statistically significant differences between every pairwise comparison of the control group with all the other groups in the first task. Likewise, the second task also showed significant differences between the control group and the other three groups. However, we found that the pairs of the audio-combined and the visual-combined had statistical significance as well. The TCT of the combined group was statistically different from the other three groups in the second task.\\n\\nNASA TLX in Study II: All participants had a statistically significantly lesser cognitive workload in the second task compared to the first. There were significant main effects of the NASA TLX results among the four groups, but no significant difference was found on the interaction between the tasks and the\\n\\n27\\n\\n28\\n\\n4 Problem Solving\\n\\ngroups. For pairwise comparison, the post hoc test showed no significant differ- ences between every pairwise comparison of audio, visual, and combined groups in the first task. However, all the other three groups were significantly different when compared to the control group. In the second task, the participants from the control group still possessed significantly more cognitive workload compared to members of the other three. Nonetheless, the pairs of the audio-combined and the visual-combined also revealed significant differences. The result means the synthesis of the two modalities of the hints we tested worked better than when there was only one modal hint in the second task, where the gaze playback was provided as the task feedback in the same context.\\n\\n5 Discussions\\n\\nIn this chapter, the insights obtained with the limitation identified regarding the four stages are described.\\n\\n5.1 Need-finding\\n\\nAR has become a renowned concept and technique with the rapid development of mobile smart devices especially the successful usage in industrial applications. Distinct from [77] (specialized in understanding VR/AR in design and construc- tion) and [19] (specialized in evaluating AR in industries), we specifically focused on exploring AR in the domain of industrial tomography to resolve research ques- tion I.\\n\\nWe found that for IPT experts, visualization is a critical inclusion of their daily work, especially 3D visualization which triggered the most difficulties. We sought to dig out more about the prospects, especially in terms of data-driven and ma- chine learning-driven applications [115]. According to our surveyed participants, HMD and HHD AR were considered to be favorable modalities which were able to bring comfort and effectiveness to benefit industrial tomography. Notably, almost all of our participants expressed a positive attitude towards the potential usage of AR due to its promising functionality such as interactivity and providing an extra platform for 3D visualization.\\n\\nAt this stage, there still exist limitations of our study. One limitation is that our study was done exclusively from the perspectives of a specific group of experts in IPT. Furthermore, we surveyed 14 Europe-wide participants, which is only a small sample of the domain practitioners. A larger number of samples would contribute more rigorous and persuasive conclusions. Last but not least, we did not control all the influential factors of the survey, such as the time length of the survey, which might affect the answers obtained.\\n\\n5.2 AR Framework and Mobile Implementation\\n\\nTo tackle research questions II and III, we entered the second stage of this thesis. The first concern of this stage is to present a complete data processing pipeline with informative visualizations, while these modalities are incorporated by the\\n\\n29\\n\\n30\\n\\n5 Discussions\\n\\nsecond concern – a preliminary AR framework to facilitate interactive and col- laborative onsite data analysis regarding volumetric visualization in IPT systems. As a pioneer study in this context, our proposed AR system, especially the de- veloped mobile AR app as the third concern, opens a new horizon of deploying this interactive technique into IPT, boosting the complicated data processing and visualizing with an MWT-controlled microwave drying process.\\n\\nWe noticed that there are some deficiencies in this preliminary AR framework supporting the visualization of data workflow. Foremost, the dataflow itself does not include all the necessary information related to the MWT-controlled process. Moreover, although we have set a precedent for bringing the AR technique to benefit IPT especially in complex visualization analysis, we have not yet evaluated our method through a systematic user study to gain constructive and critical feedback. In addition, our preliminary AR framework is embodied as a mobile app on portable smartphones or tablets. It limits the AR interface to small, bounded screens. The formulation of AR in handheld mobile devices to some extent hinders the essential advantages of AR, which should be tackled by introducing more advanced headsets that can make AR scenes more interactive and immersive.\\n\\n5.3 The Complete AR Approach with Evaluation\\n\\nWe then developed the complete AR approach with OST HMD for IPT data visualizations to fill the gap between AR technique and the IPT context to an- swer research question IV. The practical realization was achieved by an AR app supported in Microsoft HoloLens 2; a typical OST HMD which provides immer- sive interaction for users to communicate with the multiple visualizations. To evaluate our proof-of-concept, a within-subject user study with counterbalancing design was implemented based on 20 recruited participants in order to justify the hypothetical superiority of the proposed AR approach compared to conventional computers with 2D screen for IPT data visualization analysis. Our approach of- fers a complete conceptualization with the realization of an immersive AR method with the aid of OST HMD towards IPT. Our participants were highly favorable towards its interactive features. While no previous work emphasized this, we structured a systematic AR framework with high interaction capabilities for IPT users.\\n\\nAnother important property to be realized is the ubiquity and knowledge trans- ferability of the proposed AR approach. As most of the participants had little experience in IPT, we are therefore encouraged about the potential for our method to bring outsiders to this specialized technique for domain supporting. Addition- ally, although the data used in this study was generated merely from MWT, the diverse genres of IPT have high transferability since they comply with similar mechanisms [83]. It is fair to say that the superiority revealed in our AR approach is highly transferable to different genres of IPT.\\n\\nFor limitations at this stage, the virtual buttons for switching different figures in the AR application were reported as not sensitive enough. Also, although hand gestures in AR space are intuitive and highly close to what people commonly do in physical reality, some participants reported a tiny difficulty when the app was initiated. In addition, we admit that the experiment itself and the contextual IPT data were monotonous and small scale. Last but not least, the stimuli discussed in this paper had only three visualizations derived from different industrial processes supervised by the specific MWT, which lacks diversity. More manifold data from other genres of IPT could be examined to make the results more robust.\\n\\n5.4 Practical Visual Search Tasks\\n\\nThe final stage was intended to respond to research question V. What did we do and achieve in this stage? By sharing the same technique from previous stages and being inspired by it, we identified the effects of visual/audio hints in either single or combined setups, together with instant post-task feedback for AR search tasks. We developed an AR approach engaging two modules of the hints and gaze-assisted task feedback separately with the aid of Microsoft HoloLens 2. A case study for a visual book-searching task was chosen. Then, a comprehensive user study was designed, where 96 participants were allocated in Study I (n=48) and Study II (n=48) and placed randomly into four groups (control (n=12), audio (n=12), visual (n=12), and combined (n=12)) in each sub-study. The user study was based on the between-subject principle with within-subject factor.\\n\\nOur proposed solution has proved to be advantageous for increasing AR search task performance and simultaneously reducing the cognitive workload on users. The noticeable decrease of TCT and the cognitive task workload was a confir- mation of our hypotheses. We also believe the research implication of our study has high generalizability and convertibility to benefit the general AR searching specialization.\\n\\nThe limitations should be admitted as well. First, we designed two visual book-searching tasks to be performed using two bookcases that contained different books, eliminating memory as a factor in the second task. The thickness and order of the books might alter the participants’ search speeds and workload. Also, the sample size of our study might not be sufficient. In addition, due to the performance sequence, the gaze review only influenced the second task in our study. However, in order to better understand the influence of gaze review on search tasks, there could be another design solution with a counterbalancing factor [113] where the gaze playback can be recorded from the second task then used for the first task.\\n\\n31\\n\\n6 Conclusion\\n\\nTo answer the question raised in our thesis title, we are able to say that AR can be used to aid users for tackling domain tasks in both IPT and practice. To bring the AR technique to the specialized IPT field yields a lot of difficulties. As a specialized area, IPT has its particular and traditional mechanisms to deal with domain tasks, where opportunities exist for cutting-edge AR to be imported to fill certain gaps. In this thesis, we developed and implemented usable stages to harness AR within IPT to benefit domain users.\\n\\nFirst, we investigated how IPT experts can effectively work with AR by means of modeling users through a systematic survey. We designed a logically-consistent survey study, recruited 14 domain experts with relevant disciplinary backgrounds, then carried out both a pilot study and a formal study. We displayed the current status of the use of AR in IPT and explored the high potential of adopting AR more in the future owing to the superiority of interactive visualization. Second, we proposed an initial AR framework including a complete data processing and visualizing workflow in an IPT system. The overall realization of this AR frame- work was conducted by a mobile AR App run on smartphones/tablets, aiding a specific MWT controlled microwave drying process. We successfully proved that this novel system brings new conceptualization in this domain by offering interac- tivity and mobility on users’ in-situ data analysis and visualizations. In the final stage of our industrial context, we proposed a complete AR approach with OST HMD for users to immersively interact with the specific IPT data visualizations for contextual understanding and task performing. The increased understandabil- ity, reduced TCT, lower error rate, greater usability, and higher recommendation level of the methodology have been reflected in our work. A within-subject user study demonstrated the superiority of the proposed AR approach over the current standard IPT visualization analytical environment and proved that immersion in 3D AR outperforms conventional 2D screen computers in enhancing contextual understanding and user experience.\\n\\nRegarding practical domain task performance, we have described an AR ap- proach for visual search tasks with supported visual/audio hints and gaze-assisted instant post-task feedback. Based on our hypotheses, we designed and conducted a case study of visual book-searching where the gaze playback acted as instant post-task feedback. The experimental procedure consisted of a comprehensive user study (n=96) with two comparative sub-studies. The resulting analysis was based mainly on collected NASA TLX answers with TCT measurements as preliminary\\n\\n33\\n\\n34\\n\\n6 Conclusion\\n\\nanalytic metrics. We found that both visual and audio hints have a positive effect in facilitating task performance and the combination of the two hints works better than either individually, under the condition that there is instant post-task feed- back. We pointed out the high generalizability and convertibility of our research output in taking advantage of general AR searching processes.\\n\\nFor future work, we will firstly concentrate on upgrading the AR app by im- proving the touch sensitivity of the virtual buttons on the interface as well as eliciting more early guidance for users to better start with the app. In addition, another concern will be to increase the experimental diversity by, for instance, in- volving a larger number of participants from different communities including both experts and outsiders and adding more nonidentical experiments with distinct de- sign principles. Last but not least, integrating AR (HMD or mobile devices) with intelligent UIs manifesting various visualizations or visual analytics regarding the specific IPT processes can broaden the horizon of the relevant stakeholders. Our work benchmarks the intersection between OST HMD AR and the IPT domain to highlight the future direction of bringing more related and advanced techniques into different industrial scenarios. We hope our research will lead to better strate- gic design within this context and bring more interdisciplinary novelty to Industry 4.0 [116].\\n\\nReferences\\n\\nI. Ismail, J. Gamio, S. A. Bukhari, and W. Yang, “Tomography for multi- phase flow measurement in the oil industry”, Flow measurement and in- strumentation 16, 145–155 (2005) (cit. on p. 5).\\n\\n[2] H. Tapp, A. Peyton, E. Kemsley, and R. Wilson, “Chemical engineering applications of electrical process tomography”, Sensors and Actuators B: Chemical 92, 17–24 (2003) (cit. on p. 5).\\n\\n[3] G. Nolet, Seismic tomography: with applications in global seismology and exploration geophysics, Vol. 5 (Springer Science & Business Media, 2012) (cit. on p. 5).\\n\\n[4] K Primrose, “Application of process tomography in nuclear waste process- ing”, in Industrial Tomography (Elsevier, 2015), pp. 713–725 (cit. on p. 5).\\n\\n[5] A. Plaskowski, M. Beck, R Thorn, and T. Dyakowski, Imaging industrial flows: applications of electrical process tomography (CRC Press, 1995) (cit. on p. 5).\\n\\n[6] G. Rao, S. Aghajanian, Y. Zhang, L. Jackowska-Strumiłło, T. Koiranen, and M. Fjeld, “Monitoring and Visualization of Crystallization Processes Using Electrical Resistance Tomography: CaCO3 and Sucrose Crystalliza- tion Case Studies”, Sensors 22, 4431 (2022) (cit. on p. 5).\\n\\n[7] U. Hampel, L. Babout, R. Banasiak, E. Schleicher, M. Soleimani, T. Won- drak, M. Vauhkonen, T. Lähivaara, C. Tan, B. Hoyle, et al., “A review on fast tomographic imaging techniques and their potential application in industrial process Control”, Sensors 22, 2309 (2022) (cit. on p. 5).\\n\\n[8] M. S. Beck et al., Process tomography: principles, techniques and applica- tions (Butterworth-Heinemann, 2012) (cit. on p. 5).\\n\\n[9] J. Yao and M. Takei, “Application of process tomography to multiphase flow measurement in industrial and biomedical fields: A review”, IEEE Sensors Journal 17, 8196–8205 (2017) (cit. on p. 5).\\n\\n[10] Y. Zhang and M. Fjeld, “Condition monitoring for confined industrial pro- cess based on infrared images by using deep neural network and variants”, in Proceedings of the 2020 2nd International Conference on Image, Video and Signal Processing (2020), pp. 99–106 (cit. on pp. 5, 6).\\n\\n35\\n\\n36\\n\\n6 References\\n\\n[11] Y. Zhang and M. Fjeld, “\"I Am Told to Be Happy\": An Exploration of Deep Learning in Affective Colormaps in Industrial Tomography”, in 2021 2nd International Conference on Artificial Intelligence and Information Systems (2021), pp. 1–5 (cit. on p. 5).\\n\\n[12] Y. Zhang, “The Magic of Vision: Understanding What Happens in the Process”, PhD thesis (Chalmers Tekniska Hogskola (Sweden), 2021) (cit. on p. 5).\\n\\n[13] Y. Zhang, M. Fjeld, A. Said, and M. Fratarcangeli, “Task-based Colormap Design Supporting Visual Comprehension in Process Tomography”, in Eu- roVis 2020 - Short Papers, edited by A. Kerren, C. Garth, and G. E. Marai (2020) (cit. on p. 5).\\n\\n[14] Y. Zhang, M. Fjeld, M. Fratarcangeli, A. Said, and S. Zhao, “Affective Colormap Design for Accurate Visual Comprehension in Industrial Tomog- raphy”, Sensors 21, 4766 (2021) (cit. on p. 5).\\n\\n[15] W. Daily, A. Ramirez, A. Binley, and D. LeBrecque, “Electrical resistance tomography”, The Leading Edge 23, 438–442 (2004) (cit. on p. 5).\\n\\n[16] A. Nowak, M. Woźniak, Z. Rowińska, K. Grudzień, and A. Romanowski, “Towards in-situ process tomography data processing using augmented re- ality technology”, in Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers (2019), pp. 168–171 (cit. on pp. 5, 12).\\n\\n[17] Y. Zhang, Y. Ma, A. Omrani, R. Yadav, M. Fjeld, and M. Fratarcangeli, “Automatic image segmentation for microwave tomography (mwt) from implementation to comparative evaluation”, in Proceedings of the 12th In- ternational Symposium on Visual Information Communication and Inter- action (2019), pp. 1–2 (cit. on pp. 6, 18).\\n\\n[18] Y Zhang, Y Ma, A Omrani, R Yadav, and M Fjeld, “Automated microwave tomography (Mwt) image segmentation: State-of-the-art implementation and evaluation”, Journal of WSCG 2020, 126–136 (2020) (cit. on p. 6).\\n\\n[19] L. F. de Souza Cardoso, F. C. M. Q. Mariano, and E. R. Zorzal, “A survey of industrial augmented reality”, Computers & Industrial Engineering 139, 106159 (2020) (cit. on pp. 6–8, 11, 17, 29).\\n\\n[20] R. T. Azuma, “A survey of augmented reality”, Presence: Teleoperators & Virtual Environments 6, 355–385 (1997) (cit. on pp. 6, 7).\\n\\n[21] J. Leebmann, “An augmented reality system for earthquake disaster re- sponse”, International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences 34 (2004) (cit. on p. 6).\\n\\n[22] X. Liu11, Y.-H. Sohn, and D.-W. Park, “Application development with augmented reality technique using Unity 3D and Vuforia”, International Journal of Applied Engineering Research 13, 15068–15071 (2018) (cit. on p. 6).\\n\\n[23] K. Tainaka, Y. Fujimoto, M. Kanbara, H. Kato, A. Moteki, K. Kuraki, K. Osamura, T. Yoshitake, and T. Fukuoka, “Guideline and tool for design- ing an assembly task support system using augmented reality”, in 2020 IEEE International Symposium on Mixed and Augmented Reality (IS- MAR) (IEEE, 2020), pp. 486–497 (cit. on p. 6).\\n\\n[24] K. Pfeuffer, Y. Abdrabou, A. Esteves, R. Rivu, Y. Abdelrahman, S. Meit- ner, A. Saadi, and F. Alt, “ARtention: A design space for gaze-adaptive user interfaces in augmented reality”, Computers & Graphics 95, 1–12 (2021) (cit. on p. 6).\\n\\n[25] R. Piening, K. Pfeuffer, A. Esteves, T. Mittermeier, S. Prange, P. Schröder, and F. Alt, “Looking for Info: Evaluation of Gaze Based Information Re- trieval in Augmented Reality”, in IFIP Conference on Human-Computer Interaction (Springer, 2021), pp. 544–565 (cit. on p. 6).\\n\\n[26] J. Qian, J. Ma, X. Li, B. Attal, H. Lai, J. Tompkin, J. F. Hughes, and J. Huang, “Portal-ble: Intuitive free-hand manipulation in unbounded smartphone- based augmented reality”, in Proceedings of the 32nd Annual ACM Sympo- sium on User Interface Software and Technology (2019), pp. 133–145 (cit. on p. 6).\\n\\n[27] D. Ma, J. Gausemeier, X. Fan, and M. Grafe, Virtual reality & augmented reality in industry (Springer, 2011) (cit. on p. 6).\\n\\n[28] P. Fraga-Lamas, T. M. Fernández-Caramés, Ó. Blanco-Novoa, and M. A. Vilar-Montesinos, “A review on industrial augmented reality systems for the industry 4.0 shipyard”, Ieee Access 6, 13358–13375 (2018) (cit. on pp. 6, 8, 11).\\n\\n[29] F. De Pace, F. Manuri, and A. Sanna, “Augmented reality in industry 4.0”, Am J ComptSci Inform Technol 6, 17 (2018) (cit. on pp. 6, 11).\\n\\n[30] J. Cook, PTC Technology Accelerates Watson-Marlow’s Digital Transfor- mation Plans, (Feb. 2021) https://www.ptc.com/en/blogs/corporate/ ptc-technology-accelerates-watson-marlow-digital-transformation (cit. on p. 7).\\n\\n[31] M. R. Marner, R. T. Smith, J. A. Walsh, and B. H. Thomas, “Spatial user interfaces for large-scale projector-based augmented reality”, IEEE computer graphics and applications 34, 74–82 (2014) (cit. on p. 7).\\n\\n37\\n\\n38\\n\\n6 References\\n\\n[32] Y. Zhang, A. Nowak, A. Romanowski, and M. Fjeld, “On-site or Remote Working?: An Initial Solution on How COVID-19 Pandemic May Impact Augmented Reality Users”, in Proceedings of the 2022 International Con- ference on Advanced Visual Interfaces (2022), pp. 1–3 (cit. on p. 6).\\n\\n[33] R. G. Boboc, F. Gîrbacia, and E. V. Butilă, “The application of augmented reality in the automotive industry: A systematic literature review”, Applied Sciences 10, 4259 (2020) (cit. on p. 6).\\n\\n[34] M. Billinghurst, A. Clark, G. Lee, et al., “A Survey of Augmented Real- ity”, Foundations and Trends® in Human–Computer Interaction 8, 73–272 (2015) (cit. on p. 7).\\n\\n[35] C.-C. Teng, N. Jensen, T. Smith, T. Forbush, K. Fletcher, and M. Hoover, “Interactive augmented live virtual reality streaming: a health care applica- tion”, in Proceedings of the 2nd International Conference on Medical and Health Informatics (2018), pp. 143–147 (cit. on p. 7).\\n\\n[36] A. Ferrario, R. Weibel, and S. Feuerriegel, “ALEEDSA: Augmented Reality for Interactive Machine Learning”, in Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems (2020), pp. 1–8 (cit. on p. 7).\\n\\n[37] Y. Zhang, A. Nowak, G. Rao, A. Romanowski, and M. Fjeld, “Is Industrial Tomography Ready for Augmented Reality? A Need-finding Study of How Augmented Reality Can Be Adopted by Industrial Tomography Experts”, in International Conference on Human-Computer Interaction (Springer, 2023, in press) (cit. on pp. 7, 8).\\n\\n[38] K. Grudzien, “Visualization system for large-scale silo flow monitoring based on ECT technique”, IEEE Sensors Journal 17, 8242–8250 (2017) (cit. on p. 7).\\n\\n[39] D. Kalkofen, E. Mendez, and D. Schmalstieg, “Comprehensible visualiza- tion for augmented reality”, IEEE transactions on visualization and com- puter graphics 15, 193–204 (2008) (cit. on p. 8).\\n\\n[40] M. Tonnis, C. Sandor, G. Klinker, C. Lange, and H. Bubb, “Experimental evaluation of an augmented reality visualization for directing a car driver’s attention”, in Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR’05) (IEEE, 2005), pp. 56–59 (cit. on p. 8).\\n\\n[41] R. Pierdicca, E. Frontoni, P. Zingaretti, E. S. Malinverni, F. Colosi, and R. Orazi, “Making visible the invisible. augmented reality visualization for 3D reconstructions of archaeological sites”, in International Conference on Augmented and Virtual Reality (Springer, 2015), pp. 25–37 (cit. on p. 8).\\n\\n[42] D. Kahl, M. Ruble, and A. Krüger, “Investigation of Size Variations in Optical See-through Tangible Augmented Reality”, in 2021 IEEE Inter- national Symposium on Mixed and Augmented Reality (ISMAR) (IEEE, 2021), pp. 147–155 (cit. on p. 8).\\n\\n[43] L. Besançon, P. Issartel, M. Ammi, and T. Isenberg, “Mouse, tactile, and tangible input for 3D manipulation”, in Proceedings of the 2017 CHI con- ference on human factors in computing systems (2017), pp. 4727–4740 (cit. on p. 8).\\n\\n[44] B. Kress, E. Saeedi, and V. Brac-de-la Perriere, “The segmentation of the HMD market: optics for smart glasses, smart eyewear, AR and VR head- sets”, Photonics Applications for Aviation, Aerospace, Commercial, and Harsh Environments V 9202, 107–120 (2014) (cit. on p. 8).\\n\\n[45] E. Peillard, Y. Itoh, G. Moreau, J.-M. Normand, A. Lécuyer, and F. Arge- laguet, “Can retinal projection displays improve spatial perception in aug- mented reality?”, in 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) (IEEE, 2020), pp. 80–89 (cit. on p. 8).\\n\\n[46] F. A. Khan, V. V. R. M. K. Rao, D. Wu, M. S. Arefin, N. Phillips, J. E. Swan, et al., “Measuring the Perceived Three-Dimensional Location of Vir- tual Objects in Optical See-Through Augmented Reality”, in 2021 IEEE In- ternational Symposium on Mixed and Augmented Reality (ISMAR) (IEEE, 2021), pp. 109–117 (cit. on p. 8).\\n\\n[47] L. Heemsbergen, G. Bowtell, and J. Vincent, “Conceptualising Augmented Reality: From virtual divides to mediated dynamics”, Convergence 27, 830– 846 (2021) (cit. on p. 8).\\n\\n[48] E. Dubois, L. Nigay, and J. Troccaz, “Consistency in augmented reality systems”, in IFIP International Conference on Engineering for Human- Computer Interaction (Springer, 2001), pp. 111–122 (cit. on p. 8).\\n\\n[49] T. Masood and J. Egger, “Adopting augmented reality in the age of in- dustrial digitalisation”, Computers in Industry 115, 103112 (2020) (cit. on p. 8).\\n\\n[50] P. Fite-Georgel, “Is there a reality in industrial augmented reality?”, in 2011 10th ieee international symposium on mixed and augmented reality (IEEE, 2011), pp. 201–210 (cit. on p. 8).\\n\\n[51] S. K. Ong and A. Y. C. Nee, Virtual and augmented reality applications in manufacturing (Springer Science & Business Media, 2013) (cit. on p. 8).\\n\\n[52] D. Mourtzis, V. Siatras, and V. Zogopoulos, “Augmented reality visualiza- tion of production scheduling and monitoring”, Procedia CIRP 88, 151–156 (2020) (cit. on pp. 8, 11).\\n\\n39\\n\\n40\\n\\n6 References\\n\\n[53] J. B. Alves, B. Marques, C. Ferreira, P. Dias, and B. S. Santos, “Comparing augmented reality visualization methods for assembly procedures”, Virtual Reality 26, 235–248 (2022) (cit. on pp. 8, 11).\\n\\n[54] D. Dunn, O. Tursun, H. Yu, P. Didyk, K. Myszkowski, and H. Fuchs, “Stim- ulating the human visual system beyond real world performance in future augmented reality displays”, in 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) (IEEE, 2020), pp. 90–100 (cit. on p. 8).\\n\\n[55] S. J. Henderson and S. K. Feiner, “Augmented reality in the psychomotor phase of a procedural task”, in 2011 10th IEEE international symposium on mixed and augmented reality (IEEE, 2011), pp. 191–200 (cit. on p. 8).\\n\\n[56] Y. Zhang, Y. Xuan, A. Omrani, R. Yadav, and M. Fjeld, “Playing with Data: An Augmented Reality Approach to Immersively Interact with Vi- sualizations of Industrial Process Tomography”, in IFIP Conference on Human-Computer Interaction (Springer, 2023, Under review) (cit. on p. 9).\\n\\n[57] J.-Y. Lee, H.-M. Park, S.-H. Lee, T.-E. Kim, and J.-S. Choi, “Design and implementation of an augmented reality system using gaze interaction”, in 2011 International Conference on Information Science and Applications (IEEE, 2011), pp. 1–8 (cit. on p. 9).\\n\\n[58] A. Nowak, Y. Zhang, A. Romanowski, and M. Fjeld, “Augmented Reality with Industrial Process Tomography: To Support Complex Data Analysis in 3D Space”, in Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers (2021), pp. 56–58 (cit. on pp. 9, 12).\\n\\n[59] H. M. Park, S. H. Lee, and J. S. Choi, “Wearable augmented reality system using gaze interaction”, in 2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality (IEEE, 2008), pp. 175–176 (cit. on pp. 10, 13).\\n\\n[60] M. Lankes and B. Stiglbauer, “GazeAR: Mobile gaze-based interaction in the context of augmented reality games”, in International Conference on Augmented Reality, Virtual Reality and Computer Graphics (Springer, 2016), pp. 397–406 (cit. on p. 10).\\n\\n[61] M. Kytö, B. Ens, T. Piumsomboon, G. A. Lee, and M. Billinghurst, “Pin- pointing: Precise head-and eye-based target selection for augmented re- ality”, in Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (2018), pp. 1–14 (cit. on p. 10).\\n\\n[62] Y.-Y. Huang and M. Menozzi, “Effects of viewing distance and age on the performance and symptoms in a visual search task in augmented reality”, Applied Ergonomics 102, 103746 (2022) (cit. on p. 10).\\n\\n[63] P. Contreras, D. Chimbo, A. Tello, and M. Espinoza, “Semantic web and augmented reality for searching people, events and points of interest within of a university campus”, in 2017 XLIII Latin American Computer Confer- ence (CLEI) (IEEE, 2017), pp. 1–10 (cit. on pp. 10, 12).\\n\\n[64] F. S. Shaikh, “Augmented Reality Search to Improve Searching Using Aug- mented Reality”, in 2021 6th International Conference for Convergence in Technology (I2CT) (IEEE, 2021), pp. 1–5 (cit. on p. 10).\\n\\n[65] C. Reardon, K. Lee, and J. Fink, “Come see this! augmented reality to enable human-robot cooperative search”, in 2018 IEEE International Sym- posium on Safety, Security, and Rescue Robotics (SSRR) (IEEE, 2018), pp. 1–7 (cit. on p. 10).\\n\\n[66] R. Rivu, Y. Abdrabou, K. Pfeuffer, A. Esteves, S. Meitner, and F. Alt, “StARe: Gaze-Assisted Face-to-Face Communication in Augmented Re- ality”, in ACM Symposium on Eye Tracking Research and Applications (2020), pp. 1–5 (cit. on pp. 10, 13).\\n\\n[67] Y. Zhang, A. Nowak, A. Romanowski, and M. Fjeld, “An Initial Explo- ration of Visual Cues in Head-mounted Display Augmented Reality for Book Searching”, in Proceedings of the 21st International Conference on Mobile and Ubiquitous Multimedia (2022) (cit. on p. 10).\\n\\n[68] S. White, L. Lister, and S. Feiner, “Visual hints for tangible gestures in augmented reality”, in 2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality (IEEE, 2007), pp. 47–50 (cit. on pp. 10, 13).\\n\\n[69] K. Lyons, M. Gandy, and T. Starner, “Guided by voices: An audio aug- mented reality system”, in (Georgia Institute of Technology, 2000) (cit. on pp. 10, 13).\\n\\n[70] N. Sawhney and C. Schmandt, “Nomadic radio: speech and audio inter- action for contextual messaging in nomadic environments”, ACM transac- tions on Computer-Human interaction (TOCHI) 7, 353–383 (2000) (cit. on pp. 10, 13).\\n\\n[71] M. Sousa, J. Vieira, D. Medeiros, A. Arsenio, and J. Jorge, “SleeveAR: Aug- mented reality for rehabilitation using realtime feedback”, in Proceedings of the 21st international conference on intelligent user interfaces (2016), pp. 175–185 (cit. on p. 10).\\n\\n41\\n\\n42\\n\\n6 References\\n\\n[72] F. Clemente, S. Dosen, L. Lonini, M. Markovic, D. Farina, and C. Cipriani, “Humans can integrate augmented reality feedback in their sensorimotor control of a robotic hand”, IEEE Transactions on Human-Machine Systems 47, 583–589 (2016) (cit. on pp. 10, 14).\\n\\n[73] J. Vieira, M. Sousa, A. Arsénio, and J. Jorge, “Augmented reality for re- habilitation using multimodal feedback”, in Proceedings of the 3rd 2015 Workshop on ICTs for improving Patients Rehabilitation Research Tech- niques (2015), pp. 38–41 (cit. on p. 10).\\n\\n[74] Y. Zhang, A. Nowak, X. Yueming, R. Andrzej, and F. Morten, “See or Hear? Exploring the Effect of Visual and Audio Hints and Gaze-assisted Task Feedback for Visual Search Tasks in Augmented Reality”, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 2023 (Under review) (cit. on p. 10).\\n\\n[75] A. M. Treisman and G. Gelade, “A feature-integration theory of attention”, Cognitive psychology 12, 97–136 (1980) (cit. on p. 10).\\n\\n[76] F. Bruno, F. Caruso, L. De Napoli, and M. Muzzupappa, “Visualization of industrial engineering data visualization of industrial engineering data in augmented reality”, Journal of visualization 9, 319–329 (2006) (cit. on p. 11).\\n\\n[77] M. Noghabaei, A. Heydarian, V. Balali, and K. Han, “A Survey Study to Understand Industry Vision for Virtual and Augmented Reality Applica- tions in Design and Construction”, arXiv preprint arXiv:2005.02795 (2020) (cit. on pp. 11, 29).\\n\\n[78] M. Lorenz, S. Knopp, and P. Klimant, “Industrial augmented reality: Re- quirements for an augmented reality maintenance worker support system”, in 2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct) (IEEE, 2018), pp. 151–153 (cit. on p. 11).\\n\\n[79] S. Büttner, M. Funk, O. Sand, and C. Röcker, “Using head-mounted dis- plays and in-situ projection for assistive systems: A comparison”, in Pro- ceedings of the 9th ACM international conference on pervasive technologies related to assistive environments (2016), pp. 1–8 (cit. on p. 12).\\n\\n[80] G. Avalle, F. De Pace, C. Fornaro, F. Manuri, and A. Sanna, “An augmented reality system to support fault visualization in industrial robotic tasks”, IEEE Access 7, 132343–132359 (2019) (cit. on p. 12).\\n\\n[81] M. Satkowski and R. Dachselt, “Investigating the Impact of Real-World Environments on the Perception of 2D Visualizations in Augmented Re- ality”, in Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (2021), pp. 1–15 (cit. on p. 12).\\n\\n[82] R. Mann, S. Stanley, D. Vlaev, E. Wabo, and K. Primrose, “Augmented- reality visualization of fluid mixing in stirred chemical reactors using elec- trical resistance tomography”, Journal of Electronic Imaging 10, 620–630 (2001) (cit. on p. 12).\\n\\n[83] Y. Zhang, A. Omrani, R. Yadav, and M. Fjeld, “Supporting Visualization Analysis in Industrial Process Tomography by Using Augmented Reality–A Case Study of an Industrial Microwave Drying System”, Sensors 21, 6515 (2021) (cit. on pp. 12, 18, 30).\\n\\n[84] A. Rafiq and B. Ahsan, “Secure and Dynamic Model for Book Searching on Cloud Computing as Mobile Augmented Reality.”, International Journal of Modern Education & Computer Science 6 (2014) (cit. on p. 12).\\n\\n[85] C. Gebhardt, B. Hecox, B. van Opheusden, D. Wigdor, J. Hillis, O. Hilliges, and H. Benko, “Learning cooperative personalized policies from gaze data”, in Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (2019), pp. 197–208 (cit. on p. 12).\\n\\n[86] C. Trepkowski, D. Eibich, J. Maiero, A. Marquardt, E. Kruijff, and S. Feiner, “The effect of narrow field of view and information density on visual search performance in augmented reality”, in 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR) (IEEE, 2019), pp. 575–584 (cit. on p. 13).\\n\\n[87] J. Van Dam, A. Krasne, and J. L. Gabbard, “Drone-based augmented real- ity platform for bridge inspection: Effect of ar cue design on visual search tasks”, in 2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW) (IEEE, 2020), pp. 201–204 (cit. on p. 13).\\n\\n[88] A. Ajanki, M. Billinghurst, H. Gamper, T. Järvenpää, M. Kandemir, S. Kaski, M. Koskela, M. Kurimo, J. Laaksonen, K. Puolamäki, et al., “An augmented reality interface to contextual information”, Virtual reality 15, 161–173 (2011) (cit. on p. 13).\\n\\n[89] F. Lu, S. Davari, L. Lisle, Y. Li, and D. A. Bowman, “Glanceable ar: Eval- uating information access methods for head-worn augmented reality”, in 2020 IEEE conference on virtual reality and 3D user interfaces (VR) (IEEE, 2020), pp. 930–939 (cit. on p. 13).\\n\\n[90] N. Pathmanathan, M. Becher, N. Rodrigues, G. Reina, T. Ertl, D. Weiskopf, and M. Sedlmair, “Eye vs. head: Comparing gaze methods for interaction in augmented reality”, in ACM Symposium on Eye Tracking Research and Applications (2020), pp. 1–5 (cit. on p. 13).\\n\\n43\\n\\n44\\n\\n6 References\\n\\n[91] P. Sasikumar, L. Gao, H. Bai, and M. Billinghurst, “Wearable remotefusion: A mixed reality remote collaboration system with local eye gaze and remote hand gesture sharing”, in 2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct) (IEEE, 2019), pp. 393– 394 (cit. on p. 13).\\n\\n[92] J. Looser, M. Billinghurst, R. Grasset, and A. Cockburn, “An evaluation of virtual lenses for object selection in augmented reality”, in Proceedings of the 5th international conference on Computer graphics and interactive techniques in Australia and Southeast Asia (2007), pp. 203–210 (cit. on p. 13).\\n\\n[93] V. Peysakhovich, O. LefranÃ§ois, F. Dehais, and M. Causse, “The Neuroer- gonomics of Aircraft Cockpits: The Four Stages of Eye-Tracking Integration to Enhance Flight Safety”, Safety 4 (2018) 10.3390/safety4010008 (cit. on p. 13).\\n\\n[94] C. Curatu, H. Hua, and J. Rolland, “Projection-based head-mounted dis- play with eye-tracking capabilities”, in Novel Optical Systems Design and Optimization VIII, Vol. 5875 (International Society for Optics and Photon- ics, 2005), 58750J (cit. on p. 13).\\n\\n[95] E. Nonino, J. Gisler, V. Holzwarth, C. Hirt, and A. Kunz, “Subtle Attention Guidance for Real Walking in Virtual Environments”, in 2021 IEEE Inter- national Symposium on Mixed and Augmented Reality Adjunct (ISMAR- Adjunct) (IEEE, 2021), pp. 310–315 (cit. on p. 13).\\n\\n[96] R. Behringer, J. Park, and V. Sundareswaran, “Model-based visual tracking for outdoor augmented reality applications”, in Proceedings. international symposium on mixed and augmented reality (IEEE, 2002), pp. 277–322 (cit. on p. 13).\\n\\n[97] Z. Zhu, V. Branzoi, M. Wolverton, G. Murray, N. Vitovitch, L. Yarnall, G. Acharya, S. Samarasekera, and R. Kumar, “AR-mentor: Augmented reality based mentoring system”, in 2014 IEEE international symposium on mixed and augmented reality (ISMAR) (IEEE, 2014), pp. 17–22 (cit. on p. 13).\\n\\n[98] D. Wolf, D. Besserer, K. Sejunaite, M. Riepe, and E. Rukzio, “care: An augmented reality support system for dementia patients”, in The 31st An- nual ACM Symposium on User Interface Software and Technology Adjunct Proceedings (2018), pp. 42–44 (cit. on p. 13).\\n\\n[99] R. W. Lindeman, H. Noma, and P. G. De Barros, “Hear-through and mic- through augmented reality: Using bone conduction to display spatialized audio”, in 2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality (IEEE, 2007), pp. 173–176 (cit. on p. 13).\\n\\n[100] V. Sundareswaran, K. Wang, S. Chen, R. Behringer, J. McGee, C. Tam, and P. Zahorik, “3D audio augmented reality: implementation and experi- ments”, in The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings. (IEEE, 2003), pp. 296–297 (cit. on p. 13).\\n\\n[101] A. Marquardt, C. Trepkowski, T. D. Eibich, J. Maiero, and E. Kruijff, “Non-visual cues for view management in narrow field of view augmented reality displays”, in 2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) (IEEE, 2019), pp. 190–201 (cit. on p. 13).\\n\\n[102] S. Arevalo Arboleda, F. Rücker, T. Dierks, and J. Gerken, “Assisting ma- nipulation and grasping in robot teleoperation with augmented reality vi- sual cues”, in Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (2021), pp. 1–14 (cit. on p. 13).\\n\\n[103] A. Mulloni, H. Seichter, and D. Schmalstieg, “User experiences with aug- mented reality aided navigation on phones”, in 2011 10th IEEE Interna- tional Symposium on Mixed and Augmented Reality (IEEE, 2011), pp. 229– 230 (cit. on p. 14).\\n\\n[104] M. Cidota, S. Lukosch, D. Datcu, and H. Lukosch, “Comparing the effect of audio and visual notifications on workspace awareness using head-mounted displays for remote collaboration in augmented reality”, Augmented Human Research 1, 1–15 (2016) (cit. on p. 14).\\n\\n[105] Y. Cao, T. Wang, X. Qian, P. S. Rao, M. Wadhawan, K. Huo, and K. Ramani, “GhostAR: A time-space editor for embodied authoring of human- robot collaborative task with augmented reality”, in Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (2019), pp. 521–534 (cit. on p. 14).\\n\\n[106] V. Paelke, “Augmented reality in the smart factory: Supporting workers in an industry 4.0. environment”, in Proceedings of the 2014 IEEE emerging technology and factory automation (ETFA) (IEEE, 2014), pp. 1–4 (cit. on p. 14).\\n\\n[107] C. Liu, S. Huot, J. Diehl, W. Mackay, and M. Beaudouin-Lafon, “Evalu- ating the benefits of real-time feedback in mobile augmented reality with hand-held devices”, in Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (2012), pp. 2973–2976 (cit. on p. 14).\\n\\n[108] M. Zahiri, C. A. Nelson, D. Oleynikov, and K.-C. Siu, “Evaluation of aug- mented reality feedback in surgical training environment”, Surgical Inno- vation 25, 81–87 (2018) (cit. on p. 14).\\n\\n45\\n\\n46\\n\\n6 References\\n\\n[109] K. Murakami, R. Kiyama, T. Narumi, T. Tanikawa, and M. Hirose, “Poster: A wearable augmented reality system with haptic feedback and its perfor- mance in virtual assembly tasks”, in 2013 IEEE Symposium on 3D User Interfaces (3DUI) (IEEE, 2013), pp. 161–162 (cit. on p. 14).\\n\\n[110] A. Riegler, B. Aksoy, A. Riener, and C. Holzmann, “Gaze-based interaction with windshield displays for automated driving: Impact of dwell time and feedback design on task performance and subjective workload”, in 12th International Conference on Automotive User Interfaces and Interactive Vehicular Applications (2020), pp. 151–160 (cit. on p. 14).\\n\\n[111] Y. Zhang, R. Yadav, A. Omrani, and M. Fjeld, “A Novel Augmented Reality System To Support Volumetric Visualization in Industrial Process Tomog- raphy”, in Proceedings of the 2021 Conference on Interfaces and Human Computer Interaction (2021), pp. 3–9 (cit. on pp. 18, 20).\\n\\n[112] A. Omrani, R. Yadav, G. Link, and J. Jelonnek, “A Multistatic Uniform Diffraction Tomography Algorithm for Microwave Imaging in Multilayered Media for Microwave Drying”, IEEE Transactions on Antennas and Prop- agation (2022) (cit. on p. 21).\\n\\n[113] J. V. Bradley, “Complete counterbalancing of immediate sequential effects in a Latin square design”, Journal of the American Statistical Association 53, 525–528 (1958) (cit. on pp. 22, 31).\\n\\n[114] A. Bangor, P. T. Kortum, and J. T. Miller, “An empirical evaluation of the system usability scale”, Intl. Journal of Human–Computer Interaction 24, 574–594 (2008) (cit. on p. 23).\\n\\n[115] A. Romanowski, “Big Data-Driven Contextual Processing Methods for Elec- trical Capacitance Tomography”, IEEE Transactions on Industrial Infor- matics 15, 1609–1618 (2019) (cit. on p. 29).\\n\\n[116] H. Lasi, P. Fettke, H.-G. Kemper, T. Feld, and M. Hoffmann, “Industry 4.0”, Business & information systems engineering 6, 239–242 (2014) (cit. on p. 34).', metadata={'source': 'eval/46.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "# load from directory\n",
    "loader = DirectoryLoader(\"eval\")\n",
    "documents = loader.load()\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 793, which is longer than the specified 450\n",
      "Created a chunk of size 1208, which is longer than the specified 450\n",
      "Created a chunk of size 859, which is longer than the specified 450\n",
      "Created a chunk of size 526, which is longer than the specified 450\n",
      "Created a chunk of size 676, which is longer than the specified 450\n",
      "Created a chunk of size 486, which is longer than the specified 450\n",
      "Created a chunk of size 1078, which is longer than the specified 450\n",
      "Created a chunk of size 1007, which is longer than the specified 450\n",
      "Created a chunk of size 2073, which is longer than the specified 450\n",
      "Created a chunk of size 1431, which is longer than the specified 450\n",
      "Created a chunk of size 535, which is longer than the specified 450\n",
      "Created a chunk of size 757, which is longer than the specified 450\n",
      "Created a chunk of size 585, which is longer than the specified 450\n",
      "Created a chunk of size 683, which is longer than the specified 450\n",
      "Created a chunk of size 771, which is longer than the specified 450\n",
      "Created a chunk of size 625, which is longer than the specified 450\n",
      "Created a chunk of size 872, which is longer than the specified 450\n",
      "Created a chunk of size 510, which is longer than the specified 450\n",
      "Created a chunk of size 951, which is longer than the specified 450\n",
      "Created a chunk of size 877, which is longer than the specified 450\n",
      "Created a chunk of size 671, which is longer than the specified 450\n",
      "Created a chunk of size 995, which is longer than the specified 450\n",
      "Created a chunk of size 880, which is longer than the specified 450\n",
      "Created a chunk of size 679, which is longer than the specified 450\n",
      "Created a chunk of size 1656, which is longer than the specified 450\n",
      "Created a chunk of size 1344, which is longer than the specified 450\n",
      "Created a chunk of size 972, which is longer than the specified 450\n",
      "Created a chunk of size 901, which is longer than the specified 450\n",
      "Created a chunk of size 666, which is longer than the specified 450\n",
      "Created a chunk of size 1208, which is longer than the specified 450\n",
      "Created a chunk of size 719, which is longer than the specified 450\n",
      "Created a chunk of size 1003, which is longer than the specified 450\n",
      "Created a chunk of size 1202, which is longer than the specified 450\n",
      "Created a chunk of size 808, which is longer than the specified 450\n",
      "Created a chunk of size 523, which is longer than the specified 450\n",
      "Created a chunk of size 1194, which is longer than the specified 450\n",
      "Created a chunk of size 1040, which is longer than the specified 450\n",
      "Created a chunk of size 503, which is longer than the specified 450\n",
      "Created a chunk of size 1098, which is longer than the specified 450\n",
      "Created a chunk of size 844, which is longer than the specified 450\n",
      "Created a chunk of size 992, which is longer than the specified 450\n",
      "Created a chunk of size 525, which is longer than the specified 450\n",
      "Created a chunk of size 487, which is longer than the specified 450\n",
      "Created a chunk of size 619, which is longer than the specified 450\n",
      "Created a chunk of size 570, which is longer than the specified 450\n",
      "Created a chunk of size 551, which is longer than the specified 450\n",
      "Created a chunk of size 1572, which is longer than the specified 450\n",
      "Created a chunk of size 609, which is longer than the specified 450\n",
      "Created a chunk of size 863, which is longer than the specified 450\n",
      "Created a chunk of size 475, which is longer than the specified 450\n",
      "Created a chunk of size 593, which is longer than the specified 450\n",
      "Created a chunk of size 495, which is longer than the specified 450\n",
      "Created a chunk of size 803, which is longer than the specified 450\n",
      "Created a chunk of size 742, which is longer than the specified 450\n",
      "Created a chunk of size 1436, which is longer than the specified 450\n",
      "Created a chunk of size 858, which is longer than the specified 450\n",
      "Created a chunk of size 1382, which is longer than the specified 450\n",
      "Created a chunk of size 459, which is longer than the specified 450\n",
      "Created a chunk of size 555, which is longer than the specified 450\n",
      "Created a chunk of size 1371, which is longer than the specified 450\n",
      "Created a chunk of size 875, which is longer than the specified 450\n",
      "Created a chunk of size 816, which is longer than the specified 450\n",
      "Created a chunk of size 806, which is longer than the specified 450\n",
      "Created a chunk of size 769, which is longer than the specified 450\n",
      "Created a chunk of size 686, which is longer than the specified 450\n",
      "Created a chunk of size 533, which is longer than the specified 450\n",
      "Created a chunk of size 460, which is longer than the specified 450\n",
      "Created a chunk of size 868, which is longer than the specified 450\n",
      "Created a chunk of size 1021, which is longer than the specified 450\n",
      "Created a chunk of size 613, which is longer than the specified 450\n",
      "Created a chunk of size 726, which is longer than the specified 450\n",
      "Created a chunk of size 861, which is longer than the specified 450\n",
      "Created a chunk of size 728, which is longer than the specified 450\n",
      "Created a chunk of size 531, which is longer than the specified 450\n",
      "Created a chunk of size 1627, which is longer than the specified 450\n",
      "Created a chunk of size 548, which is longer than the specified 450\n",
      "Created a chunk of size 1069, which is longer than the specified 450\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=450,\n",
    "    chunk_overlap=150\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    chunk.metadata[\"file_name\"] = chunk.metadata[\"source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "embedding nodes:  32%|███▏      | 18/56 [00:04<00:07,  5.08it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m\n\u001b[1;32m     18\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m SentenceTransformerEmbeddings(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintfloat/multilingual-e5-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m generator \u001b[38;5;241m=\u001b[39m TestsetGenerator\u001b[38;5;241m.\u001b[39mfrom_langchain(\n\u001b[1;32m     21\u001b[0m     generator_llm,\n\u001b[1;32m     22\u001b[0m     critic_llm,\n\u001b[1;32m     23\u001b[0m     embeddings\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m testset \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_langchain_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43msimple\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_context\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ag/lib/python3.11/site-packages/ragas/testset/generator.py:175\u001b[0m, in \u001b[0;36mTestsetGenerator.generate_with_langchain_docs\u001b[0;34m(self, documents, test_size, distributions, with_debugging_logs, is_async, raise_exceptions, run_config)\u001b[0m\n\u001b[1;32m    173\u001b[0m distributions \u001b[38;5;241m=\u001b[39m distributions \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# chunk documents and add to docstore\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mDocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_langchain_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    180\u001b[0m     test_size\u001b[38;5;241m=\u001b[39mtest_size,\n\u001b[1;32m    181\u001b[0m     distributions\u001b[38;5;241m=\u001b[39mdistributions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m     run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[1;32m    186\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ag/lib/python3.11/site-packages/ragas/testset/docstore.py:215\u001b[0m, in \u001b[0;36mInMemoryDocumentStore.add_documents\u001b[0;34m(self, docs, show_progress)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# split documents with self.splitter into smaller nodes\u001b[39;00m\n\u001b[1;32m    211\u001b[0m nodes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    212\u001b[0m     Node\u001b[38;5;241m.\u001b[39mfrom_langchain_document(d)\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39mtransform_documents(docs)\n\u001b[1;32m    214\u001b[0m ]\n\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ag/lib/python3.11/site-packages/ragas/testset/docstore.py:252\u001b[0m, in \u001b[0;36mInMemoryDocumentStore.add_nodes\u001b[0;34m(self, nodes, show_progress)\u001b[0m\n\u001b[1;32m    245\u001b[0m         executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m    246\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextractor\u001b[38;5;241m.\u001b[39mextract,\n\u001b[1;32m    247\u001b[0m             n,\n\u001b[1;32m    248\u001b[0m             name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyphrase-extraction[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    249\u001b[0m         )\n\u001b[1;32m    250\u001b[0m         result_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 252\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n",
      "File \u001b[0;32m~/anaconda3/envs/ag/lib/python3.11/site-packages/ragas/executor.py:132\u001b[0m, in \u001b[0;36mExecutor.results\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m executor_job\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     \u001b[43mexecutor_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/ag/lib/python3.11/threading.py:1119\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1119\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/ag/lib/python3.11/threading.py:1139\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1140\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "\n",
    "generator = TestsetGenerator.with_openai()\n",
    "\n",
    "testset = generator.generate_with_langchain_docs(chunks, test_size=2, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})\n",
    "'''\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "generator_llm = Ollama(model=\"mistral\")\n",
    "critic_llm = Ollama(model=\"mistral\")\n",
    "# critic_llm = ChatOpenAI(model=\"gpt-4\")\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "testset = generator.generate_with_langchain_docs(documents, test_size=6, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allangamal/anaconda3/envs/ag/lib/python3.11/site-packages/datasets/load.py:1461: FutureWarning: The repository for explodinggradients/amnesty_qa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/explodinggradients/amnesty_qa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/Users/allangamal/anaconda3/envs/ag/lib/python3.11/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    eval: Dataset({\n",
       "        features: ['question', 'ground_truth', 'answer', 'contexts'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "amnesty_qa = load_dataset(\"explodinggradients/amnesty_qa\", \"english_v2\")\n",
    "amnesty_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 80/80 [02:35<00:00,  1.94s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_precision': 1.0000, 'faithfulness': 0.7745, 'answer_relevancy': 0.9677, 'context_recall': 1.0000}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    amnesty_qa[\"eval\"],\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "    ],\n",
    ")\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from prompt_template import prompt as prompty\n",
    "import os.path\n",
    "from subprocess import STDOUT,PIPE\n",
    "from sys import stdin\n",
    "import subprocess\n",
    "from docingesterTemp import load_document_batch\n",
    "import json\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m      5\u001b[0m vector_database \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mchunks\u001b[49m,\n\u001b[1;32m      7\u001b[0m     embeddings\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chunks' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "vector_database = Chroma.from_documents(\n",
    "    chunks,\n",
    "    embeddings\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_template import prompt as prompty\n",
    "prompt = prompty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | generator_llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m questions \u001b[38;5;241m=\u001b[39m \u001b[43mtestset\u001b[49m\u001b[38;5;241m.\u001b[39mto_pandas()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[1;32m      2\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m testset\u001b[38;5;241m.\u001b[39mto_pandas()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()\n",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m questions \u001b[38;5;241m=\u001b[39m \u001b[43mtestset\u001b[49m\u001b[38;5;241m.\u001b[39mto_pandas()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[1;32m      2\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m testset\u001b[38;5;241m.\u001b[39mto_pandas()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/ag/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ag/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "questions = testset.to_pandas()[\"question\"].to_list()\n",
    "ground_truth = testset.to_pandas()[\"ground_truth\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "questions = testset.to_pandas()[\"question\"].to_list()\n",
    "ground_truth = testset.to_pandas()[\"ground_truth\"].to_list()\n",
    "\n",
    "data = {\"question\": [], \"answer\": [], \"contexts\": [], \"ground_truth\": ground_truth}\n",
    "\n",
    "for query in questions:\n",
    "    data[\"question\"].append(query)\n",
    "    data[\"answer\"].append(rag_chain.invoke(query))\n",
    "    data[\"contexts\"].append([doc.page_content for doc in retriever.get_relevant_documents(query)])\n",
    "\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "result = evaluate(\n",
    "    dataset = dataset,\n",
    "    metrics=[\n",
    "        context_relevancy,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "df = result.to_pandas()\n",
    "\n",
    "heatmap_data = df[['context_relevancy', 'context_precision', 'context_recall', 'faithfulness', 'answer_relevancy']]\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list('green_red', ['red', 'green'])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", linewidths=.5, cmap=cmap)\n",
    "\n",
    "plt.yticks(ticks=range(len(df['question'])), labels=df['question'], rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
